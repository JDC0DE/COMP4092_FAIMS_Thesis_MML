<!DOCTYPE html> <html lang=en style><!--
 Page saved with SingleFile 
 url: https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X 
 saved date: Sun Sep 18 2022 17:43:40 GMT+1000 (Australian Eastern Standard Time)
--><meta charset=utf-8>
<meta name=citation_pii content=S003132031400199X>
<meta name=citation_issn content=0031-3203>
<meta name=citation_volume content=48>
<meta name=citation_lastpage content=1100>
<meta name=citation_issue content=4>
<meta name=citation_publisher content=Pergamon>
<meta name=citation_firstpage content=1086>
<meta name=citation_journal_title content="Pattern Recognition">
<meta name=citation_type content=JOUR>
<meta name=citation_doi content=10.1016/j.patcog.2014.05.014>
<meta name=dc.identifier content=10.1016/j.patcog.2014.05.014>
<meta name=citation_article_type content="Full-length article">
<meta property=og:description content="In this paper we introduce the DCT-GIST image representation model which is useful to summarize the context of the scene. The proposed image descripto…">
<meta property=og:image content=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0031320314X00130-cov150h.gif>
<meta name=citation_title content="Representing scenes for real-time context classification on mobile devices">
<meta property=og:title content="Representing scenes for real-time context classification on mobile devices">
<meta name=citation_publication_date content=2015/04/01>
<meta name=citation_online_date content=2014/06/09>
<meta name=robots content=INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR>
<title>Representing scenes for real-time context classification on mobile devices - ScienceDirect</title>
<link rel=canonical href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X>
<meta property=og:type content=article>
<meta name=viewport content="initial-scale=1">
<meta name=SDTech content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
<meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><style>.plx-logo{height:16px;width:65.62712px;margin:0 0 0 4px!important;vertical-align:top;border:none!important;-ms-interpolation-mode:bicubic}.PlumX-Summary{font-weight:400;overflow:hidden}.PlumX-Summary *{text-align:left}.PlumX-Summary ul,.PlumX-Summary li{list-style-type:none!important;margin:0!important;padding:0!important;line-height:normal!important;background-image:none!important}.PlumX-Summary .pps-branding{height:14px;line-height:14px;color:#6e1a62}.PlumX-Summary .pps-cols{overflow:hidden}.PlumX-Summary .pps-container{border-top:none}.PlumX-Summary .pps-seemore{color:#007dbb!important}.PlumX-Summary .pps-container-vertical .pps-seemore{text-align:center!important;position:relative;top:-0.7em}.PlumX-Summary .pps-container{position:relative}.plum-sciencedirect-theme{font-family:Arial,Helvetica,"Lucida Sans Unicode","Microsoft Sans Serif","Segoe UI Symbol",STIXGeneral,"Cambria Math","Arial Unicode MS",sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:auto}.plum-sciencedirect-theme .PlumX-Summary{font-family:Arial,Helvetica,"Lucida Sans Unicode","Microsoft Sans Serif","Segoe UI Symbol",STIXGeneral,"Cambria Math","Arial Unicode MS",sans-serif;font-size:16px}.plum-sciencedirect-theme .PlumX-Summary .pps-container{float:none;border:0}.plum-sciencedirect-theme .PlumX-Summary .pps-title{margin-bottom:12px;line-height:1.5;font-weight:normal;padding-bottom:6px;color:#505050!important}.plum-sciencedirect-theme .PlumX-Summary .pps-cols{margin-bottom:16px}.plum-sciencedirect-theme .PlumX-Summary .pps-col{float:none;width:auto;padding:4px 0;border-top:none}.plum-sciencedirect-theme .PlumX-Summary .pps-col div.plx-socialMedia>.pps-title{border-bottom:2px solid #13b7ea}.plum-sciencedirect-theme .PlumX-Summary .pps-col div.plx-citation>.pps-title{border-bottom:2px solid #fd5704}.plum-sciencedirect-theme .PlumX-Summary .pps-col div.plx-capture>.pps-title{border-bottom:2px solid #c43bf3}.plum-sciencedirect-theme .PlumX-Summary .pps-col li:after,.plum-bigben-theme .PlumX-Summary .pps-col li:after{content:"";display:table;clear:both}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore{color:#007dbb;text-decoration:none;font-size:13px}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore .svg-arrow{width:8px;height:8px;margin-left:8px;transform:rotate(270deg)}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:active,.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:focus,.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:hover,.plum-bigben-theme .PlumX-Summary .pps-seemore:active,.plum-bigben-theme .PlumX-Summary .pps-seemore:focus,.plum-bigben-theme .PlumX-Summary .pps-seemore:hover{color:#e9711c;text-decoration:underline;outline:0}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-seemore{display:inline;float:right;text-align:left;margin-top:12px}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-branding{width:auto}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-col{margin-bottom:8px}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-branding-bottom{display:inline}.plum-sciencedirect-theme .PlumX-Summary .pps-count{color:#505050;font-weight:normal;display:block;float:left;vertical-align:top;width:30%;font-size:14px;text-align:right}.plum-sciencedirect-theme .PlumX-Summary .pps-label{color:#505050;display:block;float:left;vertical-align:top;width:66%;padding-right:4%;padding-bottom:8px;font-size:14px}.MJX_Assistive_MathML{position:absolute!important;top:0;left:0;clip:rect(1px,1px,1px,1px);padding:1px 0 0 0!important;border:0!important;height:1px!important;width:1px!important;overflow:hidden!important;display:block!important;-webkit-touch-callout:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1em;bottom:1.5em;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}#\_pendo-badge\_9BcFvkCLLiElWp6hocDK3ZG6Z4E{top:auto!important;left:auto!important;bottom:0px!important;right:20px!important;position:fixed!important}.MathJax_SVG{font-style:normal;font-weight:normal;line-height:normal;text-indent:0;text-align:left;text-transform:none;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;direction:ltr;max-width:none;max-height:none;min-width:0;min-height:0;border:0;padding:0;margin:0}.MathJax_SVG *{transition:none;-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none}.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"></head>
<body><div style=visibility:hidden;overflow:hidden;position:absolute;top:0px;height:1px;width:auto;padding:0px;border:0px;margin:0px;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal><div id=MathJax_SVG_Hidden></div><svg><defs id=MathJax_SVG_glyphs><path stroke-width=1 id=MJMATHI-66 d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path stroke-width=1 id=MJMAIN-28 d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width=1 id=MJMATHI-78 d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width=1 id=MJMAIN-7C d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width=1 id=MJMATHI-3BC d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path><path stroke-width=1 id=MJMAIN-2C d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width=1 id=MJMATHI-62 d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path stroke-width=1 id=MJMAIN-29 d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width=1 id=MJMAIN-3D d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width=1 id=MJMAIN-31 d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width=1 id=MJMAIN-32 d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width=1 id=MJMAIN-65 d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path stroke-width=1 id=MJMAIN-78 d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path><path stroke-width=1 id=MJMAIN-70 d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z"></path><path stroke-width=1 id=MJMAIN-2212 d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width=1 id=MJSZ2-28 d="M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z"></path><path stroke-width=1 id=MJSZ2-29 d="M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z"></path><path stroke-width=1 id=MJMAIN-2265 d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path stroke-width=1 id=MJMAIN-30 d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width=1 id=MJMATHI-4E d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path stroke-width=1 id=MJSZ1-2211 d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path stroke-width=1 id=MJMATHI-69 d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMAIN-2E d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path stroke-width=1 id=MJMAIN-2223 d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width=1 id=MJMATHI-44 d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path><path stroke-width=1 id=MJMATHI-6A d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path><path stroke-width=1 id=MJMAIN-2026 d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path stroke-width=1 id=MJMAIN-37 d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path><path stroke-width=1 id=MJMATHI-46 d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path><path stroke-width=1 id=MJMAIN-7B d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path><path stroke-width=1 id=MJMAIN-7D d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path><path stroke-width=1 id=MJSZ1-22C3 d="M96 750Q103 750 109 748T120 744T127 737T133 730T137 723T139 718V395L140 73L142 60Q159 -43 237 -104T416 -166Q521 -166 597 -103T690 60L692 73L694 718Q708 749 735 749Q765 749 775 720Q777 714 777 398Q777 78 776 71Q766 -51 680 -140Q571 -249 416 -249H411Q261 -249 152 -140Q66 -51 56 71Q55 78 55 398Q55 714 57 720Q60 734 70 740Q80 750 96 750Z"></path><path stroke-width=1 id=MJMAIN-33 d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path stroke-width=1 id=MJMAIN-3B d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 85 94 103T137 121Q202 121 202 8Q202 -44 183 -94T144 -169T118 -194Q115 -194 106 -186T95 -174Q94 -171 107 -155T137 -107T160 -38Q161 -32 162 -22T165 -4T165 4Q165 5 161 4T142 0Q110 0 94 18T78 60Z"></path><path stroke-width=1 id=MJMATHI-72 d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMATHI-6C d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width=1 id=MJMATHI-73 d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path stroke-width=1 id=MJMAIN-2208 d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path stroke-width=1 id=MJMATHI-53 d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path><path stroke-width=1 id=MJMATHI-48 d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path><path stroke-width=1 id=MJMATHI-6F d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path><path stroke-width=1 id=MJMATHI-7A d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path stroke-width=1 id=MJMATHI-6E d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMATHI-74 d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width=1 id=MJMATHI-61 d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path stroke-width=1 id=MJMATHI-56 d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path stroke-width=1 id=MJMATHI-65 d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path stroke-width=1 id=MJMATHI-63 d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path stroke-width=1 id=MJMATHI-47 d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path><path stroke-width=1 id=MJMATHI-64 d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path><path stroke-width=1 id=MJMATHI-57 d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path><path stroke-width=1 id=MJMATHI-42 d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path><path stroke-width=1 id=MJMATHI-68 d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path stroke-width=1 id=MJMATHI-77 d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path stroke-width=1 id=MJMATHI-59 d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path><path stroke-width=1 id=MJMATHI-43 d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path stroke-width=1 id=MJSZ3-28 d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"></path><path stroke-width=1 id=MJSZ3-29 d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"></path><path stroke-width=1 id=MJMATHI-4D d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path><path stroke-width=1 id=MJSZ2-7B d="M547 -643L541 -649H528Q515 -649 503 -645Q324 -582 293 -466Q289 -449 289 -428T287 -200L286 42L284 53Q274 98 248 135T196 190T146 222L121 235Q119 239 119 250Q119 262 121 266T133 273Q262 336 284 449L286 460L287 701Q287 737 287 794Q288 949 292 963Q293 966 293 967Q325 1080 508 1148Q516 1150 527 1150H541L547 1144V1130Q547 1117 546 1115T536 1109Q480 1086 437 1046T381 950L379 940L378 699Q378 657 378 594Q377 452 374 438Q373 437 373 436Q350 348 243 282Q192 257 186 254L176 251L188 245Q211 236 234 223T287 189T340 135T373 65Q373 64 374 63Q377 49 378 -93Q378 -156 378 -198L379 -438L381 -449Q393 -504 436 -544T536 -608Q544 -611 545 -613T547 -629V-643Z"></path><path stroke-width=1 id=MJSZ2-7D d="M119 1130Q119 1144 121 1147T135 1150H139Q151 1150 182 1138T252 1105T326 1046T373 964Q378 942 378 702Q378 469 379 462Q386 394 439 339Q482 296 535 272Q544 268 545 266T547 251Q547 241 547 238T542 231T531 227T510 217T477 194Q390 129 379 39Q378 32 378 -201Q378 -441 373 -463Q342 -580 165 -644Q152 -649 139 -649Q125 -649 122 -646T119 -629Q119 -622 119 -619T121 -614T124 -610T132 -607T143 -602Q195 -579 235 -539T285 -447Q286 -435 287 -199T289 51Q294 74 300 91T329 138T390 197Q412 213 436 226T475 244L489 250L472 258Q455 265 430 279T377 313T327 366T293 434Q289 451 289 472T287 699Q286 941 285 948Q279 978 262 1005T227 1048T184 1080T151 1100T129 1109L127 1110Q119 1113 119 1130Z"></path><path stroke-width=1 id=MJMAIN-5B d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path stroke-width=1 id=MJMATHBI-3BC d="M294 -8Q265 -8 244 -5T213 1T201 4Q200 4 192 -32T172 -111T155 -168Q134 -211 86 -211Q62 -211 48 -196T34 -158Q37 -144 103 123T174 404Q182 424 201 438T244 452Q271 452 284 436T298 404Q298 392 267 269T235 114Q235 43 305 43Q342 43 375 68T418 110Q420 112 455 253T492 397Q514 444 562 444Q587 444 601 429T615 397Q615 387 599 320T563 178T542 93Q540 81 540 72Q540 42 558 42Q580 42 596 75Q606 94 616 134Q621 155 624 158T646 162H651H662Q682 162 682 148Q681 142 679 132T665 94T641 47T602 9T548 -8Q523 -8 502 -3T468 11T446 27T432 40L429 46Q367 -8 294 -8Z"></path><path stroke-width=1 id=MJMAINB-62 d="M32 686L123 690Q214 694 215 694H221V409Q289 450 378 450Q479 450 539 387T600 221Q600 122 535 58T358 -6H355Q272 -6 203 53L160 1L129 0H98V301Q98 362 98 435T99 525Q99 591 97 604T83 620Q69 624 42 624H29V686H32ZM227 105L232 99Q237 93 242 87T258 73T280 59T306 49T339 45Q380 45 411 66T451 131Q457 160 457 230Q457 264 456 284T448 329T430 367T396 389T343 398Q282 398 235 355L227 348V105Z"></path><path stroke-width=1 id=MJMAIN-5D d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path stroke-width=1 id=MJSZ1-5B d="M202 -349V850H394V810H242V-309H394V-349H202Z"></path><path stroke-width=1 id=MJSZ1-5D d="M22 810V850H214V-349H22V-309H174V810H22Z"></path><path stroke-width=1 id=MJMAIN-35 d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"></path><path stroke-width=1 id=MJMAIN-6D d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path stroke-width=1 id=MJMAIN-73 d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path></defs></svg></div><div class=zotero-notificaton style='position:relative;z-index:2147483647;background:linear-gradient(rgb(255,225,62),rgb(255,199,3));color:rgba(0,0,0,0.95);border-bottom:1px solid rgb(191,138,1);padding:3px 10px 4px;display:flex;flex-direction:row;align-items:center;box-sizing:border-box;cursor:default;transition:margin-top 300ms ease 0s,opacity 300ms ease 0s;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;margin-top:0px;margin-right:0px;margin-left:0px'><span style=flex-grow:1>Zotero detected that you are accessing www.sciencedirect.com through a proxy. Would you like to automatically redirect future requests to www.sciencedirect.com through simsrad.net.ocs.mq.edu.au?</span><span style=min-width:30px></span><a data-id=2 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>Accept</a><a data-id=1 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>Proxy Settings</a><a data-id=0 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>✕</a></div><div id=MathJax_Message style=display:none></div>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics-elsevier-com.simsrad.net.ocs.mq.edu.au/b/ss/elsevier-sd-prod/1/G.4--NS/1663487007295?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_IP&c1=ae%3A2378&c12=ae%3A21981 />
    </noscript>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-content>Skip to main content</a>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-title>Skip to article</a>
<!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
<div data-iso-key=_0><div class=App id=app data-aa-name=root data-reactroot><div class=page><section><div class=sd-flex-container><div class=sd-flex-content><header id=gh-cnt><div id=gh-main-cnt class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-lg"><a id=gh-branding class=u-flex-center-ver href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/ aria-label="ScienceDirect home page" data-aa-region=header data-aa-name=ScienceDirect><img class=gh-logo src=data:null;base64, alt="Elsevier logo" height=48 width=54><svg xmlns=http://www.w3.org/2000/svg role=img version=1.1 height=30 width=138 viewBox="0 0 138 30" class="gh-wordmark u-margin-s-left" fill=#f36d21 aria-labelledby=gh-wm-science-direct focusable=false aria-hidden=true alt="ScienceDirect Wordmark"><title id=gh-wm-science-direct>ScienceDirect</title><g><path class=a d=M4.23,21a9.79,9.79,0,0,1-4.06-.83l.29-2.08a7.17,7.17,0,0,0,3.72,1.09c2.13,0,3-1.22,3-2.39C7.22,13.85.3,13.43.3,9c0-2.37,1.56-4.29,5.2-4.29a9.12,9.12,0,0,1,3.77.75l-.1,2.08a7.58,7.58,0,0,0-3.67-1c-2.24,0-2.91,1.22-2.91,2.39,0,3,6.92,3.61,6.92,7.8C9.5,19.1,7.58,21,4.23,21Z></path><path class=a d=M20.66,20A6.83,6.83,0,0,1,16.76,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H18.81c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M23.75,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,23.75,6.9ZM22.76,9h2V20.74h-2Z></path><path class=a d=M29.55,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,32.77,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM35.63,13c-.08-2.29-1.09-2.7-3-2.7A3.78,3.78,0,0,0,31,10.7,3.7,3.7,0,0,0,29.76,13Z></path><path class=a d=M49.7,20.74h-2s.1-2.73.08-5.1c0,0,0-1.56,0-2.5-.05-1.79-.21-2.7-2-2.7a4.87,4.87,0,0,0-1.64.31,12.11,12.11,0,0,0-1.95,2.08v7.9h-2v-8.5a19.47,19.47,0,0,0-.1-2.34L39.95,9h1.85l.31,1.74a4.71,4.71,0,0,1,3.82-2.05c2.11,0,3.54.68,3.74,3.09.1,1.17.08,2.34.08,3.51C49.75,17.2,49.7,20.74,49.7,20.74Z></path><path class=a d=M61.5,20A6.83,6.83,0,0,1,57.6,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H59.66c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M64.75,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,68,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM70.84,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,65,13Z></path><path class=a d=M81.21,20.74H75.83V5h5.62c5.54,0,7.46,4.21,7.46,7.8C88.91,16.26,86.93,20.74,81.21,20.74Zm-.1-14H77.88V19.07h3c4,0,5.75-2.31,5.75-6.24C86.59,10.15,85.34,6.7,81.11,6.7Z></path><path class=a d=M92.86,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,92.86,6.9ZM91.87,9h2V20.74h-2Z></path><path class=a d=M104.48,10.83l-1.64.47c0-.18-.08-1-.83-1-1.14,0-2.08,1.9-2.5,2.91v7.49h-2V12.18a18.78,18.78,0,0,0-.1-2.29L97.3,9h1.85l.34,1.87a3.22,3.22,0,0,1,2.68-2.16,2,2,0,0,1,2.26,1.72c0,.18.05.29.05.31Z></path><path class=a d=M107.44,14.6V15c0,2.81,1.38,4.34,3.85,4.34A6.37,6.37,0,0,0,115,18.11l.16,1.82A7.94,7.94,0,0,1,110.67,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM113.53,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,107.65,13Z></path><path class=a d=M126.24,20a6.83,6.83,0,0,1-3.9,1.09c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H124.4c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M134.51,20.45a7.36,7.36,0,0,1-2.7.62c-1.53,0-2.63-.86-2.63-2.94V10.52H127V9h2.13V5.81h2V9h3.09v1.56h-3.09v7c0,1.33.34,1.85,1.25,1.85a5.66,5.66,0,0,0,2-.55Z></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label=links class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/browse/journals-and-books data-aa-region=header data-aa-name="Journals &amp; Books"><span class=anchor-text>Journals &amp; Books</span></a></ul></nav><nav aria-label=utilities class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-move-to-spine gh-help-button gh-help-icon"><div class=popover id=gh-help-icon-popover><div id=popover-trigger-gh-help-icon-popover><button class="button-link gh-nav-help-icon gh-icon-btn button-link-primary" aria-expanded=false aria-label="ScienceDirect Support Center links" type=button><svg focusable=false viewBox="0 0 114 128" aria-hidden=true alt="ScienceDirect help page" width=21.375 height=24 class="icon icon-help gh-icon"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg><span class=button-link-text></span></button></div></div><li class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-primary gh-nav-action gh-icon-btn" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/search data-aa-button=search-in-header-opened-from-article aria-label="Opens ScienceDirect Search"><span class=anchor-text></span><svg focusable=false viewBox="0 0 100 128" aria-hidden=true alt=Search width=18.75 height=24 class="icon icon-search gh-icon"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a></ul></nav></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id=institution-popover><div id=popover-trigger-institution-popover><button id=gh-inst-icon-btn class="gh-icon-btn gh-has-institution u-margin-m-left" aria-expanded=false aria-label="Institutional Access"><svg focusable=false viewBox="0 0 106 128" aria-hidden=true alt="Institutional Access" width=19.875 height=24 class="icon icon-institution gh-inst-icon"><path d="m84 98h1e1v1e1h-82v-1e1h1e1v-46h14v46h1e1v-46h14v46h1e1v-46h14v46zm-72-61.14l41-20.84 41 20.84v5.14h-82v-5.14zm92 15.14v-21.26l-51-25.94-51 25.94v21.26h1e1v36h-1e1v3e1h102v-3e1h-1e1v-36h1e1z"></path></svg></button></div></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="link-button u-margin-m-left link-button-primary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS003132031400199X" id=gh-corpsignin-btn data-aa-region=header data-aa-name="Corporate sign in"><span class=link-button-text>Corporate sign in</span></a><a class="link-button u-margin-m-left link-button-secondary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS003132031400199X&amp;from=globalheader" id=gh-signin-btn data-aa-region=header data-aa-name="Sign in"><span class=link-button-text>Sign in / register</span></a></div><div class="gh-lib-banner u-hide-from-print gh-lb-legacy"><a href=http://www.mq.edu.au/on_campus/library/ target=_blank rel="noopener noreferrer" class=gh-lib-banner-link><img class="u-img-responsive u-max-lib-height" src=data:null;base64, alt="You have institutional access"></a></div><div id=gh-mobile-menu class="mobile-menu u-hide-from-print sf-hidden"></div></div></header><div class=Article id=mathjax-container><div class="sticky-outer-wrapper active" style=height:64px><div class=sticky-inner-wrapper style=position:fixed;z-index:2;transform:translate3d(0px,0px,0px);top:0px;width:1423px><div id=screen-reader-main-content></div><div class=accessbar role=region aria-label="Download options and search"><div class=accessbar-label></div><ul aria-label="PDF Options"><li><a class="link-button link-button-primary accessbar-primary-link" role=button aria-expanded=true aria-label="Download single PDF. Opens in a new window." aria-live=polite target=_blank href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X/pdfft?md5=9fe98ccfc4bb35e3f585f11f62675789&amp;pid=1-s2.0-S003132031400199X-main.pdf" rel=nofollow><svg focusable=false viewBox="0 0 32 32" height=24 width=24 class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=link-button-text>View&nbsp;<strong>PDF</strong></span></a><li><button class="button button-anchor accessbar-anchor-link" aria-label="Download Full Issue" type=button><span class=button-text>Download Full Issue</span></button></ul><form class=QuickSearch action=/search#submit aria-label=form><input type=search class=query aria-label="Search ScienceDirect" name=qs placeholder="Search ScienceDirect" value><button class="button button-primary" type=submit aria-label="Submit search"><span class=button-text><svg focusable=false viewBox="0 0 100 128" height=20 width=18.75 class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button></form></div></div></div><div class="article-wrapper u-padding-s-top grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang=en><div class=Outline id=toc-outline><h2 class=u-h4>Outline</h2><ol class=u-padding-xs-bottom><li><li><a href=#ab0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Abstract>Abstract</a><li><a href=#key0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Keywords>Keywords</a><li><a href=#s0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction and motivations">1. Introduction and motivations</a><li><a href=#s0010 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Related works">2. Related works</a><li><a href=#s0015 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. The statistics of natural image categories in DCT domain">3. The statistics of natural image categories in DCT domain</a><li><a href=#s0020 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Proposed DCT-GIST image representation">4. Proposed DCT-GIST image representation</a><li><a href=#s0025 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. The image generation pipeline architecture">5. The image generation pipeline architecture</a><li><a href=#s0045 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="6. Experimental settings and results">6. Experimental settings and results</a><li><a href=#s0075 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="7. Conclusion and future works">7. Conclusion and future works</a><li><a href=#s0080 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="Conflict of interest">Conflict of interest</a><li><a href=#ack0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Acknowledgments>Acknowledgments</a><li><a href=#bibliog0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=References>References</a><li><a href=#bio0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Vitae>Vitae</a></ol><button class="button button-anchor" aria-expanded=false data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type=button><span class=button-text>Show full outline</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class=PageDivider></div></div><div class=CitedBy id=toc-cited-by><h2 class=u-h4><a href=#section-cited-by>Cited By (46)</a></h2><div class=PageDivider></div></div><div class=Figures id=toc-figures><h2 class=u-h4>Figures (9)</h2><ol><li><a href=#f0005 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.1. Given the luminance channel of an image (a), the feature vector associated to…" src=data:null;base64, style=max-width:94px;max-height:164px></div></a><li><a href=#f0010 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.2. Laplacian distribution at varying of μ and b" src=data:null;base64, style=max-width:210px;max-height:164px></div></a><li><a href=#f0015 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.3. Average Laplacian distributions of the AC DCT coefficients considering the 8…" src=data:null;base64, style=max-width:169px;max-height:163px></div></a><li><a href=#f0020 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.4. 2-dimensional distributions (fitted with a Gaussian model) related to the…" src=data:null;base64, style=max-width:151px;max-height:164px></div></a><li><a href=#f0025 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.5. Final AC DCT frequencies considered for representing the context of the scene…" src=data:null;base64, style=max-width:162px;max-height:164px></div></a><li><a href=#f0030 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig.6. Hierarchical subdivision of the image" src=data:null;base64, style=max-width:219px;max-height:114px></div></a></ol><button class="button button-anchor" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type=button><span class=button-text>Show all figures</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class=PageDivider></div></div><div class=Tables id=toc-tables><h2 class=u-h4>Tables (13)</h2><ol class=u-padding-s-bottom><li><a href=#t0005 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Accuracy of scene context classification on the validation dataset."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;1</a><li><a href=#t0010 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="The different configurations of the proposed DCT-GIST image representation."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;2</a><li><a href=#t0015 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Results obtained by exploiting the proposed DCT-GIST representation with configuration (F) on the 8 Scene Context Dataset [12]. Columns correspond to the inferred classes."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;3</a><li><a href=#t0020 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Results obtained exploiting the GIST representation [12] on the 8 Scene Context Dataset. Columns correspond to the inferred classes."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;4</a><li><a href=#t0025 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Natural vs Man-made classification performances of the proposed DCT-GIST representation with configuration (F). Columns correspond to the inferred classes."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;5</a><li><a href=#t0030 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Open vs Closed classification performances considering the proposed DCT-GIST representation with configuration (F). Columns correspond to the inferred classes."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table&nbsp;6</a></ol><button class="button button-anchor" data-aa-button="sd:product:journal:article:type=menu:name=show-tables" type=button><span class=button-text>Show all tables</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class=PageDivider></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role=main lang=en><div class=Publication id=publication><div class="publication-brand u-show-from-sm"><a title="Go to Pattern Recognition on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/pattern-recognition><img class=publication-brand-image src=data:null;base64, alt=Elsevier></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id=publication-title><a class=publication-title-link title="Go to Pattern Recognition on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/pattern-recognition>Pattern Recognition</a></h2><div class=text-xs><a title="Go to table of contents for this volume/issue" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/pattern-recognition/vol/48/issue/4>Volume 48, Issue 4</a>, April 2015, Pages 1086-1100</div></div><div class="publication-cover u-show-from-sm"><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/pattern-recognition/vol/48/issue/4><img class=publication-cover-image src=data:null;base64, alt="Pattern Recognition"></a></div></div><h1 id=screen-reader-main-title class="Head u-font-serif u-h2 u-margin-s-ver"><span class=title-text>Representing scenes for real-time context classification on mobile devices</span></h1><div class=Banner id=banner><div class="wrapper truncated"><div class="AuthorGroups text-xs"><div class=author-group id=author-group><span class=sr-only>Author links open overlay panel</span><a class="author size-m workspace-trigger" name=bau0005 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X#!><span class=content><span class="text given-name">G.M.</span><span class="text surname">Farinella</span><span class=author-ref id=baff0005><sup>a</sup></span><svg focusable=false viewBox="0 0 106 128" width=19.875 height=24 class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau0010 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X#!><span class=content><span class="text given-name">D.</span><span class="text surname">Ravì</span><span class=author-ref id=baff0005><sup>a</sup></span><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau0015 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X#!><span class=content><span class="text given-name">V.</span><span class="text surname">Tomaselli</span><span class=author-ref id=baff0010><sup>b</sup></span><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau0020 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X#!><span class=content><span class="text given-name">M.</span><span class="text surname">Guarnera</span><span class=author-ref id=baff0010><sup>b</sup></span><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau0025 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003132031400199X#!><span class=content><span class="text given-name">S.</span><span class="text surname">Battiato</span><span class=author-ref id=baff0005><sup>a</sup></span><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a></div></div></div><button id=show-more-btn class="button show-hide-details button-primary" type=button data-aa-button=icon-expand><span class=button-text>Show more</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><button class="button toc-button button-anchor u-hide-from-lg u-margin-s-right sf-hidden" type=button><svg focusable=false viewBox="0 0 104 128" width=19.5 height=24 class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg></button><button class="button-link AddToMendeley button show-on-desktop button-link-primary" type=button><svg focusable=false viewBox="0 0 86 128" height=16 width=16 class="icon icon-plus"><path d="m48 58v-38h-1e1v38h-38v1e1h38v38h1e1v-38h38v-1e1z"></path></svg><span class=button-link-text>Add to Mendeley</span></button><div class="Social u-display-inline-block" id=social><div class="popover social-popover" id=social-popover><div id=popover-trigger-social-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 128 128" height=16 width=16 class="icon icon-share"><path d="m9e1 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm-66-36c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-6e1c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48l-28.42-15.28c0.58-1.98 0.9-4.04 0.9-6.2s-0.32-4.22-0.9-6.2l28.42-15.28c4.04 4.58 9.92 7.48 16.48 7.48 12.14 0 22-9.86 22-22s-9.86-22-22-22-22 9.86-22 22c0 1.98 0.28 3.9 0.78 5.72l-28.64 15.38c-4.02-4.34-9.76-7.1-16.14-7.1-12.14 0-22 9.86-22 22s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-0.5 1.84-0.78 3.76-0.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class=button-text>Share</span></button></div></div></div><div class="ExportCitation u-display-inline-block" id=export-citation><div class="popover export-citation-popover" id=export-citation-popover><div id=popover-trigger-export-citation-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 106 128" height=16 width=16 class="icon icon-cited-by-66"><path xmlns=http://www.w3.org/2000/svg d="m2 58.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78v-1e1c-25.9 0-44 15.12-44 36.78zm1e2 -26.78v-1e1c-25.9 0-44 15.12-44 36.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78z"></path></svg><span class=button-text>Cite</span></button></div></div></div></div></div><div class=ArticleIdentifierLinks id=article-identifier-links><a class=doi href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.patcog.2014.05.014 target=_blank rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.patcog.2014.05.014</a><a class=rights-and-content target=_blank rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S003132031400199X&amp;orderBeanReset=true">Get rights and content</a></div><section class=ReferencedArticles></section><section class=ReferencedArticles></section><div class=PageDivider></div><div class="Abstracts u-font-serif" id=abstracts><div class="abstract author-highlights" id=ab0010><div id=abs0010><h3 class="u-h4 u-margin-m-top u-margin-xs-bottom" id=sect0010>Highlights</h3><p id=sp0120><dl class=list><dt class=list-label>•<dd class=list-description><p id=p0005>A new image descriptor for scene context classification purpose.</p><dt class=list-label>•<dd class=list-description><p id=p0010>The descriptor is suitable for Image Generation Pipeline of single sensor devices.</p><dt class=list-label>•<dd class=list-description><p id=p0015>The descriptor is computed directly on compressed domain (JPEG).</p><dt class=list-label>•<dd class=list-description><p id=p0020>The descriptor is computed in realtime on platform with low computational resources.</p><dt class=list-label>•<dd class=list-description><p id=p0025>The extraction process does not need extra information (e.g., a visual vocabulary).</p></dl><p></p></div></div><div class="abstract author" id=ab0005><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id=abs0005><p id=sp0115><span>In this paper we introduce the DCT-GIST image representation model which is useful to summarize the context of the scene. The proposed image descriptor addresses the problem of real-time scene context <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification title="Learn more about classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification</a> on devices with limited memory and low computational resources (e.g., mobile and other single sensor devices such as wearable cameras). Images are holistically represented starting from the statistics collected in the Discrete Cosine Transform (DCT) domain. Since the </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/discrete-cosine-transform title="Learn more about DCT from ScienceDirect's AI-generated Topic Pages" class=topic-link>DCT</a><span><span> coefficients are usually computed within the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/digital-signal-processor title="Learn more about digital signal processor from ScienceDirect's AI-generated Topic Pages" class=topic-link>digital signal processor</a> for the JPEG conversion/storage, the proposed solution allows to obtain an instant and “free of charge” image signature. The novel image representation exploits the DCT coefficients of natural images by modelling them as </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/laplacian-distribution title="Learn more about Laplacian distributions from ScienceDirect's AI-generated Topic Pages" class=topic-link>Laplacian distributions</a><span> which are summarized by the scale parameter in order to capture the context of the scene. Only discriminative DCT frequencies corresponding to edges and textures are retained to build the descriptor of the image. A spatial hierarchy approach allows to collect the DCT statistics on image sub-regions to better encode the spatial envelope of the scene. The proposed image descriptor is coupled with a <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/support-vector-machine title="Learn more about Support Vector Machine from ScienceDirect's AI-generated Topic Pages" class=topic-link>Support Vector Machine</a><span> <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-machine-learning title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>classifier</a> for context recognition purpose. Experiments on the well-known 8 Scene Context Dataset as well as on the MIT-67 Indoor Scene dataset demonstrate that the proposed representation technique achieves better results with respect to the popular GIST descriptor, outperforming this last representation also in terms of computational costs. Moreover, the experiments pointed out that the proposed representation model closely matches other state-of-the-art methods based on bag of Textons collected on spatial hierarchy.</span></span></span></p></div></div></div><ul id=issue-navigation class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314003720><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class=button-alternative-text><strong>Previous </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span></a><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314003483><span class=button-alternative-text><strong>Next </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></ul><div class="Keywords u-font-serif"><div id=key0005 class=keywords-section><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id=key0010 class=keyword><span>Scene representation</span></div><div id=key0015 class=keyword><span>Scene classification</span></div><div id=key0020 class=keyword><span>Image descriptor</span></div><div id=key0025 class=keyword><span>GIST</span></div><div id=key0030 class=keyword><span>JPEG</span></div><div id=key0035 class=keyword><span>DCT features</span></div><div id=key0040 class=keyword><span>Mobile devices</span></div><div id=key0045 class=keyword><span>Wearable cameras</span></div></div></div><div class="Body u-font-serif" id=body><div><section id=s0005><h2 id=sect0020 class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction and motivations</h2><p id=p0030>Scene recognition is a key process of human vision which is exploited to efficiently and rapidly understand the context and objects in front of us. Humans are able to recognize complex visual scenes at a single glance, despite the number of objects with different poses, colors, shadows and textures that may be contained in the scenes. Seminal studies in computational vision <a name=bbib1 href=#bib1 class=workspace-trigger>[1]</a> have portrayed scene recognition as a progressive reconstruction of the input from local measurements (e.g., edges and surfaces). In contrast, some experimental studies have suggested that recognition of real-world scenes may be initiated from the encoding of the global configuration, bypassing most of the details about local concepts and objects information <a name=bbib2 href=#bib2 class=workspace-trigger>[2]</a>. This ability is achieved mainly by exploiting the holistic cues of scenes that can be processed as single entity over the entire human visual field without requiring attention to local features <a name=bbib3 href=#bib3 class=workspace-trigger>[3]</a>. Successive studies suggest that the humans rely on local as much as on global information to recognize the scene category <a name=bbib4 href=#bib4 class=workspace-trigger>[4]</a>, <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>.<p id=p0035><span><span>The recognition of the scene is a useful task for many relevant <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/computer-vision-applications title="Learn more about Computer Vision applications from ScienceDirect's AI-generated Topic Pages" class=topic-link>Computer Vision applications</a>: </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/robot-navigation title="Learn more about robot navigation from ScienceDirect's AI-generated Topic Pages" class=topic-link>robot navigation</a> systems </span><a name=bbib6 href=#bib6 class=workspace-trigger>[6]</a>, semantic organization of databases of digital pictures <a name=bbib7 href=#bib7 class=workspace-trigger>[7]</a>, content-based image retrieval (CBIR) <a name=bbib8 href=#bib8 class=workspace-trigger>[8]</a>, context driven focus attention and object priming <a name=bbib9 href=#bib9 class=workspace-trigger>[9]</a>, <a name=bbib10 href=#bib10 class=workspace-trigger>[10]</a>, and scene depths estimation <a name=bbib11 href=#bib11 class=workspace-trigger>[11]</a><span>. To build a scene recognition system, different considerations about the spatial envelope properties (e.g., degree of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/naturalness title="Learn more about naturalness from ScienceDirect's AI-generated Topic Pages" class=topic-link>naturalness</a> and degree of openness) and the level of description of the scene (e.g., subordinate, basic, and superordinate) have to be taken into account </span><a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>.<p id=p0040>The results reported in <a name=bbib13 href=#bib13 class=workspace-trigger>[13]</a><span> demonstrate that a context recognition engine is important for the tuning of color constancy algorithms used in the Imaging Generation Pipeline (IGP) and hence improve the quality of the final generated image. More in general, in the research area of single <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/imaging-sensor title="Learn more about sensor imaging from ScienceDirect's AI-generated Topic Pages" class=topic-link>sensor imaging</a> devices </span><a name=bbib14 href=#bib14 class=workspace-trigger>[14]</a><span><span><span><span>, the scene context information can be used to drive different tasks performed in the IGP during both acquisition time (e.g., <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/autofocus title="Learn more about autofocus from ScienceDirect's AI-generated Topic Pages" class=topic-link>autofocus</a>, auto-exposure, and white balance) and post-acquisition time (e.g., image enhancement and image coding). For example, the auto-scene mode of consumer and </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/wearable-camera title="Learn more about wearable cameras from ScienceDirect's AI-generated Topic Pages" class=topic-link>wearable cameras</a> could allow to automatically set the acquisition parameters improving the perceived quality of the captured image according to the recognized scene (e.g., Landscape and Portrait). Furthermore, context recognition could be functional for the automatic setting of surveillance cameras which are usually placed in different scene contexts (e.g., Indoor vs Outdoor scenes and Open vs Closed scenes), as well as in the application domain of </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/assistive-technology title="Learn more about assistive technologies from ScienceDirect's AI-generated Topic Pages" class=topic-link>assistive technologies</a> for visually impaired and blind people (e.g., indoor vs outdoor recognition with </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/smart-wearable title="Learn more about wearable smart from ScienceDirect's AI-generated Topic Pages" class=topic-link>wearable smart</a> glasses). The need for the development of effective solution for scene recognition systems to be embedded in consumer imaging devices (e.g., consumer digital cameras, smartphones, and wearable cameras) is confirmed by the growing interest of consumer devices industry which are including those capabilities in their products. Different constraints have to be considered in transferring the ability of scene recognition into the IGP of a single sensor imaging devices </span><a name=bbib15 href=#bib15 class=workspace-trigger>[15]</a>: memory limitation, low computational power, as well as the input data format to be used in scene recognition task (e.g., JPEG images).<div><p id=p0045>This paper presents a new computational model to represent the context of the scene based on the image statistics collected in the Discrete Cosine Transform (DCT) domain. We call DCT-GIST the proposed scene context descriptor. Since the DCT of the image acquired by a device is always computed for JPEG conversion/storage,<a name=bfn1 href=#fn1 class=workspace-trigger><sup>1</sup></a> the features extraction process useful to compute the signature of the scene context is “free of charge” for the IGP and can be performed in real-time independently from the computational power of the device. The rationale beyond the proposed image representation is that the distributions of the AC DCT coefficients (with respect to the different AC DCT basis) differ from one class of scene context to another and hence can be used to discriminate the context of scenes. The statistics of the AC DCT coefficients can be approximated by a Laplacian distribution <a name=bbib16 href=#bib16 class=workspace-trigger>[16]</a> almost centered at zero; we extract an image signature which encodes the statistics of the scene by considering the scales of Laplacian models fitted over the distribution of AC DCT coefficients of the image under consideration (see <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>). This signature computed on a spatial pyramid <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>, together with the information related to the colors obtained considering the DC components, is then used for the automatic scene context categorization.<figure class="figure text-xs" id=f0005><span><img src=data:null;base64, height=978 alt aria-describedby=cap0005><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr1_lrg.jpg target=_blank download title="Download high-res image (1MB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (1MB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr1.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0005><p id=sp0005><span class=label>Fig.&nbsp;1</span>. <span>Given the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/luminance-channel title="Learn more about luminance channel from ScienceDirect's AI-generated Topic Pages" class=topic-link>luminance channel</a><span> of an image (a), the feature vector associated to the context of the scene is obtained considering the statistics of the AC coefficients corresponding to the different AC <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/discrete-cosine-transform title="Learn more about DCT from ScienceDirect's AI-generated Topic Pages" class=topic-link>DCT</a> basis (b). For each AC frequency, the coefficients distribution is computed (c) and fitted with a Laplacian model&nbsp;(d). Each fitted Laplacian is characterized by a scale parameter related to the slope of the distribution. The final image signature is obtained collecting the scale parameters of the fitted Laplacians among the different AC DCT coefficient distributions. As specified in </span></span><a name=bs0020 href=#s0020 class=workspace-trigger>Section 4</a>, information on colors (i.e., DC components) as well as on the spatial arrangement of the DCT feature can be included to obtain a more discriminative representation. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</p></span></span></figure></div><p id=p0050>To reduce the computational complexity involved in the image representation extraction, only a subset of the DCT frequencies (summarizing edges and textures) are considered. To this purpose a supervised greedy based selection of the most discriminative frequencies is performed. To improve the discrimination power, the spatial envelope of the scene is encoded with a spatial hierarchy approach useful to collect the AC DCT statistics on image sub-regions <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a><span>. We have coupled the proposed image representation with a <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/support-vector-machine title="Learn more about Support Vector Machine from ScienceDirect's AI-generated Topic Pages" class=topic-link>Support Vector Machine</a><span> <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-machine-learning title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>classifier</a> for final context recognition purpose. The experiments performed on the 8 Scene Context Dataset </span></span><a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> as well as on the MIT-67 Indoor Scene dataset <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a> demonstrate that the proposed DCT-GIST representation achieves better results with respect to the popular GIST scene descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. Moreover, the novel image signature outperforms GIST in terms of computational costs. Finally, with the proposed image descriptor we obtain results comparable with other more complex state-of-the-art methods exploiting spatial pyramids <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> and combination of global and local information <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>.<p id=p0055>The primary contribution of this work is related to the new descriptor for scene context classification which we call DCT-GIST. We emphasize once again the fact that the proposed descriptor is built on information already available in the IGP of single sensor devices as well as in any image coded in JPEG format. Compared to many other scene descriptors extracted starting from RGB images <a name=bbib4 href=#bib4 class=workspace-trigger>[4]</a>, <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib13 href=#bib13 class=workspace-trigger>[13]</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>, <a name=bbib19 href=#bib19 class=workspace-trigger>[19]</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>[20]</a>, the proposed representation model has the following peculiarities/advantages:<dl class=list><dt class=list-label>•<dd class=list-description><p id=p0060>the decoding/decompression of JPEG is no needed to extract the scene signature;</p><dt class=list-label>•<dd class=list-description><p id=p0065>visual vocabularies have not to be computed and maintained in memory to represent both training and test images;</p><dt class=list-label>•<dd class=list-description><p id=p0070>the extraction of the scene descriptor does not need complex operation such as convolutions with bank of filters or <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/transformation-domain title="Learn more about domain transformations from ScienceDirect's AI-generated Topic Pages" class=topic-link>domain transformations</a> (e.g., FFT);</p><dt class=list-label>•<dd class=list-description><p id=p0075>there is no need of a supervised/unsupervised learning process to build the scene descriptor (e.g., there is no need of pre-labeled data and/or clustering procedure);</p><dt class=list-label>•<dd class=list-description><p id=p0080>it can be extracted directly into the Imaging Generation Pipeline of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/mobile-device title="Learn more about mobile devices from ScienceDirect's AI-generated Topic Pages" class=topic-link>mobile devices</a> with low computational resources;</p><dt class=list-label>•<dd class=list-description><p id=p0085>the recognition results closely match state-of-the-art methods cutting down the computational resources (e.g., computational time needed to compute the image representation).</p></dl><p><p id=p0090>The remainder of this paper is organized as follows: <a name=bs0010 href=#s0010 class=workspace-trigger>Section 2</a> briefly surveys the related works. <a name=bs0015 href=#s0015 class=workspace-trigger>Section 3</a> gives the background about the AC DCT coefficients distributions for different image categories. <a name=bs0020 href=#s0020 class=workspace-trigger>Section 4</a><span> presents the proposed image representation, whereas the new Image Generation <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/pipeline-architecture title="Learn more about Pipeline architecture from ScienceDirect's AI-generated Topic Pages" class=topic-link>Pipeline architecture</a> is described in </span><a name=bs0025 href=#s0025 class=workspace-trigger>Section 5</a>. <a name=bs0045 href=#s0045 class=workspace-trigger>Section 6</a> reports the details about the experimental settings and discusses the obtained results. Finally, <a name=bs0075 href=#s0075 class=workspace-trigger>Section 7</a> concludes the paper with hints for future works.</p></section><section id=s0010><h2 id=sect0025 class="u-h3 u-margin-l-top u-margin-xs-bottom">2. Related works</h2><p id=p0095>The visual content of the scene can be described with local or global representation models. A local based representation of the image usually describes the context of the scene as a collection of previously recognized objects/concepts within the scene, whereas a global (or holistic) representation of the scene context considers the scene as a single entity, bypassing the recognition of the constituting concepts (e.g., objects) in the final representation. The representation models can significantly differ for their capability of extracting and representing important information for the context description.<p id=p0100><span>Many <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/computervision title="Learn more about Computer Vision from ScienceDirect's AI-generated Topic Pages" class=topic-link>Computer Vision</a> researchers have proved that holistic approaches can be effectively used to solve the problem of rapid and automatic context recognition. Most of the holistic approaches share the same basic structure that can be schematically summarized as follows:</span><dl class=list><dt class=list-label>1.<dd class=list-description><p id=p0105>A suitable features space is considered (e.g., textons vocabularies <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>). This space must emphasize specific image cues such as corners, oriented edges, and textures.</p><dt class=list-label>2.<dd class=list-description><p id=p0110>Each image under consideration is projected into the considered feature space. A descriptor is built considering the image as a whole entity (e.g., textons distributions <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>).</p><dt class=list-label>3.<dd class=list-description><p id=p0115>Context recognition is obtained by using Pattern Recognition and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/machine-learning-algorithm title="Learn more about Machine Learning algorithms from ScienceDirect's AI-generated Topic Pages" class=topic-link>Machine Learning algorithms</a> on the computed representation of the images (e.g., by using K-nearest neighbors and SVM).</p></dl><span>A wide class of techniques based on the above scheme, works extracting features on perceptually <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/uniform-color-space title="Learn more about uniform color spaces from ScienceDirect's AI-generated Topic Pages" class=topic-link>uniform color spaces</a><span> (e.g., CIELab). Typically, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/filter-banks title="Learn more about filter banks from ScienceDirect's AI-generated Topic Pages" class=topic-link>filter banks</a> </span></span><a name=bbib19 href=#bib19 class=workspace-trigger>[19]</a>, <a name=bbib21 href=#bib21 class=workspace-trigger>[21]</a> or local invariant descriptors <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>[20]</a> are employed to capture image cues and to build the visual vocabulary to be used in a bag of visual words model <a name=bbib22 href=#bib22 class=workspace-trigger>[22]</a>. An image is considered as a distribution of visual words and this holistic representation is used for classification purposes. Spatial information have been also exploited in order to capture the layout of the visual words within images <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>, <a name=bbib23 href=#bib23 class=workspace-trigger>[23]</a>. A review of some other state-of-the-art methods working with features extracted on spatial domain can be found in <a name=bbib24 href=#bib24 class=workspace-trigger>[24]</a>.<p><p id=p0120><span>On the other hand, different approaches have considered the frequency domain as an useful and effective source of information to holistically encode an image for scene classification. The statistics of natural images on frequency domain reveal that there are different <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/spectral-signature title="Learn more about spectral signatures from ScienceDirect's AI-generated Topic Pages" class=topic-link>spectral signatures</a> for different image categories </span><a name=bbib25 href=#bib25 class=workspace-trigger>[25]</a>. In particular by considering the shape of the FFT spectrum of an image it is possible to address scene category <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib25 href=#bib25 class=workspace-trigger>[25]</a>, <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a>, scene depth <a name=bbib11 href=#bib11 class=workspace-trigger>[11]</a>, and object priming such as identity, scale and location <a name=bbib10 href=#bib10 class=workspace-trigger>[10]</a>.<p id=p0125>As suggested by different studies in computational vision, scene recognition may be initiated from the encoding of the global configuration of the scene, disregarding details and object information. Inspired by this knowledge, Torralba and Oliva <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a> have introduced computational procedures to extract the global structural information of complex natural scenes looking at the frequency domain <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib25 href=#bib25 class=workspace-trigger>[25]</a>, <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a>. The computational model presented in <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a><span><span> works in the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/fourier-transform title="Learn more about Fourier domain from ScienceDirect's AI-generated Topic Pages" class=topic-link>Fourier domain</a> where Discriminant Structural Templates (DSTs) are built using the power spectrum. A DST is a weighting scheme over the power spectrum that assigns positive values to the frequencies that are representative for one class and negative for the others. In particular the sign of the DST values indicates the correlation between the </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/spectral-component title="Learn more about spectral components from ScienceDirect's AI-generated Topic Pages" class=topic-link>spectral components</a> and the “spatial envelope” properties of the two groups to be distinguished. When the task is to discriminate between two kinds of scenes (e.g., </span><em>Natural</em> vs. <em>Artificial</em><span>) a suitable DST is built and used for the classification. A DST is learned in a supervised way using <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/linear-discriminant-analysis title="Learn more about Linear Discriminant Analysis from ScienceDirect's AI-generated Topic Pages" class=topic-link>Linear Discriminant Analysis</a><span>. The classification of a new image is hence performed by the sign of the correlation between the power spectrum of the considered image and the DST. A relevant issue in building a DST is the sampling of the power spectrum both at the learning and classification stages (a bank of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/gabor-filter title="Learn more about Gabor filters from ScienceDirect's AI-generated Topic Pages" class=topic-link>Gabor filters</a> with different frequencies and orientation is used in </span></span><a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a>). The final classification is performed on the Principal Components of the sampled frequencies. The improved version of the DST descriptor is called GIST <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib25 href=#bib25 class=workspace-trigger>[25]</a>. Oliva and Torralba <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a><span><span> performed tests using GIST on a dataset containing pictures of 8 different environmental scenes covering a large variety of outdoor places obtaining good performances. The GIST descriptor is nowadays one of the most used representation to encode the scene as whole. It has been used in many <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/computer-vision-applications title="Learn more about Computer Vision application from ScienceDirect's AI-generated Topic Pages" class=topic-link>Computer Vision application</a> domains such as </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/robot-navigation title="Learn more about robot navigation from ScienceDirect's AI-generated Topic Pages" class=topic-link>robot navigation</a> </span><a name=bbib6 href=#bib6 class=workspace-trigger>[6]</a>, visual interestingness <a name=bbib27 href=#bib27 class=workspace-trigger>[27]</a><span>, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-retrieval title="Learn more about image retrieval from ScienceDirect's AI-generated Topic Pages" class=topic-link>image retrieval</a> </span><a name=bbib28 href=#bib28 class=workspace-trigger>[28]</a>, and video summarization <a name=bbib29 href=#bib29 class=workspace-trigger>[29]</a>.<p id=p0130>Luo and Boutell <a name=bbib30 href=#bib30 class=workspace-trigger>[30]</a> built on previous works of Torralba and Oliva <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a><span> and proposed to use <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/independent-component-analysis title="Learn more about Independent Component Analysis from ScienceDirect's AI-generated Topic Pages" class=topic-link>Independent Component Analysis</a> rather than PCA for features extraction. In addition they have combined the camera metadata related to the image capture conditions with the information provided by the power spectra to perform the final classification.</span><p id=p0135>Farinella et al. <a name=bbib31 href=#bib31 class=workspace-trigger>[31]</a> proposed to exploit features extracted by ordering the Discrete Fourier Power Spectra (DFPS) to capture the <span><em><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/naturalness title="Learn more about naturalness from ScienceDirect's AI-generated Topic Pages" class=topic-link>naturalness</a></em></span> of scenes. By ordering the DFPS the overall “shape” of the scene in frequency domain is captured. In particular the frequencies that better capture the differences in the energy “shapes” related to <em>Natural</em> and <em>Artificial</em> categories are selected and ordered by their response values in the Discrete Fourier power spectrum. In this way a “ranking number” (corresponding to the relative position in the ordering) is assigned to each discriminative frequency. The vector of the response values and the vector of the relative positions in the ordering of the discriminative frequencies are then used singularly or in combination to provide a holistic representation of the scene. The representation was used with a probabilistic model for <em>Natural</em> vs <em>Artificial</em> scene classification.<p id=p0140>The Discrete Cosine Transform (DCT) domain was explored by Farinella and Battiato <a name=bbib15 href=#bib15 class=workspace-trigger>[15]</a> to build histograms of local dominant orientations to be used as scene representation at the abstract level of description (e.g., <em>Natural</em> vs <em>Artificial</em> and <em>Indoor</em> vs <em>Outdoor</em>). The representation is built collecting the information about orientation and strength of the edges related to the JPEG image blocks <a name=bbib7 href=#bib7 class=workspace-trigger>[7]</a><span>. This representation was coupled with a logistic <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-machine-learning title="Learn more about classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>classifier</a> to discriminate between the different scene contexts.</span><p id=p0145><span>The aforementioned techniques disregard the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/spatial-layout title="Learn more about spatial layout from ScienceDirect's AI-generated Topic Pages" class=topic-link>spatial layout</a> of the discriminative frequencies. Seminal studies proposed by Torralba et al. </span><a name=bbib9 href=#bib9 class=workspace-trigger>[9]</a>, <a name=bbib10 href=#bib10 class=workspace-trigger>[10]</a>, <a name=bbib11 href=#bib11 class=workspace-trigger>[11]</a><span> have proposed to further look at the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/spatial-frequency title="Learn more about spatial frequency from ScienceDirect's AI-generated Topic Pages" class=topic-link>spatial frequency</a> layout to address more specific vision tasks by exploiting contextual information (e.g., object detection and recognition, and scene depth estimation).</span></p></section><section id=s0015><h2 id=sect0030 class="u-h3 u-margin-l-top u-margin-xs-bottom">3. The statistics of natural image categories in DCT domain</h2><p id=p0150><span>One of the most popular standard for <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/lossy-compression title="Learn more about lossy compression from ScienceDirect's AI-generated Topic Pages" class=topic-link>lossy compression</a> of images is the JPEG </span><a name=bbib32 href=#bib32 class=workspace-trigger>[32]</a>. The JPEG compression is available in every IGP of single sensor consumer devices such as digital consumer cameras, smartphones and wearable cameras (e.g., smart glasses). Moreover, most of the images on Internet (e.g., in social networks and websites) are stored in JPEG format. Nowadays, around 70% of the total images on the top 10 million websites are in JPEG format.<a name=bfn2 href=#fn2 class=workspace-trigger><sup>2</sup></a> Taking into account these facts, a scene context descriptor that can be efficiently extracted in the IGP and/or directly in the JPEG compressed domain is desirable.<p id=p0155><span>The JPEG algorithm splits the image into non-overlapping blocks of size 8×8 pixels and each block is then processed with the Discrete Cosine Transform (DCT) before <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/quantisation title="Learn more about quantization from ScienceDirect's AI-generated Topic Pages" class=topic-link>quantization</a> and entropy coding </span><a name=bbib32 href=#bib32 class=workspace-trigger>[32]</a>. The DCT has been studied by many researchers which have proposed different models for the distributions of the DCT coefficients. One of the first conjecture was that the AC coefficients have Gaussian distributions <a name=bbib33 href=#bib33 class=workspace-trigger>[33]</a>. Different other possible distributions of the coefficients have also been proposed, including Cauchy, generalized Gaussian, as well as a sum of Gaussians <a name=bbib34 href=#bib34 class=workspace-trigger>[34]</a>, <a name=bbib35 href=#bib35 class=workspace-trigger>[35]</a>, <a name=bbib36 href=#bib36 class=workspace-trigger>[36]</a>, <a name=bbib37 href=#bib37 class=workspace-trigger>[37]</a>, <a name=bbib38 href=#bib38 class=workspace-trigger>[38]</a><span><span>. The knowledge about the mathematical form of the statistical distribution of the DCT coefficient is useful for the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/quantizer-design title="Learn more about quantizer design from ScienceDirect's AI-generated Topic Pages" class=topic-link>quantizer design</a> and noise mitigation for image enhancement. Although methods to extract features directly from JPEG compressed domain have been presented in the literature in the application context of </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-retrieval title="Learn more about image retrieval from ScienceDirect's AI-generated Topic Pages" class=topic-link>image retrieval</a> </span><a name=bbib39 href=#bib39 class=workspace-trigger>[39]</a>, <a name=bbib40 href=#bib40 class=workspace-trigger>[40]</a>, at the best of our knowledge there are not works where the DCT coefficients distributions are exploited for scene classification. The proposed image representation is inspired by the works of Lam <a name=bbib16 href=#bib16 class=workspace-trigger>[16]</a>, <a name=bbib41 href=#bib41 class=workspace-trigger>[41]</a><span>, where the semantic content of the images has been characterized in terms of DCT distributions modelled as Laplacian and generalized <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/gaussian-model title="Learn more about Gaussian models from ScienceDirect's AI-generated Topic Pages" class=topic-link>Gaussian models</a>.</span><div><p id=p0160>After performing the DCT on each 8×8 block of an image and collecting the corresponding coefficients to the different AC basis of the DCT, a simple observation of the distribution indicates that they resemble a Laplacian (see <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(c)). This guess has been demonstrated through a rigorous mathematical analysis in <a name=bbib16 href=#bib16 class=workspace-trigger>[16]</a><span>. The <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/probability-density-function title="Learn more about probability density function from ScienceDirect's AI-generated Topic Pages" class=topic-link>probability density function</a> of a Laplacian distribution can be written as</span><span class=display><span id=eq0005 class=formula><span class=label>(1)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-44-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">f</mi><mo is="true">(</mo><mi is="true">x</mi><mo stretchy="false" is="true">|</mo><mi is="true">&amp;#x3BC;</mi><mo is="true">,</mo><mi is="true">b</mi><mo is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">b</mi></mrow></mfrac><mspace width="0.25em" is="true" /><mi is="true">exp</mi><mo stretchy="true" is="true">(</mo><mrow is="true"><mo is="true">&amp;#x2212;</mo><mfrac is="true"><mrow is="true"><mo is="true">|</mo><mi is="true">x</mi><mo is="true">&amp;#x2212;</mo><mi is="true">&amp;#x3BC;</mi><mo is="true">|</mo></mrow><mrow is="true"><mi is="true">b</mi></mrow></mfrac></mrow><mo stretchy="true" is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=28.603ex height=4.625ex viewBox="0 -1244 12315.2 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-66></use></g><use href=#MJSZ2-28 is=true x=550 y=-1></use><g is=true transform=translate(1148,0)><use href=#MJMATHI-78></use></g><g is=true transform=translate(1720,0)><use href=#MJMAIN-7C></use></g><g is=true transform=translate(1999,0)><use href=#MJMATHI-3BC></use></g><g is=true transform=translate(2602,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3047,0)><use href=#MJMATHI-62></use></g><use href=#MJSZ2-29 is=true x=3477 y=-1></use><g is=true transform=translate(4352,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5130,0)><g transform=translate(397,0)><rect stroke=none width=777 height=60 x=0 y=220></rect><g is=true transform=translate(211,403)><g is=true><use transform=scale(0.707) href=#MJMAIN-31></use></g></g><g is=true transform=translate(60,-395)><g is=true><use transform=scale(0.707) href=#MJMAIN-32></use></g><g is=true transform=translate(353,0)><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g></g><g is=true></g><g is=true transform=translate(6676,0)><use href=#MJMAIN-65></use><use href=#MJMAIN-78 x=444 y=0></use><use href=#MJMAIN-70 x=973 y=0></use></g><use href=#MJSZ2-28 is=true x=8205 y=-1></use><g is=true transform=translate(8803,0)><g is=true><use href=#MJMAIN-2212></use></g><g is=true transform=translate(778,0)><g transform=translate(120,0)><rect stroke=none width=1895 height=60 x=0 y=220></rect><g is=true transform=translate(60,579)><g is=true><use transform=scale(0.707) href=#MJMAIN-7C></use></g><g is=true transform=translate(196,0)><use transform=scale(0.707) href=#MJMATHI-78></use></g><g is=true transform=translate(601,0)><use transform=scale(0.707) href=#MJMAIN-2212></use></g><g is=true transform=translate(1152,0)><use transform=scale(0.707) href=#MJMATHI-3BC></use></g><g is=true transform=translate(1578,0)><use transform=scale(0.707) href=#MJMAIN-7C></use></g></g><g is=true transform=translate(796,-395)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g></g></g><use href=#MJSZ2-29 is=true x=11717 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>f</mi><mo is=true>(</mo><mi is=true>x</mi><mo stretchy=false is=true>|</mo><mi is=true>μ</mi><mo is=true>,</mo><mi is=true>b</mi><mo is=true>)</mo><mo is=true>=</mo><mfrac is=true><mrow is=true><mn is=true>1</mn></mrow><mrow is=true><mn is=true>2</mn><mi is=true>b</mi></mrow></mfrac><mspace width=0.25em is=true></mspace><mi is=true>exp</mi><mo stretchy=true is=true>(</mo><mrow is=true><mo is=true>−</mo><mfrac is=true><mrow is=true><mo is=true>|</mo><mi is=true>x</mi><mo is=true>−</mo><mi is=true>μ</mi><mo is=true>|</mo></mrow><mrow is=true><mi is=true>b</mi></mrow></mfrac></mrow><mo stretchy=true is=true>)</mo></math></span></span></span></span></span>where <em>μ</em> is the location parameter and <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-45-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">b</mi><mo is="true">&amp;#x2265;</mo><mn is="true">0</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=5.258ex height=2.202ex viewBox="0 -747.2 2264.1 947.9" role=img focusable=false style=vertical-align:-0.466ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-62></use></g><g is=true transform=translate(707,0)><use href=#MJMAIN-2265></use></g><g is=true transform=translate(1763,0)><use href=#MJMAIN-30></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>b</mi><mo is=true>≥</mo><mn is=true>0</mn></math></span></span></span> is the scale parameter. <a name=bf0010 href=#f0010 class=workspace-trigger>Fig. 2</a> reports examples of different Laplacian distributions. At varying of the scale parameter, the Laplacian distribution changes its shape. Given <em>N</em> samples {<em>x</em><sub>1</sub>,…,<em>x</em><sub><em>N</em></sub>}, the parameters <em>μ</em> and <em>b</em> can be simply estimated with the maximum likelihood estimator <a name=bbib42 href=#bib42 class=workspace-trigger>[42]</a>. Specifically, <em>μ</em> corresponds to the median of the samples,<a name=bfn3 href=#fn3 class=workspace-trigger><sup>3</sup></a> whereas <em>b</em> is computed as follows:<span class=display><span id=eq0010 class=formula><span class=label>(2)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-46-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">b</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></mfrac><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></munderover><mo is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">&amp;#x2212;</mo><mi is="true">&amp;#x3BC;</mi><mo is="true">|</mo><mo is="true">.</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=20.832ex height=3.702ex viewBox="0 -1045.3 8969.1 1593.7" role=img focusable=false style=vertical-align:-1.274ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-62></use></g><g is=true transform=translate(707,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1485,0)><g transform=translate(397,0)><rect stroke=none width=748 height=60 x=0 y=220></rect><g is=true transform=translate(197,403)><g is=true><use transform=scale(0.707) href=#MJMAIN-31></use></g></g><g is=true transform=translate(60,-387)><g is=true><use transform=scale(0.707) href=#MJMATHI-4E></use></g></g></g></g><g is=true transform=translate(2918,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><use transform=scale(0.707) href=#MJMATHI-4E></use></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(5390,961)><use href=#MJMAIN-2223 x=0 y=-751></use><use href=#MJMAIN-2223 x=0 y=-1174></use></g><g is=true transform=translate(5668,0)><g is=true><g is=true><use href=#MJMATHI-78></use></g></g><g is=true transform=translate(572,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g></g></g><g is=true transform=translate(6807,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(7808,0)><use href=#MJMATHI-3BC></use></g><g is=true transform=translate(8412,961)><use href=#MJMAIN-2223 x=0 y=-751></use><use href=#MJMAIN-2223 x=0 y=-1174></use></g><g is=true transform=translate(8690,0)><use href=#MJMAIN-2E></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>b</mi><mo is=true>=</mo><mfrac is=true><mrow is=true><mn is=true>1</mn></mrow><mrow is=true><mi is=true>N</mi></mrow></mfrac><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><mi is=true>N</mi></mrow></munderover><mo is=true>|</mo><msub is=true><mrow is=true><mi is=true>x</mi></mrow><mrow is=true><mi is=true>i</mi></mrow></msub><mo is=true>−</mo><mi is=true>μ</mi><mo is=true>|</mo><mo is=true>.</mo></math></span></span></span></span></span><figure class="figure text-xs" id=f0010><span><img src=data:null;base64, height=257 alt aria-describedby=cap0010><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr2_lrg.jpg target=_blank download title="Download high-res image (158KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (158KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr2.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0010><p id=sp0010><span class=label>Fig.&nbsp;2</span>. <span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/laplacian-distribution title="Learn more about Laplacian distribution from ScienceDirect's AI-generated Topic Pages" class=topic-link>Laplacian distribution</a> at varying of </span><em>μ</em> and <em>b</em>.</p></span></span></figure></div><div><p id=p0165>The rationale beyond the proposed representation for scene context classification is that the context of different classes of scenes differs in the scales of the AC DCT coefficient distributions. Hence, to represent the context of the scene we can use the feature vector of the scales of the AC DCT coefficients distributions of an image after a Laplacian fitting. <a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a> reports the average “shapes” of the AC DCT coefficients Laplacian distributions related to the 8 Scene Context Dataset <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. The dataset contains 2600 color images (256×256 pixels) belonging to the following 8 outdoor scene categories: <em>coast</em>, <em>mountain</em>, <em>forest</em>, <em>open country</em>, <em>street</em>, <em>inside city</em>, <span><em><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/tallest-building title="Learn more about tall buildings from ScienceDirect's AI-generated Topic Pages" class=topic-link>tall buildings</a></em></span>, <em>highways</em>. The Laplacian shapes in <a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a><span> are computed by fitting the Laplacian distributions for the different AC DCT coefficients of the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/luminance-channel title="Learn more about luminance channel from ScienceDirect's AI-generated Topic Pages" class=topic-link>luminance channel</a> of each image and then averaging the Laplacian parameters with respect to the 8 different classes (color coded in </span><a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a>). A simple observation of the slopes of the different Laplacian distributions (corresponding to the <em>b</em> parameter) is useful to better understand the rationale beyond the proposed scene descriptor. The slopes related to the different classes are captured by the <em>b</em> parameters computed (with low computational cost) from the images directly encoded in the DCT domain (i.e., JPEG format). The guess is that the multidimensional space of the <em>b</em> parameters is discriminative enough for scene context recognition. Although it is difficult to visualize the <em>N</em>-dimensional distributions of the <em>b</em> parameters, an intuition of the discriminativeness of the space can be obtained by considering two AC DCT frequencies and plotting the 2-dimensional distributions of the related Laplacian parameters. <a name=bf0020 href=#f0020 class=workspace-trigger>Fig. 4</a> shows the 2-dimensional distributions obtained by considering two DCT frequencies corresponding to the DCT basis (0,1) and (1,0) which are useful to reconstruct the vertical/horizontal edges of each image block (see <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(b)). As the figure points out, already considering only two AC DCT frequencies there is a good separation among the eight different classes. The experiments reported in <a name=bs0045 href=#s0045 class=workspace-trigger>Section 6</a> quantitatively confirm the above rationale.<figure class="figure text-xs" id=f0015><span><img src=data:null;base64, height=669 alt aria-describedby=cap0015><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr3_lrg.jpg target=_blank download title="Download high-res image (1MB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (1MB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr3.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0015><p id=sp0015><span class=label>Fig.&nbsp;3</span>. <span><span>Average <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/laplacian-distribution title="Learn more about Laplacian distributions from ScienceDirect's AI-generated Topic Pages" class=topic-link>Laplacian distributions</a> of the AC </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/discrete-cosine-transform title="Learn more about DCT from ScienceDirect's AI-generated Topic Pages" class=topic-link>DCT</a> coefficients considering the 8 Scene Context Dataset </span><a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. The different scene classes are color coded. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</p></span></span></figure><figure class="figure text-xs" id=f0020><span><img src=data:null;base64, height=552 alt aria-describedby=cap0020><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr4_lrg.jpg target=_blank download title="Download high-res image (1MB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (1MB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr4.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0020><p id=sp0020><span class=label>Fig.&nbsp;4</span>. 2-dimensional distributions (fitted with a Gaussian model) related to the Laplacian distribution parameters of the DCT frequency (0,1) and (1,0) in <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(b).</p></span></span></figure></div></section><section id=s0020><h2 id=sect0035 class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Proposed DCT-GIST image representation</h2><p id=p0170>In this section we formalize the proposed image representation which builds on the main rationale that different scene classes have different AC DCT coefficient distributions (see <a name=bs0015 href=#s0015 class=workspace-trigger>Section 3</a>). <a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a> shows the average of the AC DCT coefficient distributions after a Laplacian fitting on images belonging to different scene contexts. Differences in the slopes of the Laplacian distributions are evident and related to the different classes. As a consequence of this observation, we propose to encode the scene context by concatenating all the Laplacian parameters related to the median and slope (<em>μ</em> and <em>b</em>) which are computed by considering the different AC DCT coefficients distributions of the luminance channel of the image.<a name=bfn4 href=#fn4 class=workspace-trigger><sup>4</sup></a> In addition to these information, the mean and variance of the DC coefficients can be also included into the feature vector to capture the color information, as well as the AC DCT Laplacian distributions parameters obtained considering the <em>C</em><sub><em>b</em></sub> and <em>C</em><sub><em>r</em></sub> channels.<a name=bfn5 href=#fn5 class=workspace-trigger><sup>5</sup></a> In <a name=bs0045 href=#s0045 class=workspace-trigger>Section 6</a> we show the contribution of each component involved in the proposed DCT-GIST image descriptor.<p id=p0175>The aforementioned image features are extracted in the IGP just after the image acquisition step, without any extra complex processing. Specifically, the Laplacian parameters related to the AC DCT coefficients are obtained collecting the AC DCT coefficients inside the JPEG encoding module performed before the image storage. In case the image is already stored in JPEG (e.g., a picture from the web), the information useful for scene context representation can be directly collected in the compressed domain without any further processing. Indeed, to build the scene descriptor in the DCT compressed domain, only simple operations (i.e., the median and the mean absolute deviations from the median) are needed to compute <em>μ</em> and <em>b</em><span> for the different image channels, as well as to compute the mean and variance on the DC components. This cuts down the computational complexity with respect to other descriptors which usually involve <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/convolution-operation title="Learn more about convolution operations from ScienceDirect's AI-generated Topic Pages" class=topic-link>convolution operations</a> (e.g., with bank of filters </span><a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a><span> or <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/gaussian-kernel title="Learn more about Gaussian Kernels from ScienceDirect's AI-generated Topic Pages" class=topic-link>Gaussian Kernels</a> </span><a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>) or other more complex pipelines (e.g., Bag of Words representation <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>) to build the final scene context representation.<p id=p0180>It is well-known that some of the DCT basis are related to the reconstruction of the edges of an 8×8 image block (i.e., first row and first column of <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(b)), whereas the others are more related to the reconstruction of the textured blocks. As shown in <a name=bbib43 href=#bib43 class=workspace-trigger>[43]</a> the most prominent patterns composing natural images are the edges. High frequencies are usually affected by noise and could be not really useful for discriminating the context of a scene. For this reason we have performed an analysis to understand which of the AC DCT basis can give a real contribution to discriminate between different classes of scenes. One more motivation to select only the most discriminative AC DCT frequencies is the reduction of the complexity of the overall system.<div><p id=p0185>To properly select the AC DCT frequencies to be employed in the final image representation, we have collected (from Flickr) and labelled a set of 847 uncompressed images to be used as validation set. These images belong to the eight different classes of scene context <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> (see <a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a><span><span>) and have variable size (max size 6000×4000, min size 800×600). We used uncompressed images to avoid that the selection processes of the most discriminative frequencies could be biased by the JPEG <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/quantization-step title="Learn more about quantization step from ScienceDirect's AI-generated Topic Pages" class=topic-link>quantization step</a>. On this dataset we have performed scene context classification by representing images through the Laplacian fitting of a single AC DCT basis. This step has been repeated for each AC DCT basis. A greedy fashion approach has been hence employed to select the most discriminative frequencies. This means that as first round the classification has been performed for all the AC DCT basis separately. The images have been hence classified after performing the learning of a </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/support-vector-machine title="Learn more about support vector machine from ScienceDirect's AI-generated Topic Pages" class=topic-link>support vector machine</a><span>. A leave one out modality has been used to evaluate the discriminativeness of each AC DCT basis. Then we have selected the most discriminative frequency and we have performed another round of learning and classification considering the selected frequency coupled with one of the remaining frequencies in order to jointly consider two AC DCT basis. This procedure has been recursively repeated to greedily select frequencies. The experiments on the validation set suggested that a good trade-off between context <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-accuracy title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification accuracy</a> and computational complexity (i.e., the number of AC DCT frequencies to be included in a real IGP to fit with required computational time and memory resources) is the one which considers the AC DCT frequencies marked in red in </span></span><a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a>. Let <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-47-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">D</mi><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.528ex height=2.779ex viewBox="0 -846.5 2810.7 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-44></use></g><g is=true transform=translate(828,0)><use href=#MJMAIN-28></use></g><g is=true transform=translate(1218,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(1563,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2008,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(2421,0)><use href=#MJMAIN-29></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>D</mi><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo></math></span></span></span> , <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-48-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.404ex height=2.317ex viewBox="0 -747.2 4910.1 997.6" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-69></use></g><g is=true transform=translate(623,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1679,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2180,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2625,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(3964,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4409,0)><use href=#MJMAIN-37></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-49-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.586ex height=2.432ex viewBox="-11.5 -747.2 4988.6 1047.3" role=img focusable=false style=vertical-align:-0.697ex;margin-left:-0.027ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6A></use></g><g is=true transform=translate(690,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1746,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2247,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2692,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(4031,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4476,0)><use href=#MJMAIN-37></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>j</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn></math></span></span></span>, be the DCT components corresponding to the 2D DCT basis (<em>i</em>,<em>j</em>) in <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(b). The final set of the selected AC DCT basis in <a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a> is defined as<span class=display><span id=eq0015 class=formula><span class=label>(3)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-50-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">F</mi><mo is="true">=</mo><mo is="true">{</mo><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo><mo is="true">|</mo><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn><mo is="true">}</mo><mo is="true">&amp;#x22C3;</mo><mo is="true">{</mo><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mn is="true">1</mn><mo is="true">)</mo><mo is="true">|</mo><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">3</mn><mo is="true">}</mo><mo is="true">&amp;#x22C3;</mo><mo is="true">{</mo><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">|</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn><mo is="true">}</mo><mo is="true">&amp;#x22C3;</mo><mo is="true">{</mo><mo is="true">(</mo><mn is="true">1</mn><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">|</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">3</mn><mo is="true">}</mo><mo is="true">&amp;#x22C3;</mo><mo is="true">{</mo><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">|</mo><mi is="true">i</mi><mo is="true">=</mo><mn is="true">0</mn><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn><mo is="true">;</mo><mi is="true">j</mi><mo is="true">=</mo><mn is="true">7</mn><mo is="true">&amp;#x2212;</mo><mi is="true">i</mi><mo is="true">}</mo><mo is="true">.</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=67.348ex height=6.009ex viewBox="0 -846.5 28997.1 2587.3" role=img focusable=false style=vertical-align:-4.043ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-46></use></g><g is=true transform=translate(1027,0)><use href=#MJMAIN-3D></use></g><use href=#MJMAIN-7B is=true x=2083 y=0></use><use href=#MJMAIN-28 is=true x=2584 y=0></use><g is=true transform=translate(2973,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(3319,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3764,0)><use href=#MJMAIN-30></use></g><use href=#MJMAIN-29 is=true x=4264 y=0></use><use href=#MJMAIN-7C is=true x=4654 y=-1></use><g is=true transform=translate(4932,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(5556,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(6612,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(7112,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(7557,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(8897,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9342,0)><use href=#MJMAIN-37></use></g><use href=#MJMAIN-7D is=true x=9842 y=0></use><g is=true transform=translate(10509,-1)><use href=#MJSZ1-22C3></use></g><use href=#MJMAIN-7B is=true x=11343 y=0></use><use href=#MJMAIN-28 is=true x=11843 y=0></use><g is=true transform=translate(12233,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(12578,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(13024,0)><use href=#MJMAIN-31></use></g><use href=#MJMAIN-29 is=true x=13524 y=0></use><use href=#MJMAIN-7C is=true x=13914 y=-1></use><g is=true transform=translate(14192,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(14815,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(15872,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(16372,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(16817,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(18157,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(18602,0)><use href=#MJMAIN-33></use></g><use href=#MJMAIN-7D is=true x=19102 y=0></use><g is=true transform=translate(19769,-1)><use href=#MJSZ1-22C3></use></g><use href=#MJMAIN-7B is=true x=20603 y=0></use><use href=#MJMAIN-28 is=true x=21103 y=0></use><g is=true transform=translate(21493,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(21993,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(22439,0)><use href=#MJMATHI-6A></use></g><use href=#MJMAIN-29 is=true x=22851 y=0></use><use href=#MJMAIN-7C is=true x=23241 y=-1></use><g is=true transform=translate(23519,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(24209,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(25266,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(25766,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(26211,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(27550,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(27996,0)><use href=#MJMAIN-37></use></g><use href=#MJMAIN-7D is=true x=28496 y=0></use><g transform=translate(0,-1433)><g is=true><use href=#MJSZ1-22C3></use></g><use href=#MJMAIN-7B is=true x=833 y=0></use><use href=#MJMAIN-28 is=true x=1334 y=0></use><g is=true transform=translate(1723,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2224,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2669,0)><use href=#MJMATHI-6A></use></g><use href=#MJMAIN-29 is=true x=3081 y=0></use><use href=#MJMAIN-7C is=true x=3471 y=-1></use><g is=true transform=translate(3749,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(4439,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5496,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(5996,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(6441,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(7781,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(8226,0)><use href=#MJMAIN-33></use></g><use href=#MJMAIN-7D is=true x=8726 y=0></use><g is=true transform=translate(9393,-1)><use href=#MJSZ1-22C3></use></g><use href=#MJMAIN-7B is=true x=10227 y=0></use><use href=#MJMAIN-28 is=true x=10727 y=0></use><g is=true transform=translate(11117,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(11462,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(11908,0)><use href=#MJMATHI-6A></use></g><use href=#MJMAIN-29 is=true x=12320 y=0></use><use href=#MJMAIN-7C is=true x=12710 y=-1></use><g is=true transform=translate(12988,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(13611,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(14668,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(15335,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(16674,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(17119,0)><use href=#MJMAIN-37></use></g><g is=true transform=translate(17620,0)><use href=#MJMAIN-3B></use></g><g is=true transform=translate(18065,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(18755,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(19811,0)><use href=#MJMAIN-37></use></g><g is=true transform=translate(20534,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(21535,0)><use href=#MJMATHI-69></use></g><use href=#MJMAIN-7D is=true x=21880 y=0></use><g is=true transform=translate(22381,0)><use href=#MJMAIN-2E></use></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>F</mi><mo is=true>=</mo><mo is=true>{</mo><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo><mo is=true>|</mo><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn><mo is=true>}</mo><mo is=true>⋃</mo><mo is=true>{</mo><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mn is=true>1</mn><mo is=true>)</mo><mo is=true>|</mo><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>3</mn><mo is=true>}</mo><mo is=true>⋃</mo><mo is=true>{</mo><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>|</mo><mi is=true>j</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn><mo is=true>}</mo><mo is=true>⋃</mo><mo is=true>{</mo><mo is=true>(</mo><mn is=true>1</mn><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>|</mo><mi is=true>j</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>3</mn><mo is=true>}</mo><mo is=true>⋃</mo><mo is=true>{</mo><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>|</mo><mi is=true>i</mi><mo is=true>=</mo><mn is=true>0</mn><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn><mo is=true>;</mo><mi is=true>j</mi><mo is=true>=</mo><mn is=true>7</mn><mo is=true>−</mo><mi is=true>i</mi><mo is=true>}</mo><mo is=true>.</mo></math></span></span></span></span></span><figure class="figure text-xs" id=f0025><span><img src=data:null;base64, height=285 alt aria-describedby=cap0025><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr5_lrg.jpg target=_blank download title="Download high-res image (302KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (302KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr5.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0025><p id=sp0025><span class=label>Fig.&nbsp;5</span>. Final AC DCT frequencies considered for representing the context of the scene (marked in red). (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)</p></span></span></figure></div><div><p id=p0190><a name=bt0005 href=#t0005 class=workspace-trigger>Table 1</a><span> reports the accuracy obtained on the aforementioned <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/validation-dataset title="Learn more about validation dataset from ScienceDirect's AI-generated Topic Pages" class=topic-link>validation dataset</a> considering the Laplacian fitting of all the 63 AC DCT basis, as well as the results obtained considering the 25 selected basis in Eq. </span><a name=beq0015 href=#eq0015 class=workspace-trigger>(3)</a> (see <a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a>). Notice that the overall accuracy obtained with the only 25 selected AC DCT basis is higher than the one obtained by considering all the 63 AC DCT basis. This is due to the fact that high frequencies (i.e., the ones below the diagonal in <a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a>) could contain more noise information than the other frequencies, making confusion into the feature space.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0005><span class=captions><span id=cap0050><p id=sp0050><span class=label>Table&nbsp;1</span>. Accuracy of scene context classification on the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/validation-dataset title="Learn more about validation dataset from ScienceDirect's AI-generated Topic Pages" class=topic-link>validation dataset</a>.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Approach</strong><th scope=col class=align-left><em>Accuracy</em><tbody><tr class=valign-top><td class=align-left><em>All frequencies</em> (<em>63 AC DCT basis</em>)<td class=align-char>0.7410<tr class=valign-top><td class=align-left><em>Selected AC DCT basis</em> (<em>Eq.</em> <a name=beq0015 href=#eq0015 class=workspace-trigger>(3)</a>)<td class=align-char>0.7549<tr class=valign-top><td class=align-left><em>Selected AC DCT basis</em> (<em>Eq.</em> <a name=beq0015 href=#eq0015 class=workspace-trigger>(3)</a>) <em>and spatial hierarchy</em><td class=align-char>0.8233</table></div></div></div><div><p id=p0195><span><span>The scene context descriptor proposed so far, uses a global feature vector for describing an image by leaving out the information about the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/spatial-layout title="Learn more about spatial layout from ScienceDirect's AI-generated Topic Pages" class=topic-link>spatial layout</a> of the local features. The relative position of a </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/local-descriptor title="Learn more about local descriptor from ScienceDirect's AI-generated Topic Pages" class=topic-link>local descriptor</a> can help to disambiguate concepts that are similar in terms of local descriptor. For instance, the visual concepts “sky” and “sea” could be similar in terms of local descriptor, but they are typically different in terms of position within the scene. The relative position can be thought as the context in which a feature takes part with respect to the other features within an image. To encode information of the spatial layout of the scene, different pooling strategies have been proposed in literature </span><a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>. Building on our previous work <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> we have augmented the image representation discussed above by collecting the AC DCT distributions over a hierarchy of sub-regions. Specifically, the image is partitioned using three different modalities: horizontal, vertical and regular grid. These schemes are recursively applied to obtain a hierarchy of sub-regions as shown in <a name=bf0030 href=#f0030 class=workspace-trigger>Fig. 6</a>. For each sub-region at each resolution level, the Laplacian parameters (<em>μ</em> and <em>b</em>) over the selected AC DCT coefficients are computed and concatenated to compose the feature vector, thus introducing spatial information. As in <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> we have used three levels in the hierarchy. The integral imaging approach <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib44 href=#bib44 class=workspace-trigger>[44]</a> is exploited to efficiently compute the Laplacian parameters of the different AC DCT coefficients. The accuracy obtained on the aforementioned validation set, by considering the spatial hierarchy based representation was 0.8233%, improving the previous result of more than 6% (see <a name=bt0005 href=#t0005 class=workspace-trigger>Table 1</a>).<figure class="figure text-xs" id=f0030><span><img src=data:null;base64, height=264 alt aria-describedby=cap0030><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr6_lrg.jpg target=_blank download title="Download high-res image (134KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (134KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr6.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0030><p id=sp0030><span class=label>Fig.&nbsp;6</span>. Hierarchical subdivision of the image.</p></span></span></figure></div><p id=p0200>We can formalize the proposed DCT-GIST scene descriptor as following. Let <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-51-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3ex height=2.432ex viewBox="0 -945.9 1291.5 1047.3" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> be a sub-region of the image under consideration at level <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-52-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">l</mi><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">1</mn><mo is="true">,</mo><mn is="true">2</mn><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.414ex height=2.779ex viewBox="0 -846.5 4914.4 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6C></use></g><g is=true transform=translate(576,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(1521,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(2022,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(2522,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2967,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(3468,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3913,0)><use href=#MJMAIN-32></use></g><g is=true transform=translate(4413,0)><use href=#MJMAIN-7D></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>l</mi><mo is=true>∈</mo><mo is=true>{</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>1</mn><mo is=true>,</mo><mn is=true>2</mn><mo is=true>}</mo></math></span></span></span> of the subdivision scheme <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-53-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">s</mi><mo is="true">&amp;#x2208;</mo><mi is="true">S</mi><mo is="true">=</mo><mo is="true">{</mo><mi mathvariant="italic" is="true">Horizontal</mi><mo is="true">,</mo><mi mathvariant="italic" is="true">Vertical</mi><mo is="true">,</mo><mi mathvariant="italic" is="true">Grid</mi><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=37.144ex height=2.779ex viewBox="0 -846.5 15992.4 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-73></use></g><g is=true transform=translate(747,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(1692,0)><use href=#MJMATHI-53></use></g><g is=true transform=translate(2615,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(3672,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(4172,0)><use href=#MJMATHI-48></use><use href=#MJMATHI-6F x=831 y=0></use><use href=#MJMATHI-72 x=1317 y=0></use><use href=#MJMATHI-69 x=1768 y=0></use><use href=#MJMATHI-7A x=2114 y=0></use><use href=#MJMATHI-6F x=2579 y=0></use><use href=#MJMATHI-6E x=3065 y=0></use><use href=#MJMATHI-74 x=3665 y=0></use><use href=#MJMATHI-61 x=4027 y=0></use><use href=#MJMATHI-6C x=4556 y=0></use></g><g is=true transform=translate(9027,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9472,0)><use href=#MJMATHI-56></use><use href=#MJMATHI-65 x=583 y=0></use><use href=#MJMATHI-72 x=1050 y=0></use><use href=#MJMATHI-74 x=1501 y=0></use><use href=#MJMATHI-69 x=1863 y=0></use><use href=#MJMATHI-63 x=2208 y=0></use><use href=#MJMATHI-61 x=2642 y=0></use><use href=#MJMATHI-6C x=3171 y=0></use></g><g is=true transform=translate(12942,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(13387,0)><use href=#MJMATHI-47></use><use href=#MJMATHI-72 x=786 y=0></use><use href=#MJMATHI-69 x=1238 y=0></use><use href=#MJMATHI-64 x=1583 y=0></use></g><g is=true transform=translate(15491,0)><use href=#MJMAIN-7D></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>s</mi><mo is=true>∈</mo><mi is=true>S</mi><mo is=true>=</mo><mo is=true>{</mo><mi mathvariant=italic is=true>Horizontal</mi><mo is=true>,</mo><mi mathvariant=italic is=true>Vertical</mi><mo is=true>,</mo><mi mathvariant=italic is=true>Grid</mi><mo is=true>}</mo></math></span></span></span> (see <a name=bf0030 href=#f0030 class=workspace-trigger>Fig. 6</a>).<a name=bfn6 href=#fn6 class=workspace-trigger><sup>6</sup></a> Let <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-54-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=4.054ex height=2.548ex viewBox="0 -995.6 1745.6 1096.9" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-48></use></g></g><g is=true transform=translate(905,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> and <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-55-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=4.459ex height=2.548ex viewBox="0 -995.6 1919.7 1096.9" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-57></use></g></g><g is=true transform=translate(1079,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> be the number of 8×8 blocks of pixel with respect to the height and width of the region <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-56-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3ex height=2.432ex viewBox="0 -945.9 1291.5 1047.3" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span>. We indicate with the notation <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-57-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=5.746ex height=3.702ex viewBox="0 -1045.3 2474.2 1593.7" role=img focusable=false style=vertical-align:-1.274ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-42></use></g></g><g is=true transform=translate(759,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(759,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>B</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-58-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=14.833ex height=2.894ex viewBox="0 -995.6 6386.2 1246" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-68></use></g><g is=true transform=translate(854,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1910,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2411,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2856,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(4195,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4640,0)><g is=true><g is=true><use href=#MJMATHI-48></use></g></g><g is=true transform=translate(905,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>h</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-59-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">w</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=15.562ex height=2.894ex viewBox="0 -995.6 6700.3 1246" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-77></use></g><g is=true transform=translate(994,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(2050,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2551,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2996,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(4335,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4780,0)><g is=true><g is=true><use href=#MJMATHI-57></use></g></g><g is=true transform=translate(1079,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>w</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span>, an 8×8 block of pixels of the region <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-60-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3ex height=2.432ex viewBox="0 -945.9 1291.5 1047.3" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> considering the color channel <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-61-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">c</mi><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mi is="true">Y</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=15.249ex height=2.779ex viewBox="0 -846.5 6565.3 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-63></use></g><g is=true transform=translate(711,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(1656,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(2157,0)><use href=#MJMATHI-59></use></g><g is=true transform=translate(2920,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3365,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(4484,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4930,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g><g is=true transform=translate(6064,0)><use href=#MJMAIN-7D></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>c</mi><mo is=true>∈</mo><mo is=true>{</mo><mi is=true>Y</mi><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub><mo is=true>}</mo></math></span></span></span>. Let <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-62-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=5.907ex height=3.702ex viewBox="0 -1045.3 2543.2 1593.7" role=img focusable=false style=vertical-align:-1.274ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup></math></span></span></span> be the DCT components obtained from <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-63-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=5.746ex height=3.702ex viewBox="0 -1045.3 2474.2 1593.7" role=img focusable=false style=vertical-align:-1.274ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-42></use></g></g><g is=true transform=translate(759,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(759,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>B</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup></math></span></span></span> through a 2-dimensional DCT processing. We indicate with <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-64-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.477ex height=4.625ex viewBox="0 -1244 4941.3 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=2543 y=-1></use><g is=true transform=translate(3140,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(3486,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3931,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=4343 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-65-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.404ex height=2.317ex viewBox="0 -747.2 4910.1 997.6" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-69></use></g><g is=true transform=translate(623,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1679,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2180,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2625,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(3964,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4409,0)><use href=#MJMAIN-37></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-66-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">j</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><mn is="true">7</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.586ex height=2.432ex viewBox="-11.5 -747.2 4988.6 1047.3" role=img focusable=false style=vertical-align:-0.697ex;margin-left:-0.027ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6A></use></g><g is=true transform=translate(690,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1746,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(2247,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2692,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(4031,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4476,0)><use href=#MJMAIN-37></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>j</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><mn is=true>7</mn></math></span></span></span>, the DCT components corresponding to the 2D DCT base (<em>i</em>,<em>j</em>) of <a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>(b). Let <em>F</em> be the set of the selected AC DCT basis defined above (Eq. <a name=beq0015 href=#eq0015 class=workspace-trigger>(3)</a>). Then, the scene context descriptor of the region <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-67-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3ex height=2.432ex viewBox="0 -945.9 1291.5 1047.3" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> is computed as in the following equations <a name=beq0020 href=#eq0020 class=workspace-trigger>(4)</a>, <a name=beq0025 href=#eq0025 class=workspace-trigger>(5)</a>, <a name=beq0030 href=#eq0030 class=workspace-trigger>(6)</a>, <a name=beq0035 href=#eq0035 class=workspace-trigger>(7)</a>:<span class=display><span id=eq0020 class=formula><span class=label>(4)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-68-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></mfrac><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">w</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=44.34ex height=4.625ex viewBox="0 -1244 19091 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-3BC></use></g></g><g is=true transform=translate(603,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(603,-149)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=1443 y=-1></use><g is=true transform=translate(2040,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(2541,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2986,0)><use href=#MJMAIN-30></use></g><use href=#MJSZ2-29 is=true x=3487 y=-1></use><g is=true transform=translate(4362,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5140,0)><g transform=translate(397,0)><rect stroke=none width=2711 height=60 x=0 y=220></rect><g is=true transform=translate(1178,403)><g is=true><use transform=scale(0.707) href=#MJMAIN-31></use></g></g><g is=true transform=translate(60,-542)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g><g is=true transform=translate(1234,0)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g></g></g><g is=true transform=translate(8537,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(958,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(11172,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(506,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(1057,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(13906,0)><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=16449 y=-1></use><g is=true transform=translate(17047,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(17547,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(17992,0)><use href=#MJMAIN-30></use></g><use href=#MJSZ2-29 is=true x=18493 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>μ</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo><mo is=true>=</mo><mfrac is=true><mrow is=true><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></mfrac><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>h</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>w</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo></math></span></span></span></span></span><span class=display><span id=eq0025 class=formula><span class=label>(5)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-69-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></mfrac><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">w</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><msup is="true"><mrow is="true"><mo stretchy="false" is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo><mo is="true">&amp;#x2212;</mo><msubsup is="true"><mrow is="true"><mi is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mn is="true">0</mn><mo is="true">,</mo><mn is="true">0</mn><mo is="true">)</mo><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=59.773ex height=6.009ex viewBox="0 -1542.1 25735.5 2587.3" role=img focusable=false style=vertical-align:-2.428ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-62></use></g></g><g is=true transform=translate(429,421)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(429,-248)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ3-28 is=true x=1269 y=-1></use><g is=true transform=translate(2005,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(2506,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2951,0)><use href=#MJMAIN-30></use></g><use href=#MJSZ3-29 is=true x=3452 y=-1></use><g is=true transform=translate(4466,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5244,0)><g transform=translate(397,0)><rect stroke=none width=2711 height=60 x=0 y=220></rect><g is=true transform=translate(1178,403)><g is=true><use transform=scale(0.707) href=#MJMAIN-31></use></g></g><g is=true transform=translate(60,-542)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g><g is=true transform=translate(1234,0)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g></g></g><g is=true transform=translate(8641,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(958,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(11276,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(506,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(1057,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(14010,0)><g is=true><g is=true><use href=#MJMAIN-28></use></g><g is=true transform=translate(389,0)><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=2932 y=-1></use><g is=true transform=translate(3530,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(4030,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4475,0)><use href=#MJMAIN-30></use></g><use href=#MJSZ2-29 is=true x=4976 y=-1></use><g is=true transform=translate(5796,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(6796,0)><g is=true><g is=true><use href=#MJMATHI-3BC></use></g></g><g is=true transform=translate(603,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(603,-149)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=8240 y=-1></use><g is=true transform=translate(8837,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(9338,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9783,0)><use href=#MJMAIN-30></use></g><use href=#MJSZ2-29 is=true x=10283 y=-1></use><g is=true transform=translate(10881,0)><use href=#MJMAIN-29></use></g></g><g is=true transform=translate(11270,877)><g is=true><use transform=scale(0.707) href=#MJMAIN-32></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>b</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo><mo is=true>=</mo><mfrac is=true><mrow is=true><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></mfrac><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>h</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>w</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><msup is=true><mrow is=true><mo stretchy=false is=true>(</mo><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo><mo is=true>−</mo><msubsup is=true><mrow is=true><mi is=true>μ</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mn is=true>0</mn><mo is=true>,</mo><mn is=true>0</mn><mo is=true>)</mo><mo stretchy=false is=true>)</mo></mrow><mrow is=true><mn is=true>2</mn></mrow></msup></math></span></span></span></span></span>where Eq. <a name=beq0020 href=#eq0020 class=workspace-trigger>(4)</a>, <a name=beq0025 href=#eq0025 class=workspace-trigger>(5)</a> are evaluated for each <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-70-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">c</mi><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mi is="true">Y</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=15.249ex height=2.779ex viewBox="0 -846.5 6565.3 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-63></use></g><g is=true transform=translate(711,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(1656,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(2157,0)><use href=#MJMATHI-59></use></g><g is=true transform=translate(2920,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3365,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(4484,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(4930,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g><g is=true transform=translate(6064,0)><use href=#MJMAIN-7D></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>c</mi><mo is=true>∈</mo><mo is=true>{</mo><mi is=true>Y</mi><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub><mo is=true>}</mo></math></span></span></span>. The features in Eqs. <a name=beq0020 href=#eq0020 class=workspace-trigger>(4)</a>, <a name=beq0025 href=#eq0025 class=workspace-trigger>(5)</a> are related to the DC components of the DCT.<span class=display><span id=eq0030 class=formula><span class=label>(6)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-71-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">=</mo><mi mathvariant="italic" is="true">Median</mi><mo is="true">(</mo><mo is="true">{</mo><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">|</mo><mi is="true">h</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><mo is="true">;</mo><mi is="true">w</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">,</mo><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><mo is="true">}</mo><mo is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=69.423ex height=4.625ex viewBox="0 -1244 29890.2 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-3BC></use></g></g><g is=true transform=translate(603,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(603,-149)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=1443 y=-1></use><g is=true transform=translate(2040,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(2386,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2831,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=3244 y=-1></use><g is=true transform=translate(4119,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5175,0)><use href=#MJMATHI-4D></use><use href=#MJMATHI-65 x=970 y=0></use><use href=#MJMATHI-64 x=1437 y=0></use><use href=#MJMATHI-69 x=1957 y=0></use><use href=#MJMATHI-61 x=2303 y=0></use><use href=#MJMATHI-6E x=2832 y=0></use></g><use href=#MJSZ2-28 is=true x=8608 y=-1></use><use href=#MJSZ2-7B is=true x=9206 y=-1></use><g is=true transform=translate(9873,0)><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=12416 y=-1></use><g is=true transform=translate(13014,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(13359,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(13805,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=14217 y=-1></use><g is=true transform=translate(14815,975)><use href=#MJMAIN-2223 x=0 y=-751></use><use href=#MJMAIN-2223 x=0 y=-1201></use></g><g is=true transform=translate(15093,0)><use href=#MJMATHI-68></use></g><g is=true transform=translate(15947,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(17004,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(17504,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(17949,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(19288,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(19734,0)><g is=true><g is=true><use href=#MJMATHI-48></use></g></g><g is=true transform=translate(905,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g><g is=true transform=translate(21479,0)><use href=#MJMAIN-3B></use></g><g is=true transform=translate(21924,0)><use href=#MJMATHI-77></use></g><g is=true transform=translate(22919,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(23975,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(24475,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(24921,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(26260,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(26705,0)><g is=true><g is=true><use href=#MJMATHI-57></use></g></g><g is=true transform=translate(1079,410)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g><use href=#MJSZ2-7D is=true x=28625 y=-1></use><use href=#MJSZ2-29 is=true x=29292 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>μ</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>=</mo><mi mathvariant=italic is=true>Median</mi><mo is=true>(</mo><mo is=true>{</mo><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>|</mo><mi is=true>h</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><mo is=true>;</mo><mi is=true>w</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>…</mo><mo is=true>,</mo><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><mo is=true>}</mo><mo is=true>)</mo></math></span></span></span></span></span><span class=display><span id=eq0035 class=formula><span class=label>(7)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-72-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></mfrac><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">H</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><munderover is="true"><mrow is="true"><mo is="true">&amp;#x2211;</mo></mrow><mrow is="true"><mi is="true">w</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></mrow></munderover><mo is="true">|</mo><msubsup is="true"><mrow is="true"><mi is="true">D</mi></mrow><mrow is="true"><mi is="true">h</mi><mo is="true">,</mo><mi is="true">w</mi><mo is="true">,</mo><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">&amp;#x2212;</mo><msubsup is="true"><mrow is="true"><mi is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msubsup><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">|</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=55.864ex height=4.625ex viewBox="0 -1244 24052.6 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-62></use></g></g><g is=true transform=translate(429,421)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(429,-248)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=1269 y=-1></use><g is=true transform=translate(1866,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(2212,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2657,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=3070 y=-1></use><g is=true transform=translate(3945,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(4723,0)><g transform=translate(397,0)><rect stroke=none width=2711 height=60 x=0 y=220></rect><g is=true transform=translate(1178,403)><g is=true><use transform=scale(0.707) href=#MJMAIN-31></use></g></g><g is=true transform=translate(60,-542)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g><g is=true transform=translate(1234,0)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g></g></g><g is=true transform=translate(8120,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-48></use></g></g><g is=true transform=translate(640,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(958,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(10755,0)><g is=true><g is=true><use href=#MJSZ1-2211></use></g></g><g is=true transform=translate(1056,477)><g is=true><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-57></use></g></g><g is=true transform=translate(763,290)><g is=true><use transform=scale(0.5) href=#MJMATHI-6C></use></g><g is=true transform=translate(149,0)><use transform=scale(0.5) href=#MJMAIN-2C></use></g><g is=true transform=translate(288,0)><use transform=scale(0.5) href=#MJMATHI-73></use></g></g></g></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(506,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(1057,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(13489,1116)><use href=#MJMAIN-2223 x=0 y=-751></use><use href=#MJMAIN-2223 x=0 y=-1484></use></g><g is=true transform=translate(13768,0)><g is=true><g is=true><use href=#MJMATHI-44></use></g></g><g is=true transform=translate(828,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(828,-327)><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(604,0)><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(1111,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1308,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=16311 y=-1></use><g is=true transform=translate(16908,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(17254,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(17699,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=18111 y=-1></use><g is=true transform=translate(18931,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(19932,0)><g is=true><g is=true><use href=#MJMATHI-3BC></use></g></g><g is=true transform=translate(603,483)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g><g is=true transform=translate(603,-149)><g is=true><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ2-28 is=true x=21375 y=-1></use><g is=true transform=translate(21973,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(22318,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(22764,0)><use href=#MJMATHI-6A></use></g><use href=#MJSZ2-29 is=true x=23176 y=-1></use><g is=true transform=translate(23774,1116)><use href=#MJMAIN-2223 x=0 y=-751></use><use href=#MJMAIN-2223 x=0 y=-1484></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mrow is=true><mi is=true>b</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>=</mo><mfrac is=true><mrow is=true><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></mfrac><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>h</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>H</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><munderover is=true><mrow is=true><mo is=true>∑</mo></mrow><mrow is=true><mi is=true>w</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mrow is=true><msup is=true><mrow is=true><mi is=true>W</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></mrow></munderover><mo is=true>|</mo><msubsup is=true><mrow is=true><mi is=true>D</mi></mrow><mrow is=true><mi is=true>h</mi><mo is=true>,</mo><mi is=true>w</mi><mo is=true>,</mo><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>−</mo><msubsup is=true><mrow is=true><mi is=true>μ</mi></mrow><mrow is=true><mi is=true>c</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msubsup><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>|</mo></math></span></span></span></span></span>where Eq. <a name=beq0030 href=#eq0030 class=workspace-trigger>(6)</a>, <a name=beq0035 href=#eq0035 class=workspace-trigger>(7)</a> are evaluated for each <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-73-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">(</mo><mi is="true">c</mi><mo is="true">,</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mi is="true">c</mi><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mi is="true">Y</mi><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><mo is="true">,</mo><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub><mo is="true">}</mo><mo is="true">;</mo><mo is="true">(</mo><mi is="true">i</mi><mo is="true">,</mo><mi is="true">j</mi><mo is="true">)</mo><mo is="true">&amp;#x2208;</mo><mi is="true">F</mi><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=37.278ex height=2.779ex viewBox="0 -846.5 16050.1 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMAIN-28></use></g><g is=true transform=translate(389,0)><use href=#MJMATHI-63></use></g><g is=true transform=translate(823,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(1268,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(1613,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2058,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(2471,0)><use href=#MJMAIN-29></use></g><g is=true transform=translate(3138,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(4083,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(4584,0)><use href=#MJMATHI-63></use></g><g is=true transform=translate(5295,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(6240,0)><use href=#MJMAIN-7B></use></g><g is=true transform=translate(6741,0)><use href=#MJMATHI-59></use></g><g is=true transform=translate(7504,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(7950,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(9069,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9514,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g><g is=true transform=translate(10649,0)><use href=#MJMAIN-7D></use></g><g is=true transform=translate(11149,0)><use href=#MJMAIN-3B></use></g><g is=true transform=translate(11594,0)><use href=#MJMAIN-28></use></g><g is=true transform=translate(11984,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(12329,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(12775,0)><use href=#MJMATHI-6A></use></g><g is=true transform=translate(13187,0)><use href=#MJMAIN-29></use></g><g is=true transform=translate(13854,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(14800,0)><use href=#MJMATHI-46></use></g><g is=true transform=translate(15549,0)><use href=#MJMAIN-7D></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mo is=true>(</mo><mi is=true>c</mi><mo is=true>,</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>∈</mo><mo is=true>{</mo><mi is=true>c</mi><mo is=true>∈</mo><mo is=true>{</mo><mi is=true>Y</mi><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><mo is=true>,</mo><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub><mo is=true>}</mo><mo is=true>;</mo><mo is=true>(</mo><mi is=true>i</mi><mo is=true>,</mo><mi is=true>j</mi><mo is=true>)</mo><mo is=true>∈</mo><mi is=true>F</mi><mo is=true>}</mo></math></span></span></span>. The features in Eqs. <a name=beq0030 href=#eq0030 class=workspace-trigger>(6)</a>, <a name=beq0035 href=#eq0035 class=workspace-trigger>(7)</a> are related to the 25 selected AC components of the DCT.<p id=p0205>Let <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-74-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">[</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><mo is="true">,</mo><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">b</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><mo is="true">]</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=10.006ex height=3.355ex viewBox="0 -995.6 4308.1 1444.7" role=img focusable=false style=vertical-align:-1.043ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><use href=#MJSZ1-5B is=true></use><g is=true transform=translate(417,0)><g is=true><g is=true><use href=#MJMATHBI-3BC></use></g></g><g is=true transform=translate(708,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g><g is=true transform=translate(1965,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2411,0)><g is=true><g is=true><use href=#MJMAINB-62></use></g></g><g is=true transform=translate(639,421)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g><use href=#MJSZ1-5D is=true x=3890 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mo is=true>[</mo><msup is=true><mrow is=true><mi mathvariant=bold-italic is=true>μ</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><mo is=true>,</mo><msup is=true><mrow is=true><mi mathvariant=bold is=true>b</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><mo is=true>]</mo></math></span></span></span> be the feature vector related to the region <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-75-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3ex height=2.432ex viewBox="0 -945.9 1291.5 1047.3" role=img focusable=false style=vertical-align:-0.235ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> computed considering the Eqs. <a name=beq0020 href=#eq0020 class=workspace-trigger>(4)</a>, <a name=beq0025 href=#eq0025 class=workspace-trigger>(5)</a>, <a name=beq0030 href=#eq0030 class=workspace-trigger>(6)</a>, <a name=beq0035 href=#eq0035 class=workspace-trigger>(7)</a>. The final image representation is obtained concatenating the representations <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-76-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">[</mo><msup is="true"><mrow is="true"><mi mathvariant="bold-italic" is="true">&amp;#x3BC;</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=4.566ex height=3.24ex viewBox="0 -945.9 1966 1395" role=img focusable=false style=vertical-align:-1.043ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><use href=#MJSZ1-5B is=true></use><g is=true transform=translate(417,0)><g is=true><g is=true><use href=#MJMATHBI-3BC></use></g></g><g is=true transform=translate(708,362)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mo is=true>[</mo><msup is=true><mrow is=true><mi mathvariant=bold-italic is=true>μ</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span>, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-77-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi mathvariant="bold" is="true">b</mi></mrow><mrow is="true"><mi is="true">l</mi><mo is="true">,</mo><mi is="true">s</mi></mrow></msup><mo is="true">]</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=4.406ex height=3.355ex viewBox="0 -995.6 1897 1444.7" role=img focusable=false style=vertical-align:-1.043ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMAINB-62></use></g></g><g is=true transform=translate(639,421)><g is=true><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(211,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(408,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g><use href=#MJSZ1-5D is=true x=1479 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi mathvariant=bold is=true>b</mi></mrow><mrow is=true><mi is=true>l</mi><mo is=true>,</mo><mi is=true>s</mi></mrow></msup><mo is=true>]</mo></math></span></span></span> of all the sub-regions in the spatial hierarchy (<a name=bf0030 href=#f0030 class=workspace-trigger>Fig. 6</a>). The computational complexity to compute the proposed image representation is linear with respect to the number of 8×8 blocks composing the image region under consideration.</p></section><section id=s0025><h2 id=sect0040 class="u-h3 u-margin-l-top u-margin-xs-bottom">5. The image generation pipeline architecture</h2><div><p id=p0210><span>In this section we describe the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/system-architecture title="Learn more about system architecture from ScienceDirect's AI-generated Topic Pages" class=topic-link>system architecture</a> to embed the scene context classification engine into an Image Generation Pipeline. The overall scheme is shown in </span><a name=bf0035 href=#f0035 class=workspace-trigger>Fig. 7</a>. The “Scene Context Classification” module is connected to the “DCT” module. The “High resolution Pipe” block represents a group of algorithms devoted to the generation of high resolution images. This block is linked to the “Acquisition Information” block devoted to collect different information related to the image (e.g., exposure, gain, focus, white balance and). These information are used to capture and process the image itself. The “Viewfinder Pipe” block represents a group of algorithms which usually work on downscaled images to be shown in the viewfinder of a camera. The “Scene Context Classification” block works taking the input from the viewfinder pipe to determine the scene class of the image. The recognized class of the scene influences both the “Acquisition Information” and the “High resolution pipe” blocks in setting the parameters for the image acquisition. Moreover, the information obtained by the “Scene Context Classification” block can be exploited by the “Application Engine” block which can perform different operations according to the detected scene category. The “Memory lines” and “DMA” blocks provide the data arranged in 8×8 blocks to the “DCT” module for each image channel (<em>Y</em>, <em>C</em><sub><em>b</em></sub>, <em>C</em><sub><em>b</em></sub>). The “JPEG” block is the one that produces the final compressed image. The sub-blocks, composing the “Scene Context Classification” module, are described in the next subsections.<figure class="figure text-xs" id=f0035><span><img src=data:null;base64, height=260 alt aria-describedby=cap0035><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr7_lrg.jpg target=_blank download title="Download high-res image (256KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (256KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr7.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0035><p id=sp0035><span class=label>Fig.&nbsp;7</span>. Architecture of the IGP including the proposed scene context <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification title="Learn more about classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification</a> engine.</p></span></span></figure></div><section id=s0030><h3 id=sect0045 class="u-h4 u-margin-m-top u-margin-xs-bottom">5.1. DCT coefficients accumulator</h3><p id=p0215><span>This block is directly linked to the “DCT” block, and thus it receives the DCT coefficients for the luminance and both <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/chrominance title="Learn more about chrominance from ScienceDirect's AI-generated Topic Pages" class=topic-link>chrominance</a> channels. With reference to the hierarchical scheme shown in </span><a name=bf0030 href=#f0030 class=workspace-trigger>Fig. 6</a>, this block accumulates DCT coefficients in histograms starting from the configuration having the smallest region size (e.g., level 2 of grid subdivision). For all the larger regions in the hierarchy, the computations can be performed by merging corresponding histogram bins previously computed at fine resolution level (e.g., the information already computed at level 2 can be exploited to compute the table at level 1 of grid subdivision).</p></section><section id=s0035><h3 id=sect0050 class="u-h4 u-margin-m-top u-margin-xs-bottom">5.2. Scene context representation</h3><p id=p0220>Starting from the histograms obtained by the “DCT Coefficients Accumulator” block, all the pairs of Laplacian parameters (<em>μ</em> and <em>b</em>) are computed by using the Laplacian fitting equations presented in <a name=bs0020 href=#s0020 class=workspace-trigger>Section 4</a>. The scene context representation is then obtained by concatenating all the computed Laplacian parameters related to the selected DCT frequencies of all the sub-regions in the hierarchy for the three channels composing the image. In addition to this information, the mean and variance of the DC coefficients upon the hierarchy are computed exploiting the equations introduced in <a name=bs0020 href=#s0020 class=workspace-trigger>Section 4</a>.</p></section><section id=s0040><h3 id=sect0055 class="u-h4 u-margin-m-top u-margin-xs-bottom">5.3. Classifier</h3><p id=p0225>The “Classifier” block takes the feature vector as input (i.e., the scene context representation) to perform the final scene context classification. It takes into account a classifier learned offline (i.e., the block “Model” in <a name=bf0035 href=#f0035 class=workspace-trigger>Fig. 7</a> which is learned out of the device). A Support Vector Machine is employed in our system architecture.</p></section></section><section id=s0045><h2 id=sect0060 class="u-h3 u-margin-l-top u-margin-xs-bottom">6. Experimental settings and results</h2><p id=p0230>In this section we report the experiments performed to quantitatively assess the effectiveness of the proposed DCT-GIST scene context descriptor with respect to other related approaches. In particular, we compare the performances obtained by the proposed representation model with respect to the ones achieved by the popular GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a><span> and the Roi+Gist <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/segmentation-model title="Learn more about Segmentation model from ScienceDirect's AI-generated Topic Pages" class=topic-link>Segmentation model</a> proposed in </span><a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. Moreover, since the proposed representation is obtained collecting information on a spatial hierarchy, we have compared it with respect to the one which uses bags of textons on the same spatial hierarchy <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>. Finally, we describe how the architecture presented in <a name=bs0025 href=#s0025 class=workspace-trigger>Section 5</a> has been implemented on an IGP of a mobile device to demonstrate the effectiveness and the real-time performances of the proposed method. Experiments have been done by using a SVM and a 10-fold cross-validation protocol on each considered dataset. The images are first partitioned into 10 folds by making a random reshuffling of the dataset. Subsequently, 10 iterations of training and testing are performed such that within each iteration a different fold of the data is held-out for testing while the remaining folds are used for learning. The final results are obtained by averaging over the 10 runs. Since the proposed image representation can be used as input for any classifier, we reports also results obtained by exploiting the DCT-GIST representation with a Convolutional Neural Network classifier.<section id=s0050><h3 id=sect0065 class="u-h4 u-margin-m-top u-margin-xs-bottom">6.1. Proposed DCT-GIST representation vs GIST representation</h3><div><p id=p0235>To perform this comparison we have taken into account the scene dataset used in the paper introducing the GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. The dataset is composed by 2688 color images with resolution of 256×256 pixels (JPEG format) belonging to 8 scene categories: <span><em><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/tallest-building title="Learn more about Tall Building from ScienceDirect's AI-generated Topic Pages" class=topic-link>Tall Building</a></em></span>, <em>Inside City</em>, <em>Street</em>, <em>Highway</em>, <em>Coast</em>, <em>Open Country</em>, <em>Mountain</em>, <em>Forest</em>. This dataset, together with the original code for computing the GIST descriptor are available on the web <a name=bbib45 href=#bib45 class=workspace-trigger>[45]</a>. To better highlight the contribution of the different components involved in the proposed DCT-GIST representation (see <a name=bs0020 href=#s0020 class=workspace-trigger>Section 4</a>) we have considered the following configurations (<a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>):<dl class=list><dt class=list-label>(A)<dd class=list-description><p id=p0240>Laplacian parameters of the 63 AC DCT components computed on <em>Y</em> channel;</p><dt class=list-label>(B)<dd class=list-description><p id=p0245>Laplacian parameters of the 25 selected AC DCT components computed on <em>Y</em> channel;</p><dt class=list-label>(C)<dd class=list-description><p id=p0250>Laplacian parameters of the 25 selected AC DCT components computed on <em>Y</em> channel and spatial hierarchy with 3 levels (<em>l</em>=0,1,2);</p><dt class=list-label>(D)<dd class=list-description><p id=p0255>Laplacian parameters of the 25 selected AC DCT components computed on <em>Y</em> channel, mean and variance of the DC DCT components computed on <em>Y</em> channel, and spatial hierarchy with 3 levels (<em>l</em>=0,1,2);</p><dt class=list-label>(E)<dd class=list-description><p id=p0260>Laplacian parameters of the 25 selected AC DCT components computed on <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-78-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.432ex viewBox="0 -796.9 2835.5 1047.3" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span> channels, mean and variance of the DC DCT components computed on <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-79-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.432ex viewBox="0 -796.9 2835.5 1047.3" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span> channels;</p><dt class=list-label>(F)<dd class=list-description><p id=p0265>Laplacian parameters of the 25 selected AC DCT components computed on <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-80-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.432ex viewBox="0 -796.9 2835.5 1047.3" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span> channels, mean and variance of the DC DCT components computed on <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-81-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.432ex viewBox="0 -796.9 2835.5 1047.3" role=img focusable=false style=vertical-align:-0.582ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span> channels, and spatial hierarchy with 3 levels (<em>l</em>=0,1,2).</p></dl><p><div class="tables frame-topbot rowsep-0 colsep-0" id=t0010><span class=captions><span id=cap0055><p id=sp0055><span class=label>Table&nbsp;2</span>. The different configurations of the proposed DCT-GIST image representation.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>DCT-GIST configuration</strong><th scope=col class=align-left><strong>DCT frequencies</strong><th scope=col class=align-left><strong>Image channels</strong><th scope=col class=align-left><strong>Spatial hierarchy</strong><tbody><tr class=valign-top><td class=align-left>(<strong>A</strong>)<td class=align-left>All 63 AC components<td class=align-left><em>Y</em><td class=align-left>No<tr class=valign-top><td class=align-left>(<strong>B</strong>)<td class=align-left>Selected 25 AC components<td class=align-left><em>Y</em><td class=align-left>No<tr class=valign-top><td class=align-left>(<strong>C</strong>)<td class=align-left>Selected 25 AC components<td class=align-left><em>Y</em><td class=align-left>Yes<tr class=valign-top><td class=align-left>(<strong>D</strong>)<td class=align-left>Selected 25 AC components + DC component<td class=align-left><em>Y</em><td class=align-left>Yes<tr class=valign-top><td class=align-left>(<strong>E</strong>)<td class=align-left>Selected 25 AC components + DC component<td class=align-left><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-82-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.534ex viewBox="0 -833.3 2835.5 1091" role=img focusable=false style=vertical-align:-0.599ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span><td class=align-left>No<tr class=valign-top><td class=align-left>(<strong>F</strong>)<td class=align-left>Selected 25 AC components + DC component<td class=align-left><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-83-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mrow is="true"><mi mathvariant="italic" is="true">YC</mi></mrow><mrow is="true"><mi is="true">b</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">C</mi></mrow><mrow is="true"><mi is="true">r</mi></mrow></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=6.586ex height=2.534ex viewBox="0 -833.3 2835.5 1091" role=img focusable=false style=vertical-align:-0.599ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-59></use><use href=#MJMATHI-43 x=581 y=0></use></g></g><g is=true transform=translate(1297,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-62></use></g></g></g><g is=true transform=translate(1700,0)><g is=true><g is=true><use href=#MJMATHI-43></use></g></g><g is=true transform=translate(715,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-72></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mrow is=true><mi mathvariant=italic is=true>YC</mi></mrow><mrow is=true><mi is=true>b</mi></mrow></msub><msub is=true><mrow is=true><mi is=true>C</mi></mrow><mrow is=true><mi is=true>r</mi></mrow></msub></math></span></span></span><td class=align-left>Yes</table></div></div></div><div><p id=p0270><a name=bf0040 href=#f0040 class=workspace-trigger>Fig. 8</a> reports the average per class accuracy obtained considering all the above DCT-GIST representation configurations together with the results obtained employing the GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. The results show that the scene representation which considers only the Laplacian parameters of the 25 selected AC DCT frequencies fitted on the <em>Y</em> channel, i.e., the configuration (B), already obtains an accuracy of 75.20%. Encoding the information on the spatial hierarchy, i.e., configuration (C), is useful to improve the results of more than 6%. A small, but still useful, contribution is given by the color information obtained considering the DC DCT components, i.e., configuration (D). The proposed DCT-GIST representation obtains better results with respect to the GIST descriptor in both cases with and without spatial hierarchy (our with spatial hierarchy: 85.25%, our without spatial hierarchy: 84.60%, GIST: 84.28%). <a name=bt0015 href=#t0015 class=workspace-trigger>Table 3</a><span> reports the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/confusion-matrix title="Learn more about confusion matrix from ScienceDirect's AI-generated Topic Pages" class=topic-link>confusion matrix</a> related to the proposed DCT-GIST representation corresponding to the configuration (F), whereas </span><a name=bt0020 href=#t0020 class=workspace-trigger>Table 4</a><span> shows the confusion matrix obtained by employing the GIST descriptor. One should not overlook that the proposed DCT-GIST representation has a very limited computational overhead for the image signature generation because it is directly computed by considering DCT coefficients already available from the JPEG encoder/format. Specifically, the computation of the image representation (F) requires about 1 operation per pixel (i.e., it is linear with respect to the image size). This highly reduces the complexity of the scene recognition system. Moreover, differently than GIST descriptor, the proposed representation is suitable for mobile platforms (e.g., smartphones and wearable cameras) since the DCT is already embedded in the Image Generation Pipeline, whereas the GIST descriptor needs extra overhead to compute the signature of the image and employs operations which are not present in the current IGP of single <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/imaging-sensor title="Learn more about sensors imaging from ScienceDirect's AI-generated Topic Pages" class=topic-link>sensors imaging</a> devices (e.g., FFT on the overall image). As detailed in the Sub-Section </span><a name=bs0070 href=#s0070 class=workspace-trigger>6.5</a>, the proposed DCT-GIST descriptor with configuration (F) can be computed in 15.9&nbsp;ms on QVGA images (i.e., 320×240 pixels) with a 1&nbsp;GHz Dual-core CPU. This computational time considers also the operations needed to compute the 8×8 DCT transformation of the input image. When the 8×8 DCT coefficients of the image are already available (e.g., in case of JPEG images or considering that these feature are computed into the IGP) the time needed to compute the proposed DCT-GIST descriptor is only 0.3&nbsp;ms. As reported in <a name=bbib28 href=#bib28 class=workspace-trigger>[28]</a> where an in-depth evaluation of the complexity of the GIST has been presented, the time needed to compute the GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> on 64-bit 8-core computer and considering images of size 32×32 pixels is 35&nbsp;ms. This means that the time needed to compute the proposed representation is at least half than the one needed to compute the GIST descriptor, and it is one order of magnitude less if the DCT coefficients are already available (i.e., JPEG format).<figure class="figure text-xs" id=f0040><span><img src=data:null;base64, height=368 alt aria-describedby=cap0040><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr8_lrg.jpg target=_blank download title="Download high-res image (332KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (332KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr8.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0040><p id=sp0040><span class=label>Fig.&nbsp;8</span>. Contribution of each component involved in the proposed DCT-GIST representation and comparison with respect to the GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>.</p></span></span></figure><div class="tables frame-topbot rowsep-0 colsep-0" id=t0015><span class=captions><span id=cap0060><p id=sp0060><span class=label>Table&nbsp;3</span>. Results obtained by exploiting the proposed DCT-GIST representation with configuration (F) on the 8 Scene Context Dataset <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. Columns correspond to the inferred classes.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><em>Tall Building</em><th scope=col class=align-left><em>Inside City</em><th scope=col class=align-left><em>Street</em><th scope=col class=align-left><em>Highway</em><th scope=col class=align-left><em>Coast</em><th scope=col class=align-left><em>Open Country</em><th scope=col class=align-left><em>Mountain</em><th scope=col class=align-left><em>Forest</em><tbody><tr class=valign-top><td class=align-left><em>Tall Building</em><td class=align-char><strong>0.88</strong><td class=align-char>0.07<td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.01<td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.02<tr class=valign-top><td class=align-left><em>Inside City</em><td class=align-char>0.07<td class=align-char><strong>0.87</strong><td class=align-char>0.04<td class=align-char>0.02<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<tr class=valign-top><td class=align-left><em>Street</em><td class=align-char>0.03<td class=align-char>0.04<td class=align-char><strong>0.89</strong><td class=align-char>0.02<td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.01<td class=align-char>0.01<tr class=valign-top><td class=align-left><em>Highway</em><td class=align-char>0.00<td class=align-char>0.03<td class=align-char>0.02<td class=align-char><strong>0.82</strong><td class=align-char>0.07<td class=align-char>0.03<td class=align-char>0.03<td class=align-char>0.00<tr class=valign-top><td class=align-left><em>Coast</em><td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.02<td class=align-char><strong>0.85</strong><td class=align-char>0.11<td class=align-char>0.01<td class=align-char>0.01<tr class=valign-top><td class=align-left><em>Open Country</em><td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.02<td class=align-char>0.15<td class=align-char><strong>0.74</strong><td class=align-char>0.05<td class=align-char>0.03<tr class=valign-top><td class=align-left><em>Mountain</em><td class=align-char>0.01<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.02<td class=align-char>0.05<td class=align-char><strong>0.85</strong><td class=align-char>0.06<tr class=valign-top><td class=align-left><em>Forest</em><td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.02<td class=align-char>0.05<td class=align-char><strong>0.93</strong></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id=t0020><span class=captions><span id=cap0065><p id=sp0065><span class=label>Table&nbsp;4</span>. Results obtained exploiting the GIST representation <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> on the 8 Scene Context Dataset. Columns correspond to the inferred classes.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><em>Tall Building</em><th scope=col class=align-left><em>Inside City</em><th scope=col class=align-left><em>Street</em><th scope=col class=align-left><em>Highway</em><th scope=col class=align-left><em>Coast</em><th scope=col class=align-left><em>Open Country</em><th scope=col class=align-left><em>Mountain</em><th scope=col class=align-left><em>Forest</em><tbody><tr class=valign-top><td class=align-left><em>Tall Building</em><td class=align-char><strong>0.83</strong><td class=align-char>0.01<td class=align-char>0.03<td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.13<td class=align-char>0.00<td class=align-char>0.00<tr class=valign-top><td class=align-left><em>Inside City</em><td class=align-char>0.00<td class=align-char><strong>0.94</strong><td class=align-char>0.00<td class=align-char>0.00<td class=align-char>0.05<td class=align-char>0.01<td class=align-char>0.00<td class=align-char>0.01<tr class=valign-top><td class=align-left><em>Street</em><td class=align-char>0.07<td class=align-char>0.00<td class=align-char><strong>0.82</strong><td class=align-char>0.03<td class=align-char>0.03<td class=align-char>0.03<td class=align-char>0.02<td class=align-char>0.00<tr class=valign-top><td class=align-left><em>Highway</em><td class=align-char>0.02<td class=align-char>0.01<td class=align-char>0.01<td class=align-char><strong>0.84</strong><td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.04<td class=align-char>0.08<tr class=valign-top><td class=align-left><em>Coast</em><td class=align-char>0.01<td class=align-char>0.05<td class=align-char>0.01<td class=align-char>0.00<td class=align-char><strong>0.86</strong><td class=align-char>0.05<td class=align-char>0.00<td class=align-char>0.02<tr class=valign-top><td class=align-left><em>Open Country</em><td class=align-char>0.14<td class=align-char>0.04<td class=align-char>0.02<td class=align-char>0.00<td class=align-char>0.05<td class=align-char><strong>0.73</strong><td class=align-char>0.01<td class=align-char>0.00<tr class=valign-top><td class=align-left><em>Mountain</em><td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.03<td class=align-char>0.05<td class=align-char>0.01<td class=align-char>0.02<td class=align-char><strong>0.87</strong><td class=align-char>0.02<tr class=valign-top><td class=align-left><em>Forest</em><td class=align-char>0.00<td class=align-char>0.01<td class=align-char>0.00<td class=align-char>0.08<td class=align-char>0.02<td class=align-char>0.00<td class=align-char>0.00<td class=align-char><strong>0.88</strong></table></div></div></div><p id=p0275>Further tests have been done to demonstrate the effectiveness of the proposed representation in discriminating the <em>Naturalness</em> and <em>Openness</em> of the scene <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. Specifically, taking into account the definition given in <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, the <em>Naturalness</em> of the scene is related to the structure of a scene which strongly differs between man-made and natural environments. The notion of <em>Openness</em> is related to the open vs closed-enclosed environment, scenes with horizon vs no horizon, a vast or empty space vs a full, filled-in space <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. A closed scene is a scene with small perceived depth, whereas an open scene is a scene with a big perceived depth. Information about <em>Naturalness</em> and/or <em>Openness</em> of the scene can be very useful in setting parameters of the algorithms involved in the image generation pipeline <a name=bbib13 href=#bib13 class=workspace-trigger>[13]</a>.<div><p id=p0280>For the <em>Naturalness</em> experiment we have split the 8 scene dataset as in <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib15 href=#bib15 class=workspace-trigger>[15]</a> by considering the classes <em>Coast</em>, <em>Open Country</em>, <em>Mountain</em> and <em>Forest</em> as <em>Natural</em> environments, whereas the classes <em>Tall Building</em>, <em>Inside City</em>, <em>Street</em> and <em>Highway</em> as belonging to the <em>Man-Made</em> environments. For the <em>Openness</em> experiment, the images belonging to the classes <em>Coast</em>, <em>Open Country</em>, <em>Street</em> and <em>Highway</em> have been considered as <em>Open</em> scenes, whereas the images of the classes <em>Forest</em>, <em>Mountain</em>, <em>Tall Building</em> and <em>Inside City</em> have been considered as <em>Closed</em> scenes. The results obtained employing the proposed representation (F) are reported in <a name=bt0025 href=#t0025 class=workspace-trigger>Table&nbsp;5</a>, <a name=bt0030 href=#t0030 class=workspace-trigger>Table&nbsp;6</a>. The obtained results closely match the performances of other state-of-the-art methods <a name=bbib15 href=#bib15 class=workspace-trigger>[15]</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib26 href=#bib26 class=workspace-trigger>[26]</a> by employing less computational resources.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0025><span class=captions><span id=cap0070><p id=sp0070><span class=label>Table&nbsp;5</span>. <em>Natural</em> vs <em>Man-made</em> classification performances of the proposed DCT-GIST representation with configuration (F). Columns correspond to the inferred classes.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><strong>Natural</strong><th scope=col class=align-left><strong>Man-made</strong><tbody><tr class=valign-top><td class=align-left><strong>Natural</strong><td class=align-char><strong>97.88</strong><td class=align-char>2.12<tr class=valign-top><td class=align-left><strong>Man-made</strong><td class=align-char>4.75<td class=align-char><strong>95.25</strong></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id=t0030><span class=captions><span id=cap0075><p id=sp0075><span class=label>Table&nbsp;6</span>. <em>Open</em> vs <em>Closed</em> classification performances considering the proposed DCT-GIST representation with configuration (F). Columns correspond to the inferred classes.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><strong>Open</strong><th scope=col class=align-left><strong>Closed</strong><tbody><tr class=valign-top><td class=align-left><strong>Open</strong><td class=align-char><strong>94.17</strong><td class=align-char>5.83<tr class=valign-top><td class=align-left><strong>Closed</strong><td class=align-char>4.63<td class=align-char><strong>95.37</strong></table></div></div></div><div><p id=p0285>Finally, we have considered the problem of recognizing four scene context usually available in the auto-scene mode of digital consumer cameras: <em>Landscape</em>, <em>Man-Made Outdoor</em>, <em>Portrait</em>, <em>Snow</em>. To this purpose we have collected 2000 color images (i.e., 500 per class) with resolution 640×480 pixels from Flickr . This dataset has been used to perform a comparative test of the proposed DCT-GIST image representation with configuration (F) with respect to the popular GIST descriptor. The results are reported on <a name=bt0035 href=#t0035 class=workspace-trigger>Table&nbsp;7</a>, <a name=bt0040 href=#t0040 class=workspace-trigger>Table&nbsp;8</a>. The proposed image representation obtained an average accuracy of 89.80%, whereas GIST achieved 86.07%.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0035><span class=captions><span id=cap0080><p id=sp0080><span class=label>Table&nbsp;7</span>. Results obtained by the proposed DCT-GIST representation with configuration (F) on four classes usually used in the auto-scene mode of consumer digital cameras.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><strong>Landscape</strong><th scope=col class=align-left><strong>Man-made outdoor</strong><th scope=col class=align-left><strong>Portrait</strong><th scope=col class=align-left><strong>Snow</strong><tbody><tr class=valign-top><td class=align-left><strong>Landscape</strong><td class=align-char><strong>87.76</strong><td class=align-char>1.22<td class=align-char>0.61<td class=align-char>10.41<tr class=valign-top><td class=align-left><strong>Man-made outdoor</strong><td class=align-char>3.78<td class=align-char><strong>91.33</strong><td class=align-char>2.22<td class=align-char>2.67<tr class=valign-top><td class=align-left><strong>Portrait</strong><td class=align-char>1.02<td class=align-char>1.84<td class=align-char><strong>94.29</strong><td class=align-char>2.86<tr class=valign-top><td class=align-left><strong>Snow</strong><td class=align-char>9.62<td class=align-char>1.13<td class=align-char>3.02<td class=align-char><strong>86.23</strong></table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id=t0040><span class=captions><span id=cap0085><p id=sp0085><span class=label>Table&nbsp;8</span>. Results obtained by GIST <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> on four classes usually used in the auto-scene mode of consumer digital cameras.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-left><strong>Landscape</strong><th scope=col class=align-left><strong>Man-made outdoor</strong><th scope=col class=align-left><strong>Portrait</strong><th scope=col class=align-left><strong>Snow</strong><tbody><tr class=valign-top><td class=align-left><strong>Landscape</strong><td class=align-char><strong>84.69</strong><td class=align-char>3.27<td class=align-char>0.20<td class=align-char>11.84<tr class=valign-top><td class=align-left><strong>Man-made outdoor</strong><td class=align-char>4.44<td class=align-char><strong>87.78</strong><td class=align-char>2.44<td class=align-char>5.33<tr class=valign-top><td class=align-left><strong>Portrait</strong><td class=align-char>0.41<td class=align-char>3.47<td class=align-char><strong>91.84</strong><td class=align-char>4.29<tr class=valign-top><td class=align-left><strong>Snow</strong><td class=align-char>11.70<td class=align-char>3.40<td class=align-char>4.34<td class=align-char><strong>80.57</strong></table></div></div></div></section><section id=s0055><h3 id=sect0070 class="u-h4 u-margin-m-top u-margin-xs-bottom">6.2. Proposed representation vs bags of textons on spatial hierarchy</h3><div><p id=p0290>Since the proposed scene context representation works exploiting information collected on spatial hierarchy, we have compared it with respect to the method presented in <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, where Bags of Textons are collected for each region in the spatial hierarchy to represent the images for scene classification purposes. For this comparison we have considered the 15 Scene Classes Dataset used in <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>. This dataset is an augmented version of the 8 Scene Classes Dataset <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>. The dataset is composed by 4485 images of the following fifteen categories: <em>highway</em>, <em>inside of cities</em>, <em>tall buildings</em>, <em>streets</em>, <em>forest</em>, <em>coast</em>, <em>mountain</em>, <em>open country</em>, <em>suburb residence</em>, <em>bedroom</em>, <em>kitchen</em>, <em>living room</em>, <em>office</em>, <em>industrial</em> and <em>store</em>. Since a subset of the images of the dataset does not have color information, the tests on the 15 Scene Classes Dataset have been performed taking into account only the <em>Y</em> channel and using the DCT-GIST scene descriptor with configuration (D) (see <a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>). The results obtained on this dataset are reported in <a name=bt0045 href=#t0045 class=workspace-trigger>Table 9</a>. The average per class accuracy achieved by the proposed approach is 78.45%, whereas the method which exploit textons distributions on spatial hierarchy <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> obtained an accuracy of 79.43%. Both representations outperform the GIST one, which obtains 73.25% of accuracy on this dataset. Although the results are slightly in favor for the method proposed in <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> (of less than 1%), one should not forget that the proposed DCT-GIST representation is suitable for an implementation on the image generation pipeline of single sensor devices, whereas the method in <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> requires extra memory to store Textons vocabularies (i.e., hardware costs for industry) as well as a bigger computational overhead to represent the image to be classified (e.g., convolution with bank of filters, computation of the Textons distributions for every sub-regions, etc.). Specifically, considering an image stored in JPEG format, the computation of the Bag of Textons signature in <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a> requires the convolution of the image with a bank of 24 filters of size 49×49 (i.e., 49×49×24 operations per pixel), and the computation of the similarity of each pixel responses with respect to the Textons vocabulary (i.e., <em>T</em> operations per pixels, where <em>T</em> is the number of Textons in the vocabulary). Hence, the computational time needed to build the Bag of Textons signature is much higher than the one to compute the proposed DCT-GIST representation (i.e., linear with respect to the number of 8×8 blocks composing the image region under consideration).<div class="tables frame-topbot rowsep-0 colsep-0" id=t0045><span class=captions><span id=cap0090><p id=sp0090><span class=label>Table&nbsp;9</span>. Results obtained on the 15 Scene Dataset <a name=bbib18 href=#bib18 class=workspace-trigger>[18]</a>.</p></span></span><div class=groups><table><tbody><tr class=valign-top><td class=align-left><strong>Bags of textons with spatial hierarchy</strong> <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a><td class=align-left>79.43%<tr class=valign-top><td class=align-left><strong>Proposed DCT-GIST representation with configuration</strong> (<strong>D</strong>)<td class=align-left>78.45%<tr class=valign-top><td class=align-left><strong>GIST representation</strong> <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a><td class=align-left>73.25%</table></div></div></div><div><p id=p0295>We have performed one more test to assess the ability of the proposed representation in discriminating among <em>Indoor</em> vs <em>Outdoor</em><span> scenes. This prior can be very useful for <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/autofocus title="Learn more about autofocus from ScienceDirect's AI-generated Topic Pages" class=topic-link>autofocus</a>, auto-exposure and white balance algorithms. To this aim we have divided the images of the 15 Scene Classes Dataset as indoor vs outdoor images. The classification results are reported in </span><a name=bt0050 href=#t0050 class=workspace-trigger>Table 10</a>. Again the results confirm that the proposed representation can be employed to distinguish classes of scenes at superordinate level of description <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0050><span class=captions><span id=cap0095><p id=sp0095><span class=label>Table&nbsp;10</span>. <em>Indoor</em> vs <em>outdoor</em> classification performances considering the proposed DCT-GIST representation with configuration (D). Columns correspond to the inferred classes.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Confusion matrix</strong><th scope=col class=align-char><strong>Indoor</strong><th scope=col class=align-char><strong>Outdoor</strong><tbody><tr class=valign-top><td class=align-left><strong>Indoor</strong><td class=align-char><strong>89.75</strong><td class=align-char>10.25<tr class=valign-top><td class=align-left><strong>Outdoor</strong><td class=align-char>3.86<td class=align-char><strong>96.14</strong></table></div></div></div></section><section id=s0060><h3 id=sect0075 class="u-h4 u-margin-m-top u-margin-xs-bottom">6.3. DCT-GIST evaluation on the MIT-67 indoor scene dataset</h3><p id=p0300>To further assess the proposed image representation we have performed tests by considering the challenging problem of discriminating among different indoor scenes categories. To this aim we have considered the MIT-67 Indoor Scene Dataset <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a> which contains 67 Indoor categories and a total of 15,620 images. The MIT-67 dataset is one of the largest dataset of scenes available so far. In performing the experiments we have considered the testing protocol used in <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a> (i.e., same training and testing images). The tests have been done considering the configuration (F) of the proposed representation (see <a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>). The proposed descriptor has been compared with respect to the GIST <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> as well as with respect to the model called ROI+Gist Segmentation (RGS) which has been introduced in the paper related to MIT-67 dataset <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. The RGS representation model combine both global (i.e., GIST) and local information (i.e., spatial pyramid of visual words on ROIs obtained by segmenting the image). Hence the RGS is able to take into account global spatial properties of the scenes and the concepts/objects they contain.<div><p id=p0305>The experiments pointed out that our DCT-GIST scene descriptor achieves an average per-class accuracy of 26.7%, which is greater than the one obtained by both GIST (less than 22%) and RGS model (25.05%) <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. <a name=bt0055 href=#t0055 class=workspace-trigger>Table 11</a> reports the per-class accuracies obtained with both the proposed DCT-GIST and the RGS model. Also in this case the DCT-GIST descriptor obtains comparable recognition performances with respect to the state-of-the-art, and outperforms the state-of-the-art in terms of computational complexity (i.e., RGS model needs to compute the GIST with its related computational complexity, needs a segmentation step, and also uses a spatial based bag of visual word model. Hence, DCT-GIST is more suitable for the Imaging Generation Pipeline in terms of both time and memory resources).<div class="tables frame-topbot rowsep-0 colsep-0" id=t0055><span class=captions><span id=cap0100><p id=sp0100><span class=label>Table&nbsp;11</span>. Recognition results of the proposed DCT-GIST descriptor on the MIT-67 dataset <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. The proposed representation is compared with respect to the ROI+Gist Segmentation model <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Classes</strong><th scope=col class=align-left><strong>Proposed DCT-GIST</strong><th scope=col class=align-left><strong>RGS model</strong> <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a><tbody><tr class=valign-top><td class=align-left>elevator<td class=align-char><strong>71.40</strong><td class=align-char>61.90<tr class=valign-top><td class=align-left>greenhouse<td class=align-char><strong>65.00</strong><td class=align-char>50.00<tr class=valign-top><td class=align-left>concert hall<td class=align-char><strong>60.00</strong><td class=align-char>45.00<tr class=valign-top><td class=align-left>inside bus<td class=align-char><strong>56.50</strong><td class=align-char>39.10<tr class=valign-top><td class=align-left>corridor<td class=align-char><strong>52.40</strong><td class=align-char>38.10<tr class=valign-top><td class=align-left>bowling<td class=align-char><strong>50.00</strong><td class=align-char>45.00<tr class=valign-top><td class=align-left>buffet<td class=align-char>50.00<td class=align-char><strong>55.00</strong><tr class=valign-top><td class=align-left>classroom<td class=align-char><strong>50.00</strong><td class=align-char><strong>50.00</strong><tr class=valign-top><td class=align-left>cloister<td class=align-char><strong>50.00</strong><td class=align-char>45.00<tr class=valign-top><td class=align-left>casino<td class=align-char><strong>47.40</strong><td class=align-char>21.10<tr class=valign-top><td class=align-left>hospital room<td class=align-char><strong>45.00</strong><td class=align-char>35.00<tr class=valign-top><td class=align-left>pantry<td class=align-char><strong>45.00</strong><td class=align-char>25.00<tr class=valign-top><td class=align-left>auditorium<td class=align-char>44.40<td class=align-char><strong>55.60</strong><tr class=valign-top><td class=align-left>church inside<td class=align-char>42.10<td class=align-char><strong>63.20</strong><tr class=valign-top><td class=align-left>library<td class=align-char><strong>40.00</strong><td class=align-char><strong>40.00</strong><tr class=valign-top><td class=align-left>bathroom<td class=align-char><strong>38.90</strong><td class=align-char>33.30<tr class=valign-top><td class=align-left>clothing store<td class=align-char><strong>38.90</strong><td class=align-char>22.20<tr class=valign-top><td class=align-left>tv studio<td class=align-char><strong>38.90</strong><td class=align-char>27.80<tr class=valign-top><td class=align-left>children room<td class=align-char><strong>33.30</strong><td class=align-char>5.60<tr class=valign-top><td class=align-left>closet<td class=align-char>33.30<td class=align-char><strong>38.90</strong><tr class=valign-top><td class=align-left>inside subway<td class=align-char><strong>33.30</strong><td class=align-char>23.80<tr class=valign-top><td class=align-left>florist<td class=align-char>31.60<td class=align-char><strong>36.80</strong><tr class=valign-top><td class=align-left>studio music<td class=align-char>31.60<td class=align-char><strong>36.80</strong><tr class=valign-top><td class=align-left>airport inside<td class=align-char><strong>30.00</strong><td class=align-char>10.00<tr class=valign-top><td class=align-left>kinder garden<td class=align-char><strong>30.00</strong><td class=align-char>5.00<tr class=valign-top><td class=align-left>movie theater<td class=align-char><strong>30.00</strong><td class=align-char>15.00<tr class=valign-top><td class=align-left>dental office<td class=align-char>28.60<td class=align-char><strong>42.90</strong><tr class=valign-top><td class=align-left>grocery store<td class=align-char>28.60<td class=align-char><strong>38.10</strong><tr class=valign-top><td class=align-left>dining room<td class=align-char><strong>27.80</strong><td class=align-char>16.70<tr class=valign-top><td class=align-left>meeting room<td class=align-char><strong>27.30</strong><td class=align-char>9.10<tr class=valign-top><td class=align-left>video store<td class=align-char><strong>27.30</strong><td class=align-char><strong>27.30</strong><tr class=valign-top><td class=align-left>art studio<td class=align-char><strong>25.00</strong><td class=align-char>10.00<tr class=valign-top><td class=align-left>living room<td class=align-char><strong>25.00</strong><td class=align-char>15.00<tr class=valign-top><td class=align-left>lobby<td class=align-char><strong>25.00</strong><td class=align-char>10.00<tr class=valign-top><td class=align-left>nursery<td class=align-char>25.00<td class=align-char><strong>35.00</strong><tr class=valign-top><td class=align-left>prison cell<td class=align-char><strong>25.00</strong><td class=align-char>10.00<tr class=valign-top><td class=align-left>restaurant<td class=align-char><strong>25.00</strong><td class=align-char>5.00<tr class=valign-top><td class=align-left>computer room<td class=align-char>22.20<td class=align-char><strong>44.40</strong><tr class=valign-top><td class=align-left>garage<td class=align-char>22.20<td class=align-char><strong>27.80</strong><tr class=valign-top><td class=align-left>bakery<td class=align-char><strong>21.10</strong><td class=align-char>15.80<tr class=valign-top><td class=align-left>game room<td class=align-char>20.00<td class=align-char><strong>25.00</strong><tr class=valign-top><td class=align-left>stairscase<td class=align-char>20.00<td class=align-char><strong>30.00</strong><tr class=valign-top><td class=align-left>train station<td class=align-char>20.00<td class=align-char><strong>35.00</strong><tr class=valign-top><td class=align-left>subway<td class=align-char><strong>19.00</strong><td class=align-char>9.50<tr class=valign-top><td class=align-left>bar<td class=align-char>16.70<td class=align-char><strong>22.20</strong><tr class=valign-top><td class=align-left>gym<td class=align-char>16.70<td class=align-char><strong>27.80</strong><tr class=valign-top><td class=align-left>deli<td class=align-char>15.80<td class=align-char><strong>21.10</strong><tr class=valign-top><td class=align-left>bedroom<td class=align-char><strong>14.30</strong><td class=align-char><strong>14.30</strong><tr class=valign-top><td class=align-left>kitchen<td class=align-char>14.30<td class=align-char><strong>23.80</strong><tr class=valign-top><td class=align-left>locker room<td class=align-char>14.30<td class=align-char><strong>38.10</strong><tr class=valign-top><td class=align-left>laundromat<td class=align-char>13.60<td class=align-char><strong>31.80</strong><tr class=valign-top><td class=align-left>toystore<td class=align-char><strong>13.60</strong><td class=align-char><strong>13.60</strong><tr class=valign-top><td class=align-left>restaurant kitchen<td class=align-char><strong>13.00</strong><td class=align-char>4.30<tr class=valign-top><td class=align-left>fast-food restaurant<td class=align-char>11.80<td class=align-char><strong>23.50</strong><tr class=valign-top><td class=align-left>mall<td class=align-char><strong>10.00</strong><td class=align-char>0.00<tr class=valign-top><td class=align-left>hair salon<td class=align-char><strong>9.50</strong><td class=align-char><strong>9.50</strong><tr class=valign-top><td class=align-left>office<td class=align-char><strong>9.50</strong><td class=align-char>0.00<tr class=valign-top><td class=align-left>warehouse<td class=align-char><strong>9.50</strong><td class=align-char><strong>9.50</strong><tr class=valign-top><td class=align-left>laboratory wet<td class=align-char><strong>9.10</strong><td class=align-char>0.00<tr class=valign-top><td class=align-left>operating room<td class=align-char>5.30<td class=align-char><strong>10.50</strong><tr class=valign-top><td class=align-left>bookstore<td class=align-char>5.00<td class=align-char><strong>20.00</strong><tr class=valign-top><td class=align-left>pool inside<td class=align-char>5.00<td class=align-char><strong>25.00</strong><tr class=valign-top><td class=align-left>jewellery shop<td class=align-char><strong>4.50</strong><td class=align-char>0.00<tr class=valign-top><td class=align-left>museum<td class=align-char><strong>4.30</strong><td class=align-char><strong>4.30</strong><tr class=valign-top><td class=align-left>shoe shop<td class=align-char>0.00<td class=align-char><strong>5.30</strong><tr class=valign-top><td class=align-left>waiting room<td class=align-char>0.00<td class=align-char><strong>19.00</strong><tr class=valign-top><td class=align-left>wine cellar<td class=align-char>0.00<td class=align-char><strong>23.80</strong><tr class=valign-top><td class=align-left><strong>Average</strong><td class=align-char><strong>26.70</strong><td class=align-char>25.05</table></div></div></div></section><section id=s0065><h3 id=sect0080 class="u-h4 u-margin-m-top u-margin-xs-bottom">6.4. Instant scene context classification on mobile device</h3><p id=p0310>The experiments presented in <a name=bs0050 href=#s0050 class=workspace-trigger>6.1 Proposed DCT-GIST representation vs GIST representation</a>, <a name=bs0055 href=#s0055 class=workspace-trigger>6.2 Proposed representation vs bags of textons on spatial hierarchy</a>, <a name=bs0060 href=#s0060 class=workspace-trigger>6.3 DCT-GIST evaluation on the MIT-67 indoor scene dataset</a> have been performed on representative datasets used as benchmark in the literature. For those tests the DCT-GIST scene context representation has been obtained directly by extracting the DCT information from the compressed domain (JPEG format). The main contribution of this paper is related to the possibility to obtain a signature for the scene context directly into the image generation pipeline of a mobile platform, taking into account the architecture presented in <a name=bs0025 href=#s0025 class=workspace-trigger>Section 5</a><span>. To this aim we have implemented the proposed architecture on a Nokia N900 <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/engineering/smartphone title="Learn more about smartphone from ScienceDirect's AI-generated Topic Pages" class=topic-link>smartphone</a> </span><a name=bbib46 href=#bib46 class=workspace-trigger>[46]</a>. This mobile platform has been chosen because it has less computational power of the other smartphones (i.e., the scene context classification engine should able to classify in real-time independently of the computational power of the device). Moreover, with the chosen mobile platform, the FCam API can be employed to work within the Image Generation Pipeline of the device <a name=bbib47 href=#bib47 class=workspace-trigger>[47]</a>, <a name=bbib48 href=#bib48 class=workspace-trigger>[48]</a>. This allows to effectively build the proposed architecture and test it with real settings. Although the limited resources of the hand-held device, the implemented system works in real-time as demonstrated by the video available at the following URL: <a href=http://iplab.dmi.unict.it/DCT-GIST target=_blank rel="noreferrer noopener">http://iplab.dmi.unict.it/DCT-GIST</a>.<div><p id=p0315>For the implemented system we have used a SVM model learned offline on the 8 Scene Context Dataset (see <a name=bs0050 href=#s0050 class=workspace-trigger>Section 6.1</a>) and the configuration (F) for the DCT-GIST representation (see <a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>). The scene context representation is computed on the fly during the generation of the image to be displayed in the viewfinder. The implemented architecture can also perform classification of images already stored in the mobile (<a name=bf0045 href=#f0045 class=workspace-trigger>Fig. 9</a>).<figure class="figure text-xs" id=f0045><span><img src=data:null;base64, height=181 alt aria-describedby=cap0045><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr9_lrg.jpg target=_blank download title="Download high-res image (206KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (206KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S003132031400199X-gr9.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cap0045><p id=sp0045><span class=label>Fig.&nbsp;9</span>. Example scene context <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification title="Learn more about classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification</a> of the system implemented on the Nokia N900.</p></span></span></figure></div><p id=p0320><span>The proposed DCT-GIST based scene context classifier has been also tested on a NovaThor U9500 with <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/android title="Learn more about Android from ScienceDirect's AI-generated Topic Pages" class=topic-link>Android</a> OS. The board mounts a 1</span>&nbsp;<span>GHz Dual-core ARM Cortex-A9 CPU. The computational time performances have been evaluated by considering the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/average-latency title="Learn more about average latencies from ScienceDirect's AI-generated Topic Pages" class=topic-link>average latencies</a> of the different scene classification blocks on a set of QVGA images. We have measured the computational time of all the steps involved in the scene classification engine: DCT computation, DCT-GIST image representation with configuration (F) (see </span><a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>) and the SVM classification. The DCT computation required 15.6&nbsp;ms on the average (this value could be disregarded when DCT coefficients are directly provided by the integrated JPEG encoder or by working directly on compressed domain). The overall computational time to build the image signature with configuration (F) (i.e., the one with spatial hierarchy and all the three image channels of the image) was only 0.3 ms. Finally, the SVM classification required 117.4&nbsp;ms. This test confirmed that the proposed image signature can be computed in realtime within a mobile platform. Note that the GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> is not suitable for the IGP (i.e., FFT is not present into the IGP) and it is known from <a name=bbib28 href=#bib28 class=workspace-trigger>[28]</a> that the time needed for its computation is <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-84-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mo is="true">&amp;#x2265;</mo><mn is="true">35</mn><mspace width="0.25em" is="true" /><mi is="true">ms</mi></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=8.598ex height=2.202ex viewBox="0 -747.2 3701.9 947.9" role=img focusable=false style=vertical-align:-0.466ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMAIN-2265></use></g><g is=true transform=translate(1056,0)><use href=#MJMAIN-33></use><use href=#MJMAIN-35 x=500 y=0></use></g><g is=true></g><g is=true transform=translate(2473,0)><use href=#MJMAIN-6D></use><use href=#MJMAIN-73 x=833 y=0></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mo is=true>≥</mo><mn is=true>35</mn><mspace width=0.25em is=true></mspace><mi is=true>ms</mi></math></span></span></span> (i.e., higher than the one needed to compute proposed DCT-GIST descriptor).</p></section><section id=s0070><h3 id=sect0085 class="u-h4 u-margin-m-top u-margin-xs-bottom">6.5. Further experiments exploiting convolutional neural network classifier</h3><div><p id=p0325>The proposed DCT-GIST representation can be used with any classifier. The test reported so far have been performed by employing the SVM classifier to compare our approach with respect to the other compared scene descriptors <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a>, <a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. To further test the proposed DCT-GIST representation with respect to the GIST we have employed Convolutional Neural Network as classifier. The results of this comparison are reported in <a name=bt0060 href=#t0060 class=workspace-trigger>Table&nbsp;12</a>, <a name=bt0065 href=#t0065 class=workspace-trigger>Table&nbsp;13</a><span>. Note that the average per class accuracy is in favor of the proposed descriptor. The results obtained with <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/convolutional-neural-network title="Learn more about CNN from ScienceDirect's AI-generated Topic Pages" class=topic-link>CNN</a> are slightly better than the one obtained with SVM in almost all cases.</span><div class="tables frame-topbot rowsep-0 colsep-0" id=t0060><span class=captions><span id=cap0105><p id=sp0105><span class=label>Table&nbsp;12</span>. Comparison of DCT-GIST with respect to GIST <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> employing Convolutional Neural Network classifier on the 8 Scene Dataset.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Proposed DCT-GIST</strong><th scope=col class=align-left><strong>GIST</strong><tbody><tr class=valign-top><td class=align-left><strong>86.49</strong><td class=align-left>86.47</table></div></div><div class="tables frame-topbot rowsep-0 colsep-0" id=t0065><span class=captions><span id=cap0110><p id=sp0110><span class=label>Table&nbsp;13</span>. Comparison of DCT-GIST with respect to GIST <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> employing Convolutional Neural Network classifier on the MIT-67 Dataset.</p></span></span><div class=groups><table><thead><tr class="valign-top rowsep-1"><th scope=col class=align-left><strong>Proposed DCT-GIST</strong><th scope=col class=align-left><strong>GIST</strong><tbody><tr class=valign-top><td class=align-left><strong>28.81</strong><td class=align-left>22.68</table></div></div></div></section></section><section id=s0075><h2 id=sect0090 class="u-h3 u-margin-l-top u-margin-xs-bottom">7. Conclusion and future works</h2><p id=p0330>This paper introduces the DCT-GIST image representation to be exploited for scene context classification on mobile platforms. The proposed scene descriptor is based on the statistics of the DCT coefficients. Starting from the knowledge that the distribution of the AC DCT coefficients can be approximated by Laplacian distributions, and from the observation that different scene context present differences in the Laplacian scales, we proposed a signature of the scene that can be efficiently computed directly in the compressed domain (from JPEG format), as well as in the image generation pipeline of single sensor devices (e.g., smartphones, consumer digital cameras, and wearable smart cameras). The effectiveness of the proposed scene context descriptor has been demonstrated on representative datasets by comparing it with respect to the popular GIST descriptor <a name=bbib12 href=#bib12 class=workspace-trigger>[12]</a> and the representation based on textons distributions on spatial hierarchy <a name=bbib17 href=#bib17 class=workspace-trigger>[17]</a><span> and the ROI+Gist <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/segmentation-model title="Learn more about segmentation model from ScienceDirect's AI-generated Topic Pages" class=topic-link>segmentation model</a> </span><a name=bbib5 href=#bib5 class=workspace-trigger>[5]</a>. Moreover, the proposed scene context recognition architecture has been implemented and tested on a real acquisition pipeline of a mobile phone to demonstrate the real-time performances of the overall system. Differently than other state-of-the-art scene descriptors, the computation of the proposed signature does not need extra information to be stored in memory (e.g., visual vocabulary) or complex operations (e.g., convolutions, FFT, learning phase). The proposed holistic scene representation provides an efficient way to obtain information about the context of the scene which can be extremely useful as first step for object detection and context driven focus attention algorithms by priming typical objects, scales and locations <a name=bbib9 href=#bib9 class=workspace-trigger>[9]</a>, <a name=bbib10 href=#bib10 class=workspace-trigger>[10]</a>. It can be also exploited to have priors for setting the parameters of the algorithm involved in the IGP (e.g., white balance) to improve the quality of the final acquired image <a name=bbib13 href=#bib13 class=workspace-trigger>[13]</a>. Future works could consider the integration of camera metadata related to the image capture conditions to improve recognition accuracy <a name=bbib49 href=#bib49 class=workspace-trigger>[49]</a>, <a name=bbib50 href=#bib50 class=workspace-trigger>[50]</a>.</p></section><section id=s0080><h2 id=sect0095 class="u-h3 u-margin-l-top u-margin-xs-bottom">Conflict of interest</h2><p id=p0335>None declared.</p></section></div><section id=ack0005><h2 id=sect0100 class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><p id=p0340>The authors would like to thank <span id=gsp0005>Nokia Research Center</span> <a name=bbib51 href=#bib51 class=workspace-trigger>[51]</a> for providing the N900 smartphones. This research has been supported by <span id=gsp0010>STMicroelectronics</span> <a name=bbib52 href=#bib52 class=workspace-trigger>[52]</a>.</p></section></div><div class="related-content-links u-hide-from-md sf-hidden"></div><div class=Tail></div><section class="bibliography u-font-serif text-s" id=bibliog0005><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">References</h2><section class=bibliography-sec id=bb0005><dl class=references id=reference-links-bb0005><dt class=label><a href=#bbib1 id=ref-id-bib1 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[1]</a><dd class=reference id=sbref1><div class=contribution>D. Marr<div id=ref-id-sbref1><strong class=title>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</strong></div></div><div class=host>W.H. Freeman, Henry Holt and Co., Inc., New York, NY, USA (1982)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Vision%3A%20A%20Computational%20Investigation%20into%20the%20Human%20Representation%20and%20Processing%20of%20Visual%20Information&amp;publication_year=1982&amp;author=D.%20Marr" aria-describedby=ref-id-sbref1>Google Scholar</a></div><dt class=label><a href=#bbib2 id=ref-id-bib2 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[2]</a><dd class=reference id=othref0005><span>I. Biederman, Aspects and extension of a theory of human image understanding, in: Computational Processes in Human Vision: An Interdisciplinary Perspective, 1988.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=I.%20Biederman,%20Aspects%20and%20extension%20of%20a%20theory%20of%20human%20image%20understanding,%20in:%20Computational%20Processes%20in%20Human%20Vision:%20An%20Interdisciplinary%20Perspective,%201988.">Google Scholar</a></div><dt class=label><a href=#bbib3 id=ref-id-bib3 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[3]</a><dd class=reference id=sbref3><div class=contribution>A. Oliva, A. Torralba<div id=ref-id-sbref3><strong class=title>Building the gist of a scene: the role of global image features in recognition</strong></div></div><div class=host>Vis. Percept.: Prog. Brain Res., 155 (2006), pp. 251-256</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Building%20the%20gist%20of%20a%20scene%3A%20the%20role%20of%20global%20image%20features%20in%20recognition&amp;publication_year=2006&amp;author=A.%20Oliva&amp;author=A.%20Torralba" aria-describedby=ref-id-sbref3>Google Scholar</a></div><dt class=label><a href=#bbib4 id=ref-id-bib4 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[4]</a><dd class=reference id=sbref4><div class=contribution>J. Vogel, A. Schwaninger, C. Wallraven, H.H. Bülthoff<div id=ref-id-sbref4><strong class=title>Categorization of natural scenes: local versus global information and the role of color</strong></div></div><div class=host>ACM Trans. Appl. Percept., 4 (3) (2007), pp. 1-21</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.14452/MR-058-08-2007-01_1></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.14452/MR-058-08-2007-01_1 aria-describedby=ref-id-sbref4>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Categorization%20of%20natural%20scenes%3A%20local%20versus%20global%20information%20and%20the%20role%20of%20color&amp;publication_year=2007&amp;author=J.%20Vogel&amp;author=A.%20Schwaninger&amp;author=C.%20Wallraven&amp;author=H.H.%20B%C3%BClthoff" aria-describedby=ref-id-sbref4>Google Scholar</a></div><dt class=label><a href=#bbib5 id=ref-id-bib5 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[5]</a><dd class=reference id=othref0010><span>A. Quattoni, A. Torralba, Recognizing indoor scenes, in: IEEE Conference on Computer Vision and Pattern Recognition, 2009.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Quattoni,%20A.%20Torralba,%20Recognizing%20indoor%20scenes,%20in:%20IEEE%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition,%202009.">Google Scholar</a></div><dt class=label><a href=#bbib6 id=ref-id-bib6 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[6]</a><dd class=reference id=othref0015><span>A. Torralba, K.P. Murphy, W.T. Freeman, M.A. Rubin, Context-based vision system for place and object recognition, in: IEEE International Conference on Computer Vision (ICCV-03), 2003, pp. 273–280.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Torralba,%20K.P.%20Murphy,%20W.T.%20Freeman,%20M.A.%20Rubin,%20Context-based%20vision%20system%20for%20place%20and%20object%20recognition,%20in:%20IEEE%20International%20Conference%20on%20Computer%20Vision%20,%202003,%20pp.%20273280.">Google Scholar</a></div><dt class=label><a href=#bbib7 id=ref-id-bib7 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[7]</a><dd class=reference id=sbref7><div class=contribution>P. Ladret, A. Guérin-Dugué<div id=ref-id-sbref7><strong class=title>Categorisation and retrieval of scene photographs from JPEG compressed database</strong></div></div><div class=host>Pattern Anal. Appl., 4 (2001), pp. 185-199</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0039840395></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0039840395&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref7>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Categorisation%20and%20retrieval%20of%20scene%20photographs%20from%20JPEG%20compressed%20database&amp;publication_year=2001&amp;author=P.%20Ladret&amp;author=A.%20Gu%C3%A9rin-Dugu%C3%A9" aria-describedby=ref-id-sbref7>Google Scholar</a></div><dt class=label><a href=#bbib8 id=ref-id-bib8 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[8]</a><dd class=reference id=sbref8><div class=contribution>J. Vogel, B. Schiele<div id=ref-id-sbref8><strong class=title>Semantic modeling of natural scenes for content-based image retrieval</strong></div></div><div class=host>Int. J. Comput. Vis., 72 (2) (2007), pp. 133-157</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/s11263-006-8614-1></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/s11263-006-8614-1 aria-describedby=ref-id-sbref8>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33846249578&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref8>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Semantic%20modeling%20of%20natural%20scenes%20for%20content-based%20image%20retrieval&amp;publication_year=2007&amp;author=J.%20Vogel&amp;author=B.%20Schiele" aria-describedby=ref-id-sbref8>Google Scholar</a></div><dt class=label><a href=#bbib9 id=ref-id-bib9 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[9]</a><dd class=reference id=sbref9><div class=contribution>A. Torralba<div id=ref-id-sbref9><strong class=title>Contextual priming for object detection</strong></div></div><div class=host>Int. J. Comput. Vis., 53 (2) (2003), pp. 169-191</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0037500818></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0037500818&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref9>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Contextual%20priming%20for%20object%20detection&amp;publication_year=2003&amp;author=A.%20Torralba" aria-describedby=ref-id-sbref9>Google Scholar</a></div><dt class=label><a href=#bbib10 id=ref-id-bib10 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[10]</a><dd class=reference id=othref0020><span>A. Torralba, S. Pawan, Statistical context priming for object detection, in: IEEE International Conference on Computer Vision, 2001.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Torralba,%20S.%20Pawan,%20Statistical%20context%20priming%20for%20object%20detection,%20in:%20IEEE%20International%20Conference%20on%20Computer%20Vision,%202001.">Google Scholar</a></div><dt class=label><a href=#bbib11 id=ref-id-bib11 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[11]</a><dd class=reference id=sbref11><div class=contribution>A. Torralba, A. Oliva<div id=ref-id-sbref11><strong class=title>Depth estimation from image structure</strong></div></div><div class=host>IEEE Trans. Pattern Anal. Mach. Intell., 24 (9) (2002), pp. 1226-1238</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0036709043></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0036709043&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref11>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Depth%20estimation%20from%20image%20structure&amp;publication_year=2002&amp;author=A.%20Torralba&amp;author=A.%20Oliva" aria-describedby=ref-id-sbref11>Google Scholar</a></div><dt class=label><a href=#bbib12 id=ref-id-bib12 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[12]</a><dd class=reference id=sbref12><div class=contribution>A. Oliva, A. Torralba<div id=ref-id-sbref12><strong class=title>Modeling the shape of the scene: a holistic representation of the spatial envelope</strong></div></div><div class=host>Int. J. Comput. Vis., 42 (2001), pp. 145-175</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0035328421></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0035328421&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref12>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Modeling%20the%20shape%20of%20the%20scene%3A%20a%20holistic%20representation%20of%20the%20spatial%20envelope&amp;publication_year=2001&amp;author=A.%20Oliva&amp;author=A.%20Torralba" aria-describedby=ref-id-sbref12>Google Scholar</a></div><dt class=label><a href=#bbib13 id=ref-id-bib13 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[13]</a><dd class=reference id=sbref13><div class=contribution>S. Bianco, G. Ciocca, C. Cusano, R. Schettini<div id=ref-id-sbref13><strong class=title>Improving color constancy using indoor–outdoor image classification</strong></div></div><div class=host>IEEE Trans. Image Process., 17 (12) (2008), pp. 2381-2392</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-57049165387></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-57049165387&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref13>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Improving%20color%20constancy%20using%20indooroutdoor%20image%20classification" aria-describedby=ref-id-sbref13>Google Scholar</a></div><dt class=label><a href=#bbib14 id=ref-id-bib14 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[14]</a><dd class=reference id=sbref14><div class=contribution>S. Battiato, A.R. Bruna, G. Messina, G. Puglisi<div id=ref-id-sbref14><strong class=title>Image Processing for Embedded Devices</strong></div></div><div class=host>Bentham Science Publisher, Bentham Science Publishers, The Netherlands (2010)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Image%20Processing%20for%20Embedded%20Devices&amp;publication_year=2010&amp;author=S.%20Battiato&amp;author=A.R.%20Bruna&amp;author=G.%20Messina&amp;author=G.%20Puglisi" aria-describedby=ref-id-sbref14>Google Scholar</a></div><dt class=label><a href=#bbib15 id=ref-id-bib15 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[15]</a><dd class=reference id=sbref15><div class=contribution>G.M. Farinella, S. Battiato<div id=ref-id-sbref15><strong class=title>Scene classification in compressed and constrained domain</strong></div></div><div class=host>IET Comput. Vis., 5 (5) (2011), pp. 320-334</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1049/iet-cvi.2010.0056></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1049/iet-cvi.2010.0056 aria-describedby=ref-id-sbref15>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-80053274430&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref15>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Scene%20classification%20in%20compressed%20and%20constrained%20domain&amp;publication_year=2011&amp;author=G.M.%20Farinella&amp;author=S.%20Battiato" aria-describedby=ref-id-sbref15>Google Scholar</a></div><dt class=label><a href=#bbib16 id=ref-id-bib16 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[16]</a><dd class=reference id=sbref16><div class=contribution>E.Y. Lam, J.W. Goodman<div id=ref-id-sbref16><strong class=title>A mathematical analysis of the DCT coefficient distributions for images</strong></div></div><div class=host>IEEE Trans. Image Process., 9 (10) (2000), pp. 1661-1666</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0034298708></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0034298708&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref16>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20mathematical%20analysis%20of%20the%20DCT%20coefficient%20distributions%20for%20images&amp;publication_year=2000&amp;author=E.Y.%20Lam&amp;author=J.W.%20Goodman" aria-describedby=ref-id-sbref16>Google Scholar</a></div><dt class=label><a href=#bbib17 id=ref-id-bib17 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[17]</a><dd class=reference id=sbref17><div class=contribution>S. Battiato, G.M. Farinella, G. Gallo, D. Ravì<div id=ref-id-sbref17><strong class=title>Exploiting textons distributions on spatial hierarchy for scene classification</strong></div></div><div class=host>Eurasip J. Image Video Process. (2010), pp. 1-13</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1155/2010/919367></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1155/2010/919367 aria-describedby=ref-id-sbref17>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Exploiting%20textons%20distributions%20on%20spatial%20hierarchy%20for%20scene%20classification&amp;publication_year=2010&amp;author=S.%20Battiato&amp;author=G.M.%20Farinella&amp;author=G.%20Gallo&amp;author=D.%20Rav%C3%AC" aria-describedby=ref-id-sbref17>Google Scholar</a></div><dt class=label><a href=#bbib18 id=ref-id-bib18 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[18]</a><dd class=reference id=othref0025><span>S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial pyramid matching for recognizing natural scene categories, in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR-06), 2006, pp. 2169–2178.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=S.%20Lazebnik,%20C.%20Schmid,%20J.%20Ponce,%20Beyond%20bags%20of%20features:%20spatial%20pyramid%20matching%20for%20recognizing%20natural%20scene%20categories,%20in:%20IEEE%20International%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20,%202006,%20pp.%2021692178.">Google Scholar</a></div><dt class=label><a href=#bbib19 id=ref-id-bib19 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[19]</a><dd class=reference id=sbref19><div class=contribution>L.W. Renninger, J. Malik<div id=ref-id-sbref19><strong class=title>When is scene recognition just texture recognition?</strong></div></div><div class=host>Vis. Res., 44 (2004), pp. 2301-2311</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0042698904001919 aria-describedby=ref-id-sbref19>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0042698904001919/pdfft?md5=b3ed533a39e61102f71b2c239ad3cfaf&amp;pid=1-s2.0-S0042698904001919-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-2942687330&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref19>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=When%20is%20scene%20recognition%20just%20texture%20recognition" aria-describedby=ref-id-sbref19>Google Scholar</a></div><dt class=label><a href=#bbib20 id=ref-id-bib20 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[20]</a><dd class=reference id=othref0030><span>A. Bosch, A. Zisserman, X. Muñoz, Scene classification via PLSA, in: European Conference on Computer Vision (ECCV-06), 2006, pp. 517–530.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Bosch,%20A.%20Zisserman,%20X.%20Mu%C3%B1oz,%20Scene%20classification%20via%20PLSA,%20in:%20European%20Conference%20on%20Computer%20Vision%20,%202006,%20pp.%20517530.">Google Scholar</a></div><dt class=label><a href=#bbib21 id=ref-id-bib21 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[21]</a><dd class=reference id=othref0035><span>S. Battiato, G.M. Farinella, G. Gallo, D. Ravì, Scene categorization using bag of textons on spatial hierarchy, in: IEEE International Conference on Image Processing, 2008, pp. 2536–2539.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=S.%20Battiato,%20G.M.%20Farinella,%20G.%20Gallo,%20D.%20Rav%C3%AC,%20Scene%20categorization%20using%20bag%20of%20textons%20on%20spatial%20hierarchy,%20in:%20IEEE%20International%20Conference%20on%20Image%20Processing,%202008,%20pp.%2025362539.">Google Scholar</a></div><dt class=label><a href=#bbib22 id=ref-id-bib22 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[22]</a><dd class=reference id=othref0040><span>G. Csurka, C. Dance, L. Fan, J. Willamowski, C. Bray, Visual categorization with bags of keypoints, in: ECCV International Workshop on Statistical Learning in Computer Vision, 2004, pp. 1–22.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=G.%20Csurka,%20C.%20Dance,%20L.%20Fan,%20J.%20Willamowski,%20C.%20Bray,%20Visual%20categorization%20with%20bags%20of%20keypoints,%20in:%20ECCV%20International%20Workshop%20on%20Statistical%20Learning%20in%20Computer%20Vision,%202004,%20pp.%20122.">Google Scholar</a></div><dt class=label><a href=#bbib23 id=ref-id-bib23 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[23]</a><dd class=reference id=othref0045><span>S. Battiato, G.M. Farinella, G. Gallo, D. Ravì, Spatial hierarchy of textons distributions for scene classification, in: International Conference on MultiMedia Modeling, Lecture Notes in Computer Science, vol. 5371, 2009, pp. 333–343.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=S.%20Battiato,%20G.M.%20Farinella,%20G.%20Gallo,%20D.%20Rav%C3%AC,%20Spatial%20hierarchy%20of%20textons%20distributions%20for%20scene%20classification,%20in:%20International%20Conference%20on%20MultiMedia%20Modeling,%20Lecture%20Notes%20in%20Computer%20Science,%20vol.%205371,%202009,%20pp.%20333343.">Google Scholar</a></div><dt class=label><a href=#bbib24 id=ref-id-bib24 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[24]</a><dd class=reference id=sbref24><div class=contribution>A. Bosch, X. Munoz, R. Martí<div id=ref-id-sbref24><strong class=title>Review: which is the best way to organize/classify images by content?</strong></div></div><div class=host>Image Vis. Comput., 25 (6) (2007), pp. 778-791</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0262885606002253 aria-describedby=ref-id-sbref24>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0262885606002253/pdfft?md5=5c5326c95f96d947d9dad16ee0cb9764&amp;pid=1-s2.0-S0262885606002253-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-34047192082&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref24>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Review:%20which%20is%20the%20best%20way%20to%20organizeclassify%20images%20by%20content" aria-describedby=ref-id-sbref24>Google Scholar</a></div><dt class=label><a href=#bbib25 id=ref-id-bib25 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[25]</a><dd class=reference id=sbref25><div class=contribution>A. Torralba, A. Oliva<div id=ref-id-sbref25><strong class=title>Statistics of natural image categories</strong></div></div><div class=host>Netw.: Comput. Neural Syst., 14 (2003), pp. 391-412</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0142261230></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0142261230&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref25>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Statistics%20of%20natural%20image%20categories&amp;publication_year=2003&amp;author=A.%20Torralba&amp;author=A.%20Oliva" aria-describedby=ref-id-sbref25>Google Scholar</a></div><dt class=label><a href=#bbib26 id=ref-id-bib26 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[26]</a><dd class=reference id=othref0050><span>A. Torralba, A. Oliva, Semantic organization of scenes using discriminant structural templates, in: IEEE International Conference on Computer Vision (ICCV-99), 1999, pp. 1253–1258.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Torralba,%20A.%20Oliva,%20Semantic%20organization%20of%20scenes%20using%20discriminant%20structural%20templates,%20in:%20IEEE%20International%20Conference%20on%20Computer%20Vision%20,%201999,%20pp.%2012531258.">Google Scholar</a></div><dt class=label><a href=#bbib27 id=ref-id-bib27 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[27]</a><dd class=reference id=othref0055><span>H. Grabner, F. Nater, M. Druey, L.V. Gool, Visual interestingness in image sequences, in: ACM International Conference on Multimedia, 2013.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=H.%20Grabner,%20F.%20Nater,%20M.%20Druey,%20L.V.%20Gool,%20Visual%20interestingness%20in%20image%20sequences,%20in:%20ACM%20International%20Conference%20on%20Multimedia,%202013.">Google Scholar</a></div><dt class=label><a href=#bbib28 id=ref-id-bib28 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[28]</a><dd class=reference id=othref0060><span>M. Douze, H. Jégou, H. Sandhawalia, L. Amsaleg, C. Schmid, Evaluation of GIST descriptors for web-scale image search, in: ACM International Conference on Image and Video Retrieval, 2009, pp. 1–8.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=M.%20Douze,%20H.%20J%C3%A9gou,%20H.%20Sandhawalia,%20L.%20Amsaleg,%20C.%20Schmid,%20Evaluation%20of%20GIST%20descriptors%20for%20web-scale%20image%20search,%20in:%20ACM%20International%20Conference%20on%20Image%20and%20Video%20Retrieval,%202009,%20pp.%2018.">Google Scholar</a></div><dt class=label><a href=#bbib29 id=ref-id-bib29 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[29]</a><dd class=reference id=othref0065><span>Z. Lu, K. Grauman, Story-driven summarization for egocentric video, in: IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 2714–2721.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Z.%20Lu,%20K.%20Grauman,%20Story-driven%20summarization%20for%20egocentric%20video,%20in:%20IEEE%20International%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition%20,%202013,%20pp.%2027142721.">Google Scholar</a></div><dt class=label><a href=#bbib30 id=ref-id-bib30 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[30]</a><dd class=reference id=sbref30><div class=contribution>J. Luo, M.R. Boutell<div id=ref-id-sbref30><strong class=title>Natural scene classification using overcomplete ICA</strong></div></div><div class=host>Pattern Recognit., 38 (10) (2005), pp. 1507-1519</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320305001275 aria-describedby=ref-id-sbref30>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320305001275/pdfft?md5=bfc9a79184e9edcdadf8f76fe9d1413d&amp;pid=1-s2.0-S0031320305001275-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-22844444560&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref30>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Natural%20scene%20classification%20using%20overcomplete%20ICA&amp;publication_year=2005&amp;author=J.%20Luo&amp;author=M.R.%20Boutell" aria-describedby=ref-id-sbref30>Google Scholar</a></div><dt class=label><a href=#bbib31 id=ref-id-bib31 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[31]</a><dd class=reference id=othref0070><span>G. Farinella, S. Battiato, G. Gallo, R. Cipolla, Natural versus artificial scene classification by ordering discrete Fourier power spectra, in: Structural, Syntactic, and Statistical Pattern Recognition, Lecture Notes in Computer Science, vol. 5342, Springer, Berlin, Heidelberg, 2008, pp. 137–146.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=G.%20Farinella,%20S.%20Battiato,%20G.%20Gallo,%20R.%20Cipolla,%20Natural%20versus%20artificial%20scene%20classification%20by%20ordering%20discrete%20Fourier%20power%20spectra,%20in:%20Structural,%20Syntactic,%20and%20Statistical%20Pattern%20Recognition,%20Lecture%20Notes%20in%20Computer%20Science,%20vol.%205342,%20Springer,%20Berlin,%20Heidelberg,%202008,%20pp.%20137146.">Google Scholar</a></div><dt class=label><a href=#bbib32 id=ref-id-bib32 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[32]</a><dd class=reference id=sbref32><div class=contribution>G.K. Wallace<div id=ref-id-sbref32><strong class=title>The JPEG still picture compression standard</strong></div></div><div class=host>Commun. ACM, 34 (4) (1991), pp. 18-34</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0008733891></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0008733891&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref32>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20JPEG%20still%20picture%20compression%20standard&amp;publication_year=1991&amp;author=G.K.%20Wallace" aria-describedby=ref-id-sbref32>Google Scholar</a></div><dt class=label><a href=#bbib33 id=ref-id-bib33 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[33]</a><dd class=reference id=sbref33><div class=contribution>W.K. Pratt<div id=ref-id-sbref33><strong class=title>Digital Image Processing</strong></div></div><div class=host>John Wiley &amp; Sons, Inc., New York, NY, USA (1978)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Digital%20Image%20Processing&amp;publication_year=1978&amp;author=W.K.%20Pratt" aria-describedby=ref-id-sbref33>Google Scholar</a></div><dt class=label><a href=#bbib34 id=ref-id-bib34 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[34]</a><dd class=reference id=sbref34><div class=contribution>J.D. Eggerton<div id=ref-id-sbref34><strong class=title>Statistical distributions of image DCT coefficients</strong></div></div><div class=host>Comput. Electr. Eng., 12 (1986), pp. 137-145</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/0045790686900054 aria-describedby=ref-id-sbref34>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/0045790686900054/pdf?md5=f2e5f3ef3b9c999d06156603acfb209d&amp;pid=1-s2.0-0045790686900054-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0022875595&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref34>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Statistical%20distributions%20of%20image%20DCT%20coefficients&amp;publication_year=1986&amp;author=J.D.%20Eggerton" aria-describedby=ref-id-sbref34>Google Scholar</a></div><dt class=label><a href=#bbib35 id=ref-id-bib35 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[35]</a><dd class=reference id=sbref35><div class=contribution>F. Müller<div id=ref-id-sbref35><strong class=title>Distribution shape of two-dimensional DCT coefficients of natural images</strong></div></div><div class=host>Electron. Lett., 29 (1993), pp. 1935-1936</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1049/el:19931288></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1049/el:19931288 aria-describedby=ref-id-sbref35>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0027681636&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref35>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Distribution%20shape%20of%20two-dimensional%20DCT%20coefficients%20of%20natural%20images&amp;publication_year=1993&amp;author=F.%20M%C3%BCller" aria-describedby=ref-id-sbref35>Google Scholar</a></div><dt class=label><a href=#bbib36 id=ref-id-bib36 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[36]</a><dd class=reference id=othref0075><span>T. Eude, R. Grisel, H. Cherifi, R. Debrie, On the distribution of the DCT coefficients, in: IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 5, 1994, pp. 365–368.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=T.%20Eude,%20R.%20Grisel,%20H.%20Cherifi,%20R.%20Debrie,%20On%20the%20distribution%20of%20the%20DCT%20coefficients,%20in:%20IEEE%20International%20Conference%20on%20Acoustics,%20Speech,%20and%20Signal%20Processing,%20vol.%205,%201994,%20pp.%20365368.">Google Scholar</a></div><dt class=label><a href=#bbib37 id=ref-id-bib37 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[37]</a><dd class=reference id=othref0080><span>S. Smoot, L.A. Rowe, Study of DCT coefficient distributions, in: SPIE Symposium on Electronic Imaging, vol. 2657, 1996, pp. 403–411.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=S.%20Smoot,%20L.A.%20Rowe,%20Study%20of%20DCT%20coefficient%20distributions,%20in:%20SPIE%20Symposium%20on%20Electronic%20Imaging,%20vol.%202657,%201996,%20pp.%20403411.">Google Scholar</a></div><dt class=label><a href=#bbib38 id=ref-id-bib38 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[38]</a><dd class=reference id=othref0085><span>G.S. Yovanof, S. Liu, Statistical analysis of the DCT coefficients and their quantization, in: Conference Record of the Thirtieth Asilomar Conference on Signals, Systems and Computers, 1996.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=G.S.%20Yovanof,%20S.%20Liu,%20Statistical%20analysis%20of%20the%20DCT%20coefficients%20and%20their%20quantization,%20in:%20Conference%20Record%20of%20the%20Thirtieth%20Asilomar%20Conference%20on%20Signals,%20Systems%20and%20Computers,%201996.">Google Scholar</a></div><dt class=label><a href=#bbib39 id=ref-id-bib39 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[39]</a><dd class=reference id=sbref39><div class=contribution>C.-C. Chang, J.-C. Chuang, Y.-S. Hu<div id=ref-id-sbref39><strong class=title>Retrieving digital images from a JPEG compressed image database</strong></div></div><div class=host>Image Vis. Comput., 22 (6) (2004), pp. 471-484</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0262885603002427 aria-describedby=ref-id-sbref39>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0262885603002427/pdfft?md5=6714a0a44618b8ff12eae17f62e68037&amp;pid=1-s2.0-S0262885603002427-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-1342329566&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref39>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Retrieving%20digital%20images%20from%20a%20JPEG%20compressed%20image%20database&amp;publication_year=2004&amp;author=C.-C.%20Chang&amp;author=J.-C.%20Chuang&amp;author=Y.-S.%20Hu" aria-describedby=ref-id-sbref39>Google Scholar</a></div><dt class=label><a href=#bbib40 id=ref-id-bib40 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[40]</a><dd class=reference id=sbref40><div class=contribution>G. Schaefer<div id=ref-id-sbref40><strong class=title>Content-based image retrieval: advanced topics</strong></div></div><div class=host>T. Czachórski, S. Kozielski, U. Stańczyk (Eds.), Man-Machine Interactions 2, Advances in Intelligent and Soft Computing, vol. 103, Springer, Berlin Heidelberg (2011), pp. 31-37</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/978-3-642-23169-8_4></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/978-3-642-23169-8_4 aria-describedby=ref-id-sbref40>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-80052934413&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref40>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Content-based%20image%20retrieval%3A%20advanced%20topics&amp;publication_year=2011&amp;author=G.%20Schaefer" aria-describedby=ref-id-sbref40>Google Scholar</a></div><dt class=label><a href=#bbib41 id=ref-id-bib41 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[41]</a><dd class=reference id=sbref41><div class=contribution>E. Lam<div id=ref-id-sbref41><strong class=title>Analysis of the DCT coefficient distributions for document coding</strong></div></div><div class=host>IEEE Signal Process. Lett., 11 (2) (2004), pp. 97-100</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0442279698></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0442279698&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref41>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Analysis%20of%20the%20DCT%20coefficient%20distributions%20for%20document%20coding&amp;publication_year=2004&amp;author=E.%20Lam" aria-describedby=ref-id-sbref41>Google Scholar</a></div><dt class=label><a href=#bbib42 id=ref-id-bib42 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[42]</a><dd class=reference id=sbref42><div class=contribution>R.M. Norton<div id=ref-id-sbref42><strong class=title>The double exponential distribution: using calculus to find a maximum likelihood estimator</strong></div></div><div class=host>Am. Stat., 38 (2) (1984), pp. 135-136</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-28344431762></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-28344431762&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref42>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20double%20exponential%20distribution%3A%20using%20calculus%20to%20find%20a%20maximum%20likelihood%20estimator&amp;publication_year=1984&amp;author=R.M.%20Norton" aria-describedby=ref-id-sbref42>Google Scholar</a></div><dt class=label><a href=#bbib43 id=ref-id-bib43 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[43]</a><dd class=reference id=othref0090><span>G. Yu, G. Sapiro, S. Mallat, Image modeling and enhancement via structured sparse model selection, in: IEEE International Conference on Image Processing (ICIP), 2010, pp. 1641–1644.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=G.%20Yu,%20G.%20Sapiro,%20S.%20Mallat,%20Image%20modeling%20and%20enhancement%20via%20structured%20sparse%20model%20selection,%20in:%20IEEE%20International%20Conference%20on%20Image%20Processing%20,%202010,%20pp.%2016411644.">Google Scholar</a></div><dt class=label><a href=#bbib44 id=ref-id-bib44 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[44]</a><dd class=reference id=sbref44><div class=contribution>P. Viola, M.J. Jones<div id=ref-id-sbref44><strong class=title>Robust real-time face detection</strong></div></div><div class=host>Int. J. Comput. Vis., 57 (2) (2004), pp. 137-154</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Robust%20real-time%20face%20detection&amp;publication_year=2004&amp;author=P.%20Viola&amp;author=M.J.%20Jones" aria-describedby=ref-id-sbref44>Google Scholar</a></div><dt class=label><a href=#bbib45 id=ref-id-bib45 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[45]</a><dd class=reference id=othref0095><span>A. Oliva, A. Torralba, Gist Descriptor, 2001, URL 〈<a href=http://people.csail.mit.edu.simsrad.net.ocs.mq.edu.au/torralba/code/spatialenvelope/ target=_blank rel="noreferrer noopener">http://people.csail.mit.edu.simsrad.net.ocs.mq.edu.au/torralba/code/spatialenvelope/</a>〉.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A.%20Oliva,%20A.%20Torralba,%20Gist%20Descriptor,%202001,%20URL%20http:people.csail.mit.edu.simsrad.net.ocs.mq.edu.autorralbacodespatialenvelope.">Google Scholar</a></div><dt class=label><a href=#bbib46 id=ref-id-bib46 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[46]</a><dd class=reference id=othref0100><span>S. Battiato, G. Farinella, M. Guarnera, D. Ravì, V. Tomaselli, Instant scene recognition on mobile platform, in: European Conference on Computer Vision (ECCV) – Workshops and Demonstrations, Lecture Notes in Computer Science, vol. 7585, 2012, pp. 655–658.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=S.%20Battiato,%20G.%20Farinella,%20M.%20Guarnera,%20D.%20Rav%C3%AC,%20V.%20Tomaselli,%20Instant%20scene%20recognition%20on%20mobile%20platform,%20in:%20European%20Conference%20on%20Computer%20Vision%20%20%20Workshops%20and%20Demonstrations,%20Lecture%20Notes%20in%20Computer%20Science,%20vol.%207585,%202012,%20pp.%20655658.">Google Scholar</a></div><dt class=label><a href=#bbib47 id=ref-id-bib47 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[47]</a><dd class=reference id=sbref47><div class=contribution>A. Adams, E.-V. Talvala, S.H. Park, D.E. Jacobs, B. Ajdin, N. Gelfand, J. Dolson, D. Vaquero, J. Baek, M. Tico, H.P.A. Lensch, W. Matusik, K. Pulli, M. Horowitz, M. Levoy<div id=ref-id-sbref47><strong class=title>The frankencamera: an experimental platform for computational photography</strong></div></div><div class=host>ACM Trans. Gr., 29 (4) (2010), pp. 1-12</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20frankencamera%3A%20an%20experimental%20platform%20for%20computational%20photography&amp;publication_year=2010&amp;author=A.%20Adams&amp;author=E.-V.%20Talvala&amp;author=S.H.%20Park&amp;author=D.E.%20Jacobs&amp;author=B.%20Ajdin&amp;author=N.%20Gelfand&amp;author=J.%20Dolson&amp;author=D.%20Vaquero&amp;author=J.%20Baek&amp;author=M.%20Tico&amp;author=H.P.A.%20Lensch&amp;author=W.%20Matusik&amp;author=K.%20Pulli&amp;author=M.%20Horowitz&amp;author=M.%20Levoy" aria-describedby=ref-id-sbref47>Google Scholar</a></div><dt class=label><a href=#bbib48 id=ref-id-bib48 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[48]</a><dd class=reference id=othref0105><span>F. Garage, Fcam api, 〈<a href=http://fcam.garage.maemo.org/ target=_blank rel="noreferrer noopener">http://fcam.garage.maemo.org/</a>〉, 2012.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=F.%20Garage,%20Fcam%20api,%20http:fcam.garage.maemo.org,%202012.">Google Scholar</a></div><dt class=label><a href=#bbib49 id=ref-id-bib49 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[49]</a><dd class=reference id=othref0110><span>M. Boutell, J. Luo, Bayesian fusion of camera metadata cues in semantic scene classification, in: IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004, pp. 623–630.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=M.%20Boutell,%20J.%20Luo,%20Bayesian%20fusion%20of%20camera%20metadata%20cues%20in%20semantic%20scene%20classification,%20in:%20IEEE%20Computer%20Society%20Conference%20on%20Computer%20Vision%20and%20Pattern%20Recognition,%202004,%20pp.%20623630.">Google Scholar</a></div><dt class=label><a href=#bbib50 id=ref-id-bib50 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[50]</a><dd class=reference id=sbref50><div class=contribution>M. Boutell, J. Luo<div id=ref-id-sbref50><strong class=title>Beyond pixels: exploiting camera metadata for photo classification</strong></div></div><div class=host>Pattern Recognit., 38 (6) (2005), pp. 935-946</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320304003978 aria-describedby=ref-id-sbref50>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320304003978/pdfft?md5=625301e0c1b570798aadfcf187c3472d&amp;pid=1-s2.0-S0031320304003978-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-14644422552&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-sbref50>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Beyond%20pixels%3A%20exploiting%20camera%20metadata%20for%20photo%20classification&amp;publication_year=2005&amp;author=M.%20Boutell&amp;author=J.%20Luo" aria-describedby=ref-id-sbref50>Google Scholar</a></div><dt class=label><a href=#bbib51 id=ref-id-bib51 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[51]</a><dd class=reference id=othref0115><span>Nokia Research Center, Visual Computing and Ubiquitous Imaging, 2013, URL 〈<a href=http://research.nokia.com/ target=_blank rel="noreferrer noopener">http://research.nokia.com/</a>〉.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Nokia%20Research%20Center,%20Visual%20Computing%20and%20Ubiquitous%20Imaging,%202013,%20URL%20http:research.nokia.com.">Google Scholar</a></div><dt class=label><a href=#bbib52 id=ref-id-bib52 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">[52]</a><dd class=reference id=othref0120><span>STMicroelectronics, Advanced System Technology – Computer Vision Group, URL 〈<a href=http://www.st.com/ target=_blank rel="noreferrer noopener">http://www.st.com/</a>〉.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=STMicroelectronics,%20Advanced%20System%20Technology%20%20Computer%20Vision%20Group,%20URL%20http:www.st.com.">Google Scholar</a></div></dl></section></section><div id=section-cited-by><section class="ListArticles preview"><div class=PageDivider></div><header id=citing-articles-header><h2 class="u-h3 u-margin-l-ver u-font-serif">Cited by (46)</h2></header><div aria-describedby=citing-articles-header><div class="citing-articles u-margin-l-bottom"><ul><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320322001881 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article0-title>Harmonic convolutional networks based on discrete cosine transform</h3></a><div class="article-source ellipsis">2022, Pattern Recognition</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article0-title aria-controls=citing-articles-article0 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0168169920331112 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article1-title>DropLeaf: A precision farming smartphone tool for real-time quantification of pesticide application coverage</h3></a><div class="article-source ellipsis">2021, Computers and Electronics in Agriculture</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article1-title aria-controls=citing-articles-article1 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S016786551830237X target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article2-title>Market basket analysis from egocentric videos</h3></a><div class="article-source ellipsis">2018, Pattern Recognition Letters</div><div class=CitedSection><div class=snippet><div class=cite-header>Citation Excerpt :</div><p>The research presented in this paper is part of an experimental program carried out by Centro Studi S.r.l. to infer the behavior of customers in real retail stores using egocentric shopping carts. The main contributions of this paper are the following: (1) we present the novel problem of Visual Market Basket Analysis and propose a hierarchy of 14 behaviors to be analysed from egocentric videos; (2) we introduce a novel dataset of 15 videos acquired during real shopping sessions; (3) we investigate a multi-modal classification method which combines visual features such as (GIST [18,19], Deep Features [20]), audio (MFCC [21]) and motion cues (Optical Flow [22]) within the framework of Direct Acyclic Graph SVM [23]. Experimental results highlight that (1) combining features arising from different sources (e.g., images, audio and motion) is useful to improve the performance of the overall system and (2) explicitly considering the hierarchical structure of classes is beneficial in the considered context.</p></div></div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article2-title aria-controls=citing-articles-article2 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S1047320318300269 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article3-title>Personal-location-based temporal segmentation of egocentric videos for lifelogging applications</h3></a><div class="article-source ellipsis">2018, Journal of Visual Communication and Image Representation</div><div class=CitedSection><div class=snippet><div class=cite-header>Citation Excerpt :</div><p>Torralba et al. [25] designed a context-based vision system for place and scene recognition. Farinella et al. [26,27] engineered efficient computational methods for scene recognition which can be easily deployed to embedded devices. Rhinehart et al. [18] explored the relationship between actions and locations to improve both localization and action prediction.</p></div></div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article3-title aria-controls=citing-articles-article3 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/B9780128134450000010 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article4-title>Computer Vision for Sight: Computer Vision Techniques to Assist Visually Impaired People to Navigate in an Indoor Environment</h3></a><div class="article-source ellipsis">2018, Computer Vision For Assistive Healthcare</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article4-title aria-controls=citing-articles-article4 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320317302819 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article5-title>Organizing egocentric videos of daily living activities</h3></a><div class="article-source ellipsis">2017, Pattern Recognition</div><div class=CitedSection><div class=snippet><div class=cite-header>Citation Excerpt :</div><p>Future works can consider to extend the system to perform recognition of contexts from egocentric images [2,31–33] and to recognize the activities performed by the user [14].</p></div></div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article5-title aria-controls=citing-articles-article5 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div></ul><a class="button-alternative button-alternative-secondary button-cited-by-more" href="http://www-scopus-com.simsrad.net.ocs.mq.edu.au/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-84920711406&amp;md5=de32c075963a5ed991861138b138c5df" id=citing-articles-view-all-btn target=_blank><svg focusable=false viewBox="0 0 78 128" width=32 height=32 class="icon icon-arrow-up-right"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg><span class=button-alternative-text>View all citing articles on Scopus</span></a></div></div></section></div><div class=article-biography id=bio0005><p id=sp0125><strong>Giovanni Maria Farinella</strong> received the M.S. degree in computer science (egregia cum laude) from the University of Catania, Italy, in 2004, and the Ph.D. degree in computer science in 2008. He joined the Image Processing Laboratory (IPLAB) at the Department of Mathematics and Computer Science, University of Catania, in 2008, as a Contract Researcher. He is a Contract Professor of Computer Vision at the Academy of Arts of Catania (since 2004) and Adjunct Professor of Computer Science at the University of Catania (since 2008). His research interests lie in the fields of computer vision, pattern recognition and machine learning. He has edited four volumes and coauthored more than 60 papers in international journals, conference proceedings and book chapters. He is a co-inventor of four international patents. He serves as a reviewer and on the programme committee for major international journals and international conferences. He founded (in 2006) and currently directs the International Computer Vision Summer School.</p></div><div class=article-biography id=bio0010><p id=sp0130><strong>Daniele Ravì</strong> was born in Sant’Agata di Militello (ME), Italy, in 1983. He received the Master Degree in Computer Science (summa cum laude) in 2007 from University of Catania. From 2008 to 2010 he worked at STMicroelectronics (Advanced System Technology Imaging Group) as consultant. He recently has finished his Ph.D. in Computer Science and now he is a research engineer at Visual Atoms. His interests lie in the fields of computer vision, image analysis, visual search and machine learning.</p></div><div class=article-biography id=bio0015><p id=sp0135><strong>Valeria Tomaselli</strong> is a SW Design Senior Engineer and Project Leader at STMicroelectronics in Catania. She received the Master Degree in Software Engineering (summa cum laude) in 2003 from University of Catania. From 2003 she has been working at STMicroelectronics, in the Advanced System Technology group, where she researches innovative algorithms for Digital Still Camera and Mobile Imaging applications in the image processing and computer vision fields. She is author of patents and papers about image processing and computer vision, and she also serves as a reviewer. She has been also involved in many national and international research projects.</p></div><div class=article-biography id=bio0020><p id=sp0140><strong>Mirko Guarnera</strong> received his Master Degree in Electronic Engineering from the University of Palermo and the Ph.D. from University of Messina. He joined STMicroelectronics at the AST Labs in Catania in 1999, where he currently holds the position of R&amp;D Project Manager. He is IEEE member and member of the technical committee of SPIE Electronic Imaging – Digital Photography conference. His research interests include image processing and pattern recognition for camera, TV, printers and projectors. He is author of many Papers in journals, book chapters and Patents.</p></div><div class=article-biography id=bio0025><p id=sp0145><strong>Sebastiano Battiato</strong> received his degree in computer science (summa cum laude) in 1995 from University of Catania and his Ph.D. in computer science and applied mathematics from University of Naples in 1999. From 1999 to 2003 he was the leader of the “Imaging” team at STMicroelectronics in Catania. He joined the Department of Mathematics and Computer Science at the University of Catania as assistant professor in 2004 and became associate professor in the same department in 2011. His research interests include image enhancement and processing, image coding, camera imaging technology and multimedia forensics. He has edited 4 books and co-authored more than 150 papers in international journals, conference proceedings and book chapters. He is a co-inventor of about 15 international patents, reviewer for several international journals, and he has been regularly a member of numerous international conference committees. Prof. Battiato has participated in many international and national research projects. Chair of several international events (IWCV2012, ECCV2012, VISAPP 2012–2013–2014, ICIAP 2011, ACM MiFor 2010–2011, SPIE EI Digital Photography 2011–2012–2013, etc.). He is an associate editor of the IEEE Transactions on Circuits and System for Video Technology and of the SPIE Journal of Electronic Imaging. Guest editor of the following special issues: "Emerging Methods for Color Image and Video Quality Enhancement" published on EURASIP Journal on Image and Video Processing (2010) and "Multimedia in Forensics, Security and Intelligence" published on IEEE Multimedia Magazine (2012). He is the recipient of the 2011 Best Associate Editor Award of the IEEE Transactions on Circuits and Systems for Video Technology. He is director (and co-founder) of the International Computer Vision Summer School (ICVSS), Sicily, Italy. He is a senior member of the IEEE.</p></div><div class=Footnotes><dl class=footnote><dt class=footnote-label><sup><a href=#bfn1>1</a></sup><dd class=u-margin-xxl-left><p id=ntp0005>JPEG is the most common used format for images and videos.</p></dl><dl class=footnote><dt class=footnote-label><sup><a href=#bfn2>2</a></sup><dd class=u-margin-xxl-left><p id=ntp0010>Source: <a href=http://w3techs.com/technologies/overview/image_format/all target=_blank rel="noreferrer noopener">http://w3techs.com/technologies/overview/image_format/all</a>. The statistics is computed on the top 10&nbsp;million websites according to the Amazon.com company (Nov 2013).</p></dl><dl class=footnote><dt class=footnote-label><sup><a href=#bfn3>3</a></sup><dd class=u-margin-xxl-left><p id=ntp0015>Note that for the different AC DCT distributions the <em>μ</em> value is not equal to zero.</p></dl><dl class=footnote><dt class=footnote-label><sup><a href=#bfn4>4</a></sup><dd class=u-margin-xxl-left><p id=ntp0020>Note that in the JPEG format the image is converted in the <em>YC</em><sub><em>b</em></sub><em>C</em><sub><em>r</em></sub> color model as first step.</p></dl><dl class=footnote><dt class=footnote-label><sup><a href=#bfn5>5</a></sup><dd class=u-margin-xxl-left><p id=ntp0025>The DCT chrominance exhibits the same distribution as for the luminance channel <a name=bbib16 href=#bib16 class=workspace-trigger>[16]</a>.</p></dl><dl class=footnote><dt class=footnote-label><sup><a href=#bfn6>6</a></sup><dd class=u-margin-xxl-left><p id=ntp0030>Note that we define <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-42-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msup is="true"><mrow is="true"><mi is="true">r</mi></mrow><mrow is="true"><mn is="true">0</mn><mo is="true">,</mo><mi is="true">s</mi></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3.331ex height=2.534ex viewBox="0 -961.2 1434.3 1091" role=img focusable=false style=vertical-align:-0.302ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><g is=true><use href=#MJMATHI-72></use></g></g><g is=true transform=translate(451,362)><g is=true><use transform=scale(0.707) href=#MJMAIN-30></use></g><g is=true transform=translate(353,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(550,0)><use transform=scale(0.707) href=#MJMATHI-73></use></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msup is=true><mrow is=true><mi is=true>r</mi></mrow><mrow is=true><mn is=true>0</mn><mo is=true>,</mo><mi is=true>s</mi></mrow></msup></math></span></span></span> as the entire image under consideration for every <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-43-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">s</mi><mo is="true">&amp;#x2208;</mo><mi is="true">S</mi></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=5.43ex height=2.237ex viewBox="0 -833.3 2338.1 963.2" role=img focusable=false style=vertical-align:-0.302ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-73></use></g><g is=true transform=translate(747,0)><use href=#MJMAIN-2208></use></g><g is=true transform=translate(1692,0)><use href=#MJMATHI-53></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>s</mi><mo is=true>∈</mo><mi is=true>S</mi></math></span></span></span>.</p></dl></div><a class="anchor abstract-link" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/abs/pii/S003132031400199X><span class=anchor-text>View Abstract</span></a><div class=Copyright><span class=copyright-line>Copyright © 2014 Elsevier Ltd. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class=RelatedContent aria-label="Related content"><section class="SidePanel u-margin-s-bottom"><header id=recommended-articles-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type=button><span class=button-link-text><h2 class="section-title u-h4">Recommended articles</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=recommended-articles-header><div id=recommended-articles><ul><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314004336><h3 class="article-title ellipsis text-s" id=recommended-articles-article0-title><span>Study of visual saliency detection via nonlocal anisotropic diffusion equation</span></h3></a><div class="article-source ellipsis"><div class=source>Pattern Recognition, Volume 48, Issue 4, 2015, pp. 1315-1327</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314004336/pdfft?md5=494e450dff49922b5f4ca8214bfee5e8&amp;pid=1-s2.0-S0031320314004336-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article0-title aria-controls=recommended-articles-article0 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article0 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314004476><h3 class="article-title ellipsis text-s" id=recommended-articles-article1-title><span>Errata and comments on “Orthogonal moments based on exponent functions: Exponent-Fourier moments”</span></h3></a><div class="article-source ellipsis"><div class=source>Pattern Recognition, Volume 48, Issue 4, 2015, pp. 1571-1573</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314004476/pdfft?md5=7183e1b6b1a8fc571f2d059950f51857&amp;pid=1-s2.0-S0031320314004476-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article1-title aria-controls=recommended-articles-article1 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article1 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314002167><h3 class="article-title ellipsis text-s" id=recommended-articles-article2-title><span>Connectivity calculus of fractal polyhedrons</span></h3></a><div class="article-source ellipsis"><div class=source>Pattern Recognition, Volume 48, Issue 4, 2015, pp. 1150-1160</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0031320314002167/pdfft?md5=41daff7c1c98d6f8b6e76d60bbf6c0b7&amp;pid=1-s2.0-S0031320314002167-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article2-title aria-controls=recommended-articles-article2 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article2 aria-hidden=true></div></ul></div><div class="pagination u-position-relative u-padding-s-bottom"><span class=u-position-absolute></span><span class=pagination-pages-label><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class=u-position-absolute><button class="button-link button-link-secondary next-button" data-aa-button="sd:product:journal:article:location=recommended-articles:type=Next" type=button><span class=button-link-text>Next</span><svg focusable=false viewBox="0 0 54 128" width=10.125 height=24 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel u-margin-s-bottom"><header id=metrics-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true type=button><span class=button-link-text><h2 class="section-title u-h4">Article Metrics</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=metrics-header><div class=plum-sciencedirect-theme><div class=PlumX-Summary><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top sf-hidden"></div><div class=pps-cols><div class="pps-col plx-citation"><div class=plx-citation><div class=pps-title>Citations</div><ul><li class=plx-citation><span class=pps-label>Citation Indexes: </span><span class=pps-count>40</span></ul></div></div><div class="pps-col plx-capture"><div class=plx-capture><div class=pps-title>Captures</div><ul><li class=plx-capture><span class=pps-label>Exports-Saves: </span><span class=pps-count>2</span><li class=plx-capture><span class=pps-label>Readers: </span><span class=pps-count>37</span></ul></div></div><div class="pps-col plx-socialMedia"><div class=plx-socialMedia><div class=pps-title>Social Media</div><ul><li class=plx-socialMedia><span class=pps-label>Shares, Likes &amp; Comments: </span><span class=pps-count>2</span></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src=data:image/png;base64,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 class=plx-logo></div><a target=_blank href="https://plu.mx/plum/a/?doi=10.1016/j.patcog.2014.05.014&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class=pps-seemore title="PlumX Metrics Detail Page">View details<svg fill=currentColor tabindex=-1 focusable=false width=16 height=16 viewBox="0 0 16 16" class=svg-arrow><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div><div></div></div></div><footer role=contentinfo class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a aria-label="Elsevier home page (opens in a new tab)" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/ target=_blank rel=nofollow><img class=footer-logo src=data:null;base64, alt="Elsevier logo with wordmark" height=64 width=58 loading=lazy></a></div><div class=els-footer-content><div class=u-remove-if-print><ul class="els-footer-links u-margin-xs-bottom" style=list-style:none><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/solutions/sciencedirect id=els-footer-about-science-direct target=_blank rel=nofollow><span class=anchor-text>About ScienceDirect</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=" id=els-footer-remote-access target=_blank rel=nofollow><span class=anchor-text>Remote access</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://sd-cart-elsevier-com.simsrad.net.ocs.mq.edu.au/? id=els-footer-shopping-cart target=_blank rel=nofollow><span class=anchor-text>Shopping cart</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=http://elsmediakits.com/ id=els-footer-advertise target=_blank rel=nofollow><span class=anchor-text>Advertise</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/contact/supporthub/sciencedirect/ id=els-footer-contact-support target=_blank rel=nofollow><span class=anchor-text>Contact and support</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/elsevier-website-terms-and-conditions id=els-footer-terms-condition target=_blank rel=nofollow><span class=anchor-text>Terms and conditions</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/privacy-policy id=els-footer-privacy-policy target=_blank rel=nofollow><span class=anchor-text>Privacy policy</span></a></ul></div><p id=els-footer-cookie-message class=u-remove-if-print>We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the <a class="anchor u-clr-grey8 u-margin-0-right" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/legal/use-of-cookies target=_blank rel=nofollow><span class=anchor-text><strong>use of cookies</strong></span></a>.<p id=els-footer-copyright>Copyright © 2022 Elsevier B.V. or its licensors or contributors. <span class=u-remove-if-print>ScienceDirect® is a registered trademark of Elsevier B.V.</span><p class="u-remove-if-not-print sf-hidden">ScienceDirect® is a registered trademark of Elsevier B.V.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a aria-label="RELX home page (opens in a new tab)" id=els-footer-relx href=https://www.relx.com/ target=_blank rel=nofollow><img loading=lazy src=data:null;base64, width=93 height=20 alt="RELX group home page"></a></div></footer></div></section></div></div></div>
<div id=UMS_TOOLTIP style=position:absolute;cursor:pointer;z-index:2147483647;background:transparent;top:-100000px;left:-100000px></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><button aria-label=Feedback type=button id=_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E data-layout=badgeBlank class="_pendo-badge _pendo-badge_" style="z-index:19000;margin:0px;line-height:1;font-size:0px;background:rgba(255,255,255,0);padding:0px;height:32px;width:128px;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;border:0px;float:none;vertical-align:baseline;cursor:pointer;position:absolute;top:43527px;left:1423px"><img id=pendo-image-badge-c2b2bcc0 src=data:image/svg+xml;base64,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 data-_pendo-image-1 class="_pendo-image _pendo-badge-image" style="display:block;height:32px;width:128px;padding:0px;margin:0px;line-height:1;border:none;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;float:none;vertical-align:baseline"></button><umsdataelement id=UMSSendDataEventElement data=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.patcog.2014.05.014 docguid=null></umsdataelement><div id=tmtoolbar_manual_rating_injected style=display:none>init</div>