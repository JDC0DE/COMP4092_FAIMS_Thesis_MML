<!DOCTYPE html> <html lang=en style><!--
 Page saved with SingleFile 
 url: https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X 
 saved date: Sun Sep 18 2022 16:58:23 GMT+1000 (Australian Eastern Standard Time)
--><meta charset=utf-8>
<meta name=citation_pii content=S092427161630661X>
<meta name=citation_issn content=0924-2716>
<meta name=citation_volume content=130>
<meta name=citation_lastpage content=293>
<meta name=citation_publisher content=Elsevier>
<meta name=citation_firstpage content=277>
<meta name=citation_fulltext_world_readable content>
<meta name=citation_journal_title content="ISPRS Journal of Photogrammetry and Remote Sensing">
<meta name=citation_type content=JOUR>
<meta name=citation_doi content=10.1016/j.isprsjprs.2017.06.001>
<meta name=dc.identifier content=10.1016/j.isprsjprs.2017.06.001>
<meta name=citation_article_type content="Review article">
<meta property=og:description content="Object-based image classification for land-cover mapping purposes using remote-sensing imagery has attracted significant attention in recent years. Nu…">
<meta property=og:image content=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0924271617X00075-cov150h.gif>
<meta name=citation_title content="A review of supervised object-based land-cover image classification">
<meta property=og:title content="A review of supervised object-based land-cover image classification">
<meta name=citation_publication_date content=2017/08/01>
<meta name=citation_online_date content=2017/06/23>
<meta name=robots content=INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR>
<title>A review of supervised object-based land-cover image classification - ScienceDirect</title>
<link rel=canonical href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X>
<meta property=og:type content=article>
<meta name=viewport content="initial-scale=1">
<meta name=SDTech content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
<meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><style>#MathJax_Message{position:fixed;left:1em;bottom:1.5em;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}#\_pendo-badge\_9BcFvkCLLiElWp6hocDK3ZG6Z4E{top:auto!important;left:auto!important;bottom:0px!important;right:20px!important;position:fixed!important}.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"></head>
<body><div id=MathJax_Message style=display:none></div>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics-elsevier-com.simsrad.net.ocs.mq.edu.au/b/ss/elsevier-sd-prod/1/G.4--NS/1663484293836?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_IP&c1=ae%3A2378&c12=ae%3A21981 />
    </noscript>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-content>Skip to main content</a>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-title>Skip to article</a>
<!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
<div data-iso-key=_0><div class=App id=app data-aa-name=root data-reactroot><div class=page><section><div class=sd-flex-container><div class=sd-flex-content><header id=gh-cnt><div id=gh-main-cnt class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-lg"><a id=gh-branding class=u-flex-center-ver href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/ aria-label="ScienceDirect home page" data-aa-region=header data-aa-name=ScienceDirect><img class=gh-logo src=data:null;base64, alt="Elsevier logo" height=48 width=54><svg xmlns=http://www.w3.org/2000/svg role=img version=1.1 height=30 width=138 viewBox="0 0 138 30" class="gh-wordmark u-margin-s-left" fill=#f36d21 aria-labelledby=gh-wm-science-direct focusable=false aria-hidden=true alt="ScienceDirect Wordmark"><title id=gh-wm-science-direct>ScienceDirect</title><g><path class=a d=M4.23,21a9.79,9.79,0,0,1-4.06-.83l.29-2.08a7.17,7.17,0,0,0,3.72,1.09c2.13,0,3-1.22,3-2.39C7.22,13.85.3,13.43.3,9c0-2.37,1.56-4.29,5.2-4.29a9.12,9.12,0,0,1,3.77.75l-.1,2.08a7.58,7.58,0,0,0-3.67-1c-2.24,0-2.91,1.22-2.91,2.39,0,3,6.92,3.61,6.92,7.8C9.5,19.1,7.58,21,4.23,21Z></path><path class=a d=M20.66,20A6.83,6.83,0,0,1,16.76,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H18.81c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M23.75,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,23.75,6.9ZM22.76,9h2V20.74h-2Z></path><path class=a d=M29.55,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,32.77,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM35.63,13c-.08-2.29-1.09-2.7-3-2.7A3.78,3.78,0,0,0,31,10.7,3.7,3.7,0,0,0,29.76,13Z></path><path class=a d=M49.7,20.74h-2s.1-2.73.08-5.1c0,0,0-1.56,0-2.5-.05-1.79-.21-2.7-2-2.7a4.87,4.87,0,0,0-1.64.31,12.11,12.11,0,0,0-1.95,2.08v7.9h-2v-8.5a19.47,19.47,0,0,0-.1-2.34L39.95,9h1.85l.31,1.74a4.71,4.71,0,0,1,3.82-2.05c2.11,0,3.54.68,3.74,3.09.1,1.17.08,2.34.08,3.51C49.75,17.2,49.7,20.74,49.7,20.74Z></path><path class=a d=M61.5,20A6.83,6.83,0,0,1,57.6,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H59.66c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M64.75,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,68,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM70.84,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,65,13Z></path><path class=a d=M81.21,20.74H75.83V5h5.62c5.54,0,7.46,4.21,7.46,7.8C88.91,16.26,86.93,20.74,81.21,20.74Zm-.1-14H77.88V19.07h3c4,0,5.75-2.31,5.75-6.24C86.59,10.15,85.34,6.7,81.11,6.7Z></path><path class=a d=M92.86,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,92.86,6.9ZM91.87,9h2V20.74h-2Z></path><path class=a d=M104.48,10.83l-1.64.47c0-.18-.08-1-.83-1-1.14,0-2.08,1.9-2.5,2.91v7.49h-2V12.18a18.78,18.78,0,0,0-.1-2.29L97.3,9h1.85l.34,1.87a3.22,3.22,0,0,1,2.68-2.16,2,2,0,0,1,2.26,1.72c0,.18.05.29.05.31Z></path><path class=a d=M107.44,14.6V15c0,2.81,1.38,4.34,3.85,4.34A6.37,6.37,0,0,0,115,18.11l.16,1.82A7.94,7.94,0,0,1,110.67,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM113.53,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,107.65,13Z></path><path class=a d=M126.24,20a6.83,6.83,0,0,1-3.9,1.09c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H124.4c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M134.51,20.45a7.36,7.36,0,0,1-2.7.62c-1.53,0-2.63-.86-2.63-2.94V10.52H127V9h2.13V5.81h2V9h3.09v1.56h-3.09v7c0,1.33.34,1.85,1.25,1.85a5.66,5.66,0,0,0,2-.55Z></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label=links class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/browse/journals-and-books data-aa-region=header data-aa-name="Journals &amp; Books"><span class=anchor-text>Journals &amp; Books</span></a></ul></nav><nav aria-label=utilities class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-move-to-spine gh-help-button gh-help-icon"><div class=popover id=gh-help-icon-popover><div id=popover-trigger-gh-help-icon-popover><button class="button-link gh-nav-help-icon gh-icon-btn button-link-primary" aria-expanded=false aria-label="ScienceDirect Support Center links" type=button><svg focusable=false viewBox="0 0 114 128" aria-hidden=true alt="ScienceDirect help page" width=21.375 height=24 class="icon icon-help gh-icon"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg><span class=button-link-text></span></button></div></div><li class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-primary gh-nav-action gh-icon-btn" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/search data-aa-button=search-in-header-opened-from-article aria-label="Opens ScienceDirect Search"><span class=anchor-text></span><svg focusable=false viewBox="0 0 100 128" aria-hidden=true alt=Search width=18.75 height=24 class="icon icon-search gh-icon"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a></ul></nav></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id=institution-popover><div id=popover-trigger-institution-popover><button id=gh-inst-icon-btn class="gh-icon-btn gh-has-institution u-margin-m-left" aria-expanded=false aria-label="Institutional Access"><svg focusable=false viewBox="0 0 106 128" aria-hidden=true alt="Institutional Access" width=19.875 height=24 class="icon icon-institution gh-inst-icon"><path d="m84 98h1e1v1e1h-82v-1e1h1e1v-46h14v46h1e1v-46h14v46h1e1v-46h14v46zm-72-61.14l41-20.84 41 20.84v5.14h-82v-5.14zm92 15.14v-21.26l-51-25.94-51 25.94v21.26h1e1v36h-1e1v3e1h102v-3e1h-1e1v-36h1e1z"></path></svg></button></div></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="link-button u-margin-m-left link-button-primary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS092427161630661X" id=gh-corpsignin-btn data-aa-region=header data-aa-name="Corporate sign in"><span class=link-button-text>Corporate sign in</span></a><a class="link-button u-margin-m-left link-button-secondary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS092427161630661X&amp;from=globalheader" id=gh-signin-btn data-aa-region=header data-aa-name="Sign in"><span class=link-button-text>Sign in / register</span></a></div><div class="gh-lib-banner u-hide-from-print gh-lb-legacy"><a href=http://www.mq.edu.au/on_campus/library/ target=_blank rel="noopener noreferrer" class=gh-lib-banner-link><img class="u-img-responsive u-max-lib-height" src=data:null;base64, alt="You have institutional access"></a></div><div id=gh-mobile-menu class="mobile-menu u-hide-from-print sf-hidden"></div></div></header><div class=Article id=mathjax-container><div class=sticky-outer-wrapper><div class=sticky-inner-wrapper style=position:relative;z-index:2;transform:translate3d(0px,0px,0px)><div id=screen-reader-main-content></div><div class=accessbar role=region aria-label="Download options and search"><div class=accessbar-label></div><ul aria-label="PDF Options"><li><a class="link-button link-button-primary accessbar-primary-link" role=button aria-expanded=true aria-label="Download single PDF. Opens in a new window." aria-live=polite target=_blank href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X/pdfft?md5=939cee4a86d12ebf88d22003efb3d33b&amp;pid=1-s2.0-S092427161630661X-main.pdf" rel=nofollow><svg focusable=false viewBox="0 0 32 32" height=24 width=24 class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=link-button-text>View&nbsp;<strong>PDF</strong></span></a><li><button class="button button-anchor accessbar-anchor-link" aria-label="Download Full Issue" type=button><span class=button-text>Download Full Issue</span></button></ul><form class=QuickSearch action=/search#submit aria-label=form><input type=search class=query aria-label="Search ScienceDirect" name=qs placeholder="Search ScienceDirect" value><button class="button button-primary" type=submit aria-label="Submit search"><span class=button-text><svg focusable=false viewBox="0 0 100 128" height=20 width=18.75 class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button></form></div></div></div><div class="article-wrapper u-padding-s-top grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang=en><div class=Outline id=toc-outline><h2 class=u-h4>Outline</h2><ol class=u-padding-xs-bottom><li><a href=#ab005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Abstract>Abstract</a><li><a href=#kg005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Keywords>Keywords</a><li><a href=#s0005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction">1. Introduction</a><li><a href=#s0010 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. Methods">2. Methods</a><li><a href=#s0030 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Results and discussion">3. Results and discussion</a><li><a href=#s0085 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Methodological advances: issues and future prospects">4. Methodological advances: issues and future prospects</a><li><a href=#s0110 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Uncertainty in object-based supervised classification">5. Uncertainty in object-based supervised classification</a><li><a href=#s0115 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="6. Conclusions">6. Conclusions</a><li><a href=#ak005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Acknowledgments>Acknowledgments</a><li><a href=#bi005 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=References>References</a></ol><button class="button button-anchor" aria-expanded=false data-aa-button="sd:product:journal:article:type=menu:name=show-full-outline" type=button><span class=button-text>Show full outline</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class=PageDivider></div></div><div class=CitedBy id=toc-cited-by><h2 class=u-h4><a href=#section-cited-by>Cited By (512)</a></h2><div class=PageDivider></div></div><div class=Figures id=toc-figures><h2 class=u-h4>Figures (12)</h2><ol><li><a href=#f0005 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 1. Number of relevant publications per journal" src=data:null;base64, style=max-width:219px;max-height:127px></div></a><li><a href=#f0010 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 2. Distribution of research institutions according to the country reported in the…" src=data:null;base64, style=max-width:219px;max-height:106px></div></a><li><a href=#f0015 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 3. Distribution of image spatial resolution used in the investigated case studies" src=data:null;base64, style=max-width:219px;max-height:123px></div></a><li><a href=#f0020 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 4. Correlation between resolution and scope of study area (R2=0" src=data:null;base64, style=max-width:219px;max-height:160px></div></a><li><a href=#f0025 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 5. Distribution of overall accuracies for different sensor types" src=data:null;base64, style=max-width:219px;max-height:96px></div></a><li><a href=#f0030 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 6. Correlation between spatial resolution and segmentation scale (R2=0" src=data:null;base64, style=max-width:219px;max-height:162px></div></a></ol><button class="button button-anchor" data-aa-button="sd:product:journal:article:type=menu:name=show-figures" type=button><span class=button-text>Show all figures</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class=PageDivider></div></div><div class=Tables id=toc-tables><h2 class=u-h4>Tables (5)</h2><ol class=u-padding-s-bottom><li><a href=#t0005 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Checklist of items used when constructing the meta-analysis database for supervised object-based image land-cover classification. ‘NA’ denotes that no specific methods, type, or data was available for..."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 1</a><li><a href=#t0010 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Mean overall accuracy for different sensor types."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 2</a><li><a href=#t0015 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Representative studies on segmentation scale optimization."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 3</a><li><a href=#t0020 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Supervised classification articles ranked by number of citations normalized by years (as of April 15, 2016)."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 4</a><li><a href=#t0025 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Feature selection methods used in OBIA."><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 5</a></ol><div class=PageDivider></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role=main lang=en><div class=Publication id=publication><div class="publication-brand u-show-from-sm"><a title="Go to ISPRS Journal of Photogrammetry and Remote Sensing on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/isprs-journal-of-photogrammetry-and-remote-sensing><img class=publication-brand-image src=data:null;base64, alt=Elsevier></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id=publication-title><a class=publication-title-link title="Go to ISPRS Journal of Photogrammetry and Remote Sensing on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/isprs-journal-of-photogrammetry-and-remote-sensing>ISPRS Journal of Photogrammetry and Remote Sensing</a></h2><div class=text-xs><a title="Go to table of contents for this volume/issue" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/isprs-journal-of-photogrammetry-and-remote-sensing/vol/130/suppl/C>Volume 130</a>, August 2017, Pages 277-293</div></div><div class="publication-cover u-show-from-sm"><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/isprs-journal-of-photogrammetry-and-remote-sensing/vol/130/suppl/C><img class=publication-cover-image src=data:null;base64, alt="ISPRS Journal of Photogrammetry and Remote Sensing"></a></div></div><h1 id=screen-reader-main-title class="Head u-font-serif u-h2 u-margin-s-ver"><span class=title-text>A review of supervised object-based land-cover image classification</span></h1><div class=Banner id=banner><div class="wrapper truncated"><div class="AuthorGroups text-xs"><div class=author-group id=author-group><span class=sr-only>Author links open overlay panel</span><a class="author size-m workspace-trigger" name=bau005 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X#!><span class=content><span class="text given-name">Lei</span><span class="text surname">Ma</span><span class=author-ref id=baf005><sup>a</sup></span><span class=author-ref id=baf010><sup>b</sup></span><span class=author-ref id=baf015><sup>c</sup></span><svg focusable=false viewBox="0 0 106 128" width=19.875 height=24 class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau010 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X#!><span class=content><span class="text given-name">Manchun</span><span class="text surname">Li</span><span class=author-ref id=baf005><sup>a</sup></span><span class=author-ref id=baf010><sup>b</sup></span><span class=author-ref id=baf015><sup>c</sup></span><svg focusable=false viewBox="0 0 106 128" width=19.875 height=24 class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=bau015 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X#!><span class=content><span class="text given-name">Xiaoxue</span><span class="text surname">Ma</span><span class=author-ref id=baf015><sup>c</sup></span><span class=author-ref id=baf020><sup>d</sup></span></span></a><a class="author size-m workspace-trigger" name=bau020 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X#!><span class=content><span class="text given-name">Liang</span><span class="text surname">Cheng</span><span class=author-ref id=baf005><sup>a</sup></span><span class=author-ref id=baf010><sup>b</sup></span><span class=author-ref id=baf015><sup>c</sup></span></span></a><a class="author size-m" name=bau025 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/author/7103064199/peijun-du><span class=content><span class="text given-name">Peijun</span><span class="text surname">Du</span><span class=author-ref id=baf005><sup>a</sup></span><span class=author-ref id=baf010><sup>b</sup></span><span class=author-ref id=baf015><sup>c</sup></span></span></a><a class="author size-m workspace-trigger" name=bau030 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161630661X#!><span class=content><span class="text given-name">Yongxue</span><span class="text surname">Liu</span><span class=author-ref id=baf005><sup>a</sup></span><span class=author-ref id=baf010><sup>b</sup></span><span class=author-ref id=baf015><sup>c</sup></span></span></a></div></div></div><button id=show-more-btn class="button show-hide-details button-primary" type=button data-aa-button=icon-expand><span class=button-text>Show more</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><button class="button toc-button button-anchor u-hide-from-lg u-margin-s-right sf-hidden" type=button><svg focusable=false viewBox="0 0 104 128" width=19.5 height=24 class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg></button><button class="button-link AddToMendeley button show-on-desktop button-link-primary" type=button><svg focusable=false viewBox="0 0 86 128" height=16 width=16 class="icon icon-plus"><path d="m48 58v-38h-1e1v38h-38v1e1h38v38h1e1v-38h38v-1e1z"></path></svg><span class=button-link-text>Add to Mendeley</span></button><div class="Social u-display-inline-block" id=social><div class="popover social-popover" id=social-popover><div id=popover-trigger-social-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 128 128" height=16 width=16 class="icon icon-share"><path d="m9e1 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm-66-36c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-6e1c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48l-28.42-15.28c0.58-1.98 0.9-4.04 0.9-6.2s-0.32-4.22-0.9-6.2l28.42-15.28c4.04 4.58 9.92 7.48 16.48 7.48 12.14 0 22-9.86 22-22s-9.86-22-22-22-22 9.86-22 22c0 1.98 0.28 3.9 0.78 5.72l-28.64 15.38c-4.02-4.34-9.76-7.1-16.14-7.1-12.14 0-22 9.86-22 22s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-0.5 1.84-0.78 3.76-0.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class=button-text>Share</span></button></div></div></div><div class="ExportCitation u-display-inline-block" id=export-citation><div class="popover export-citation-popover" id=export-citation-popover><div id=popover-trigger-export-citation-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 106 128" height=16 width=16 class="icon icon-cited-by-66"><path xmlns=http://www.w3.org/2000/svg d="m2 58.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78v-1e1c-25.9 0-44 15.12-44 36.78zm1e2 -26.78v-1e1c-25.9 0-44 15.12-44 36.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78z"></path></svg><span class=button-text>Cite</span></button></div></div></div></div></div><div class=ArticleIdentifierLinks id=article-identifier-links><a class=doi href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.isprsjprs.2017.06.001 target=_blank rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.isprsjprs.2017.06.001</a><a class=rights-and-content target=_blank rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S092427161630661X&amp;orderBeanReset=true">Get rights and content</a></div><div class=LicenseInfo><div class=License><span>Under a Creative Commons </span><a target=_blank rel="noreferrer noopener" href=https://creativecommons.org/licenses/by-nc-nd/4.0/>license</a></div><div class=OpenAccessLabel><span class=access-indicator></span>Open access</div></div><section class=ReferencedArticles></section><section class=ReferencedArticles></section><div class=PageDivider></div><div class="Abstracts u-font-serif" id=abstracts><div class="abstract author" id=ab005 lang=en><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id=as005><p id=sp0005><span><span><span><span>Object-based image <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification title="Learn more about classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification</a> for land-cover mapping purposes using remote-sensing imagery has attracted significant attention in recent years. Numerous studies conducted over the past decade have investigated a broad array of sensors, feature selection, classifiers, and other factors of interest. However, these research results have not yet been synthesized to provide coherent guidance on the effect of different supervised object-based land-cover classification processes. In this study, we first construct a database with 28 fields using qualitative and quantitative information extracted from 254 experimental cases described in 173 scientific papers. Second, the results of the meta-analysis are reported, including </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/general-characteristics title="Learn more about general characteristics from ScienceDirect's AI-generated Topic Pages" class=topic-link>general characteristics</a><span> of the studies (e.g., the geographic range of relevant institutes, preferred journals) and the relationships between factors of interest (e.g., spatial resolution and study area or optimal segmentation scale, accuracy and number of targeted classes), especially with respect to the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-accuracy title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification accuracy</a><span> of different sensors, segmentation scale, training set size, supervised classifiers, and land-cover types. Third, useful data on supervised object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-classification title="Learn more about image classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>image classification</a> are determined from the meta-analysis. For example, we find that supervised object-based classification is currently experiencing rapid advances, while development of the fuzzy technique is limited in the object-based framework. Furthermore, spatial resolution correlates with the optimal segmentation scale and study area, and </span></span></span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/random-decision-forest title="Learn more about Random Forest from ScienceDirect's AI-generated Topic Pages" class=topic-link>Random Forest</a> (RF) shows the best performance in object-based classification. The area-based accuracy assessment method can obtain stable classification performance, and indicates a strong correlation between accuracy and training set size, while the accuracy of the point-based method is likely to be unstable due to mixed objects. In addition, the overall accuracy benefits from </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/high-spatial-resolution title="Learn more about higher spatial resolution from ScienceDirect's AI-generated Topic Pages" class=topic-link>higher spatial resolution</a> images (e.g., unmanned aerial vehicle) or agricultural sites where it also correlates with the number of targeted classes. More than 95.6% of studies involve an area less than 300</span>&nbsp;ha, and the spatial resolution of images is predominantly between 0 and 2&nbsp;m. Furthermore, we identify some methods that may advance supervised object-based image classification. For example, deep learning and type-2 fuzzy techniques may further improve classification accuracy. Lastly, scientists are strongly encouraged to report results of uncertainty studies to further explore the effects of varied factors on supervised object-based image classification.</p></div></div></div><ul id=issue-navigation class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271617302654><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class=button-alternative-text><strong>Previous </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span></a><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271616306438><span class=button-alternative-text><strong>Next </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></ul><div class="Keywords u-font-serif"><div id=kg005 class=keywords-section><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div id=k0005 class=keyword><span>OBIA</span></div><div id=k0010 class=keyword><span>GEOBIA</span></div><div id=k0015 class=keyword><span>Meta-analysis</span></div><div id=k0020 class=keyword><span>Supervised object-based classification</span></div><div id=k0025 class=keyword><span>Land-cover mapping</span></div><div id=k0030 class=keyword><span>Review</span></div></div></div><div class="Body u-font-serif" id=body><div><section id=s0005><h2 id=st015 class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><p id=p0005><span>In recent years, with advances in <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/remote-sensing-data title="Learn more about remote sensing data from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing data</a><span> acquisition technologies and the increased demand for <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/remote-sensing-application title="Learn more about remote sensing applications from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing applications</a><span>, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/high-spatial-resolution title="Learn more about high spatial resolution from ScienceDirect's AI-generated Topic Pages" class=topic-link>high spatial resolution</a><span> <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/remote-sensing title="Learn more about remote sensing from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing</a> data is steadily becoming more widespread (</span></span></span></span><a name=bb0020 href=#b0020 class=workspace-trigger>Belward and Skøien, 2015</a><span><span>). This includes satellite (e.g., <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/worldview title="Learn more about WorldView from ScienceDirect's AI-generated Topic Pages" class=topic-link>WorldView</a>, Gaofen, SuperView) and aerial (e.g., </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/unmanned-aerial-vehicle title="Learn more about unmanned aerial vehicle from ScienceDirect's AI-generated Topic Pages" class=topic-link>unmanned aerial vehicle</a><span> (UAV)) remote sensing data. The availability and accessibility of vast amounts of high-resolution remote sensing data have created a challenge for remote sensing image <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification title="Learn more about classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification</a>. As a result, object-based image analysis (OBIA) techniques have emerged to address these issues. The OBIA technique has now replaced the traditional pixel-based method as the new standard method (</span></span><a name=bb0040 href=#b0040 class=workspace-trigger>Blaschke et al., 2014</a>) that will facilitate land-cover classification from high spatial resolution remote sensing imagery. However, it has not yet been quantitatively verified, although consensus appears to have been achieved amongst numerous researchers (<a name=bb0070 href=#b0070 class=workspace-trigger>Cleve et al., 2008</a>, <a name=bb0370 href=#b0370 class=workspace-trigger>Myint et al., 2011</a>, <a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al., 2012a</a>, <a name=bb0515 href=#b0515 class=workspace-trigger>Tehrany et al., 2014</a>).<p id=p0010>Over almost the last twenty years, the remote sensing community has undertaken considerable efforts to promote the use of object-based technology for land-cover mapping (<a name=bb0045 href=#b0045 class=workspace-trigger>Blaschke and Strobl, 2001</a>, <a name=bb0035 href=#b0035 class=workspace-trigger>Blaschke et al., 2004</a>, <a name=bb0540 href=#b0540 class=workspace-trigger>Walker and Blaschke, 2008</a>). The first biennial international conference on OBIA was held in Salzburg, Austria in 2006. It is the most influential international event to date in the OBIA community, and the six conferences have, undoubtedly, considerably promoted the development of OBIA techniques and applications (<a name=bb0220 href=#b0220 class=workspace-trigger>Hay and Castilla, 2008</a>, <a name=bb0420 href=#b0420 class=workspace-trigger>Powers et al., 2012</a>, <a name=bb0015 href=#b0015 class=workspace-trigger>Arvor et al., 2013</a>, <a name=bb0090 href=#b0090 class=workspace-trigger>Costa et al., 2014</a>, <a name=bb0040 href=#b0040 class=workspace-trigger>Blaschke et al., 2014</a>). Thanks to the publication of special issues on OBIA in various journals, e.g., the special issue “Geographic Object-Based Image Analysis (GEOBIA)” for journal “Photogrammetric Engineering &amp; Remote Sensing” (<a name=bb0215 href=#b0215 class=workspace-trigger>Hay and Blaschke, 2010</a>), and the special issue “Advances in Geographic Object-Based Image Analysis (GEOBIA)” for journal “Remote Sensing” (<a href=http://www.mdpi.com/journal/remotesensing/special_issues/geobia#editors target=_blank rel="noreferrer noopener">http://www.mdpi.com/journal/remotesensing/special_issues/geobia#editors</a><span>, 2014), supervised object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-technique title="Learn more about classification techniques from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification techniques</a> have been an integral part of remote sensing research related to land-cover mapping since 2010 (</span><a name=bb0370 href=#b0370 class=workspace-trigger>Myint et al., 2011</a>, <a name=bb0115 href=#b0115 class=workspace-trigger>Dronova et al., 2011</a>, <a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al., 2012a</a>, <a name=bb0430 href=#b0430 class=workspace-trigger>Puissant et al., 2014</a>, <a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>).<p id=p0015>Generally, land-cover mapping is a complicated process with numerous factors influencing the quality of the final product (<a name=bb0240 href=#b0240 class=workspace-trigger>Khatami et al., 2016</a><span><span>). For supervised object-based classification processes, many options must be selected, including image type, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/segmentation-method title="Learn more about segmentation method from ScienceDirect's AI-generated Topic Pages" class=topic-link>segmentation method</a>, accuracy assessment, </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-algorithm title="Learn more about classification algorithm from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification algorithm</a><span>, training sample sets, input features, and target classes. To deal with these uncertainties, many researchers have devised supervised object-based classification methods that are specifically adapted to individual study areas, which are further compared with existing methods and processes, thereby validating their applicability. However, due to variations between study areas, it is difficult to derive generalized research results. Namely, a certain method may exhibit good classification accuracy and be applicable to a certain study area, yet derive inconsistent results in other study areas. For example, it was already proved that the K-Nearest-Neighbors (K-NN) method generally performed better for land-cover mapping than Decision Tree (DT) and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/support-vector-machine title="Learn more about Support Vector Machines from ScienceDirect's AI-generated Topic Pages" class=topic-link>Support Vector Machines</a> (SVM) methods using SPOT 5 images (</span></span><a name=bb0515 href=#b0515 class=workspace-trigger>Tehrany et al., 2014</a><span>), whereas a superior capability for producing higher classification accuracies using SPOT 5 images in agriculture areas with SVM or <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/random-decision-forest title="Learn more about Random Forest from ScienceDirect's AI-generated Topic Pages" class=topic-link>Random Forest</a> (RF) methods was demonstrated by </span><a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al. (2012a)</a>. Therefore, it is important to determine which classification process is the most promising and how various uncertainties affect classification performance. To do this, it is necessary to synthesize the collective knowledge on this topic, as opposed to using individual experience and expertise.<p id=p0020>Past review articles have provided useful descriptive summaries and guidelines for the general object-based image analysis technique (<a name=bb0030 href=#b0030 class=workspace-trigger>Blaschke, 2010</a>, <a name=bb0040 href=#b0040 class=workspace-trigger>Blaschke et al., 2014</a><span><span>), which have focused on the review of more extensive OBIA techniques, including change detection. However, in recent years, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/supervised-classification title="Learn more about supervised classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>supervised classification</a> has shown rapid advances, and thus more and more issues have arisen. Hence, this review presents a summary of the advances in current supervised object-based classification techniques and examines future development prospects. Although the literature on object-based image </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-analysis title="Learn more about analysis classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>analysis classification</a> was already reviewed by </span><a name=bb0110 href=#b0110 class=workspace-trigger>Dronova (2015)</a>, the classification objects of concern only included wetlands. Furthermore, they also reviewed literature on object-based fuzzy rule-based classification, which generated considerable limitations in their research because substantial discrepancies remain between fuzzy rule-based classification and supervised classification.<p id=p0025>Meta-analysis techniques provide a unique chance to integrate results from peer-reviewed studies rather than simply describing the results, and therefore allow us to quantitatively or qualitatively assess the patterns and relationships of an effect (e.g., classification performance) due to uncertain factors (e.g., sensor type, classification algorithm, and other variables of interest) (<a name=bb0065 href=#b0065 class=workspace-trigger>Chirici et al., 2016</a>). In recent years, meta-analysis of remote sensing applications from various perspectives has provided reliable scientific guidance for quantitatively integrating remote sensing research analysis. For example, meta-analysis techniques was employed to examine the rate and magnitude of global urban land expansion (<a name=bb0485 href=#b0485 class=workspace-trigger>Seto et al., 2011</a>); Numerous studies on supervised pixel-based image classification was synthesized to provide coherent guidance on the relative performance of different classification processes, mainly involving classification algorithms and input data (<a name=bb0240 href=#b0240 class=workspace-trigger>Khatami et al., 2016</a><span>); The results of terrestrial <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/aboveground-biomass title="Learn more about aboveground biomass from ScienceDirect's AI-generated Topic Pages" class=topic-link>aboveground biomass</a> estimation from published studies was summarized to discuss the implications of model errors by sensor type, vegetation type, and plot size (</span><a name=bb0610 href=#b0610 class=workspace-trigger>Zolkos et al., 2013</a>).<p id=p0030>The main objective of this work is to conduct a meta-analysis of the results of existing studies, in order to (1) document the development and application of supervised object-based land-cover image classification of various factors including sensors, land cover types, targeted classes, supervised classifiers, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/geographical-region title="Learn more about geographical regions from ScienceDirect's AI-generated Topic Pages" class=topic-link>geographical regions</a>, training sample sizes, segmentation algorithms, accuracy assessment methods, and other uncertain variables; (2) identify and briefly summarize the scientific advances in supervised object-based classification; (3) and provide scientific guidelines for specific readers regarding the use of supervised object-based classification methods for land-cover mapping. Thus, we first describe, in detail, the methods required to perform the meta-analysis, including the data collection process, construction of the database, and the meta-analysis. Then, the results are presented to evaluate the effects with respect to multiple uncertain factors including sensors, land cover types, methodologies, and other variables of interest. Lastly, guidance and important research gaps are presented to improve the supervised classification of object-based land-cover mapping.</p></section><section id=s0010><h2 id=st020 class="u-h3 u-margin-l-top u-margin-xs-bottom">2. Methods</h2><section id=s0015><h3 id=st025 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.1. Data collection</h3><p id=p0035><span>The <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/remote-sensing title="Learn more about remote sensing from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing</a> literature reviewed here includes studies applying the supervised object-based image analysis technique to land-cover classification. The systematic literature search was conducted using Scopus databases, which have comprehensively indexed various major international remote sensing journals. Then, Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) were used for study selection (</span><a name=bb0360 href=#b0360 class=workspace-trigger>Moher et al., 2009</a><span>). First, two major categories of keywords were designed to search the relevant literature: OBIA-related keywords (“Object-based image analysis” OR “Object-oriented image analysis” OR “Geographic object-based image analysis” OR “Object-based classification” OR “OBIA” OR “GEOBIA”) and keywords related to land-cover classification (“Classification” OR “Land-cover” OR “Mapping” OR “Land use” OR “Classifier”). By performing the query based on these keywords and using a date of April 15, 2016, 1275 publications were returned using the automated retrieval from Scopus databases, which was further filtered to 783 publications related to Article, Article press, and Review. Following a quick analysis of information related to the 783 publications such as titles or abstracts, 333 publications pertaining to relevant studies on land-cover classification using OBIA techniques were manually selected, of which 193 publications were <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/supervised-classification title="Learn more about supervised classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>supervised classification</a> related, and 140 were fuzzy classification related. The following rules were further devised to manually screen out literature and case studies pertaining to supervised object-based image classification.</span><dl class=list><dt class=list-label>(1)<dd class=list-description><p id=p0040>Extraction of studies on both fuzzy rule-based classification and those on supervised classification, thereby analyzing the level of concern over time.</p><dt class=list-label>(2)<dd class=list-description><p id=p0045>Elimination of ambiguously stated case studies with only simple descriptions that employ eCognition software for object-based image classification. In these studies, whether the specific classification method is fuzzy rule-based or supervised remains unspecified. Because eCognition software also features the Nearest Neighbor (NN) supervised classifier in addition to the integrated fuzzy rule-based classification method, it is difficult to confirm which method was used.</p><dt class=list-label>(3)<dd class=list-description><p id=p0050>Exclusion of studies concluding that object-based image classification methods offer no advantage over other classification methods (e.g., pixel-based). For example, research results from Montereale <a name=bb0175 href=#b0175 class=workspace-trigger>Gavazzi et al. (2016)</a> demonstrate that OBIA exhibits the worst performance with regards to classification accuracies, despite the fact that comparisons are performed against the best classification accuracy in their research.</p><dt class=list-label>(4)<dd class=list-description><p id=p0055>Elimination of case studies focusing on the extraction of individual classes without covering techniques pertaining to supervised classification, for example, Weed Seedling Detection (<a name=bb0050 href=#b0050 class=workspace-trigger>Borra-Serrano et al., 2015</a>).</p><dt class=list-label>(5)<dd class=list-description><p id=p0060>Exclusion of case studies that only briefly mention OBIA and fail to clearly expound the sampling process and the specific classification methods from which classification results were obtained. Furthermore, studies that directly employ various unsupervised object-based classification methods for land-cover identification were also excluded. These studies were also excluded from the meta-analysis process presented in this study, e.g., <a name=bb0325 href=#b0325 class=workspace-trigger>Ma et al., 2014</a>, <a name=bb0290 href=#b0290 class=workspace-trigger>Langner et al., 2014</a>, <a name=bb0415 href=#b0415 class=workspace-trigger>Pereira Júnior et al., 2014</a>, <a name=bb0060 href=#b0060 class=workspace-trigger>Charoenjit et al., 2015</a>.</p><dt class=list-label>(6)<dd class=list-description><p id=p0065>Removal of related articles aiming to primarily address change detection without clearly indicating the classification performance of specific supervised classification methods (<a name=bb0535 href=#b0535 class=workspace-trigger>Walter, 2004</a>).</p><dt class=list-label>(7)<dd class=list-description><p id=p0070><span>Elimination of reviews related to OBIA, despite the fact that many reviews significantly furthered OBIA development. These reviews are nonetheless ruled out of meta-analysis, owing to their lack of useful <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/data-classification title="Learn more about classification data from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification data</a> for quantitative analysis. This includes the reviews of </span><a name=bb0315 href=#b0315 class=workspace-trigger>Liu et al., 2006</a>, <a name=bb0030 href=#b0030 class=workspace-trigger>Blaschke, 2010</a>, and <a name=bb0040 href=#b0040 class=workspace-trigger>Blaschke et al. (2014)</a>.</p><dt class=list-label>(8)<dd class=list-description><p id=p0075>When screening or identifying accuracy assessment methods with regards to classification case studies, accuracy assessments using pixels as a statistical unit of accuracy were regarded as area-based assessment, which involves pixel quantity statistics, where the area of each pixel remains constant, e.g., <a name=bb0150 href=#b0150 class=workspace-trigger>Fernandes et al. (2014)</a>.</p><dt class=list-label>(9)<dd class=list-description><p id=p0080>Classifiers such as CART (Classification and Regression Trees), C4.5, and DT perform image classification by generating binary decision trees in OBIA, which are therefore uniformly categorized as <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/decision-tree-classifier title="Learn more about DT classifiers from ScienceDirect's AI-generated Topic Pages" class=topic-link>DT classifiers</a> in this study.</p></dl><p><p id=p0085>Based on above rules, and the careful perusal of 333 publications, 173 related publications that elaborate on the process of supervised object-based classification were selected and used to collect data for meta-analysis.</p></section><section id=s0020><h3 id=st030 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.2. Data</h3><p id=p0090><span>For the meta-analysis of supervised object-based land-cover image classification, a database with 28 fields was constructed based on the 173 articles related to supervised object-based classification. In addition to general literature identification fields such as Title and Author, this database also contained OBIA-specific information fields, e.g., <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/segmentation-method title="Learn more about segmentation method from ScienceDirect's AI-generated Topic Pages" class=topic-link>segmentation method</a>, segmentation scale, classification method, thus enabling the identification of </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/general-characteristics title="Learn more about general characteristics from ScienceDirect's AI-generated Topic Pages" class=topic-link>general characteristics</a> of supervised object-based land-cover image classification. Then, qualitative and quantitative information was collected from the literature reviews describing the 254 experiments. Specifically, the resulting matrix, which contained 254 records with 28 fields for the different supervised object-based classification configurations, was used to conduct further meta-analysis and systematic review.</p></section><section id=s0025><h3 id=st035 class="u-h4 u-margin-m-top u-margin-xs-bottom">2.3. Data analysis</h3><p id=p0095>As well as journal and country information, we focused on synthesizing the studies and independently investigating several features of the supervised object-based land-cover image classification process, including: (1) sensors or spatial resolution of images used; (2) segmentation methods or scale; (3) training sample sets; (4) supervised classifiers; and (5) study types or number of classified categories. We extracted and analyzed measures of overall accuracy from the individual case studies. In this study, these measures were calculated into a mean measure and a standard deviation for specific conditions in order to analyze the effect of each individual factor. That is, a specific sensor and a specific supervised classifier were used to evaluate the size of the effect from these various factors. For example, the difference between the mean overall accuracies of different classifiers may be used to assess the effect from classifiers. Coherent guidance on the relative performance of various features of the classification process was also provided to reveal ways of improving classification accuracy by controlling these uncertain factors. For example, the correlation between spatial resolution and study area or segmentation scale was reported, and it was found that the spatial resolution of the image used can determine the characteristics of the study.<p id=p0100>To investigate the effect of spatial resolution, spatial resolution was split into 15 groups at intervals of 2&nbsp;m from 0 to 30&nbsp;m, thereby collecting statistics on the frequency of image use at different spatial resolutions. Furthermore, sensors were sub-categorized based on their spatial resolution, allowing discrepancies in the frequency of use of different sensors to also be observed.<p id=p0105>To assess the effect of training set size on classification accuracy, the assessment methods for classification accuracies reported in the literature were classified into two categories, namely point-based and area-based methods. Thus, the correlation between the size of the training samples and classification accuracies was also assessed.<div><p id=p0110>It is worth noting that, during the analysis of the influence of each uncertainty, some case studies failed to clearly provide information for all fields in <a name=bt0005 href=#t0005 class=workspace-trigger>Table 1</a>. Hence, in alignment with the specific research objectives, only relevant case studies that clearly expound the corresponding uncertainties were taken into consideration during our statistical analyses. Therefore, the number of experimental case studies that were used for statistical analyses was, in fact, less than 254.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0005><span class=captions><span id=cn0065><p id=sp0070><span class=label>Table 1</span>. Checklist of items used when constructing the meta-analysis database for supervised object-based image land-cover classification. ‘NA’ denotes that no specific methods, type, or data was available for this field.</p></span></span><div class=groups><table><thead><tr class="rowsep-1 valign-top"><th scope=col class=align-left>ID<th scope=col class=align-left>Fields<th scope=col class=align-left>Definition<th scope=col class=align-left>Type<th scope=col class=align-left>Categories<tbody><tr class=valign-top><td class=align-left>1<td class=align-left>Title<td class=align-left>Title of the article<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>2<td class=align-left>Authors<td class=align-left>Author<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>3<td class=align-left>Year<td class=align-left>Year of publication<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>4<td class=align-left>Source title<td class=align-left>Journal name<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>5<td class=align-left>Document type<td class=align-left>Publication type<td class=align-left>Classes<td class=align-left>Article; Conference; Book<tr class=valign-top><td class=align-left>6<td class=align-left>Citations<td class=align-left>No. of citations by other articles<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>7<td class=align-left>Study type<td class=align-left>Type of study<td class=align-left>Classes<td class=align-left>Application; Theoretical; Comparison; Review<tr class=valign-top><td class=align-left>8<td class=align-left>Research institute<td class=align-left>Name of research institutes<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>9<td class=align-left>City research institutes<td class=align-left>City where the research institutes are located<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>10<td class=align-left>Study country<td class=align-left>Country where the study area is located<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>11<td class=align-left>Geographic area<td class=align-left>Region (e.g., a province) in country of origin<td class=align-left>Free text<td class=align-left><tr class=valign-top><td class=align-left>12<td class=align-left>Image resolution<td class=align-left>Spatial resolution of imagery<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>13<td class=align-left>Sensor (Data) type<td class=align-left>Sensors<td class=align-left>Classes<td class=align-left>UAV; SPOT-5; IKONOS; GeoEye-1; Airborne; WorldView-2; Landsat; PolSAR; RapidEye; ASTER; Pléiades; Others<tr class=valign-top><td class=align-left>14<td class=align-left>Area<td class=align-left>Area of study area<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>15<td class=align-left>Pre-processing<td class=align-left>Whether band transformation is implemented<td class=align-left>Classes<td class=align-left>Yes; No; NA<tr class=valign-top><td class=align-left>16<td class=align-left>Sampling strategy<td class=align-left>Method for collecting training sample objects<td class=align-left>Classes<td class=align-left>Stratified random sampling; simple random sampling; Others; NA<tr class=valign-top><td class=align-left>17<td class=align-left>Site type<td class=align-left>Type of study area<td class=align-left>Classes<td class=align-left>Urban; Agriculture; Others, NA<tr class=valign-top><td class=align-left>18<td class=align-left>Segmentation method<td class=align-left>Segmentation method<td class=align-left>Classes<td class=align-left>Multi resolution; Others<tr class=valign-top><td class=align-left>19<td class=align-left>Feature selection<td class=align-left>Whether feature selection is performed<td class=align-left>Classes<td class=align-left>Feature Space Optimization (FSO); RF; GINI; Relief-F; Correlation-Based Feature Selection (CFS); Others; NA<tr class=valign-top><td class=align-left>20<td class=align-left>Classification method<td class=align-left>Supervised classification method<td class=align-left>Classes<td class=align-left>SVM; RF; NN; DT; Others; NA<tr class=valign-top><td class=align-left>21<td class=align-left>Accuracy measure<td class=align-left>Accuracy assessment index<td class=align-left>Classes<td class=align-left>OA; Others; NA<tr class=valign-top><td class=align-left>22<td class=align-left>Accuracy value<td class=align-left>Best accuracy value<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>23<td class=align-left>Accuracy assessment level<td class=align-left>Accuracy assessment methods used<td class=align-left>Classes<td class=align-left>Point; Area<tr class=valign-top><td class=align-left>24<td class=align-left>Confidence interval<td class=align-left>Whether confidence level is employed for reliability validation of accuracy assessment<td class=align-left>Classes<td class=align-left>Yes; No; NA<tr class=valign-top><td class=align-left>25<td class=align-left>Samples<td class=align-left>Number of samples<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>26<td class=align-left>Class number<td class=align-left>Number of classification classes<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>27<td class=align-left>Scale<td class=align-left>Segmentation scale<td class=align-left>Numeric<td class=align-left><tr class=valign-top><td class=align-left>28<td class=align-left>Feature calculation<td class=align-left>Software for feature calculation and segmentation<td class=align-left>Classes<td class=align-left>eCognition; ENVI; SPRING; Others; NA</table></div></div></div></section></section><section id=s0030><h2 id=st040 class="u-h3 u-margin-l-top u-margin-xs-bottom">3. Results and discussion</h2><section id=s0035><h3 id=st045 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.1. General characteristics of studies</h3><div><p id=p0115>Following the in-depth perusal of 173 publications on supervised object-based classification, relevant data was obtained using the methods described in Section <a name=bs0010 href=#s0010 class=workspace-trigger>2</a><span>. The main sources of information were articles published in scientific journals. Statistics reveal that 47 journals published the 173 publications on supervised object-based remote sensing imagery classification. A journal that published only one relevant article was eliminated from the statistics, and the majority of articles were found in 20 journals, detailed below, which together contained 146 articles or approximately 84% of the identified journal papers. The remaining 27 journals were distributed across a wide range of journals, from computer applications to <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/urban-planning title="Learn more about urban planning from ScienceDirect's AI-generated Topic Pages" class=topic-link>urban planning</a>, wetlands, wildfires, and forest applications. The top 20 journals for articles related to object-based supervised classification are shown in </span><a name=bf0005 href=#f0005 class=workspace-trigger>Fig. 1</a>. Furthermore, the top 5 journals for object-based supervised classification papers were: Remote sensing (25, 14.5%), Remote Sensing of Environment (22, 12.7%), International Journal of Remote Sensing (20, 11.6%), International Journal of Applied Earth Observation and Geoinformation (18, 10.4%), and ISPRS Journal of Photogrammetry and Remote Sensing (11, 6.4%).<figure class="figure text-xs" id=f0005><span><img src=data:null;base64, height=385 alt aria-describedby=cn0005><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr1_lrg.jpg target=_blank download title="Download high-res image (388KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (388KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr1.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0005><p id=sp0010><span class=label>Fig. 1</span>. Number of relevant publications per journal.</p></span></span></figure></div><p id=p0120>The first object-based supervised classification paper identified from the literature search dates back to 2004 (<a name=bb0285 href=#b0285 class=workspace-trigger>Laliberte et al., 2004</a>). Prior to 2010, object-based supervised classification did not appear to attract much attention, as the number of related publications did not noticeably increase. However, from 2010 onwards, the number of publications on object-based supervised classification began to increase annually until the present. Moreover, the amount is currently accelerating (see Section <a name=bs0055 href=#s0055 class=workspace-trigger>3.5</a>). Of the 173 published papers, approximately 61.6% of studies focus on specific applications whereas the remainder focus on methodological issues.<div><p id=p0125>Research institutions with publications pertaining to supervised object-based classification are mainly located in Europe and North America (<a name=bf0010 href=#f0010 class=workspace-trigger>Fig. 2</a>). All publications come from 34 countries, and the countries with more than 10 publications include, respectively, United States (44), China (19), Canada (15), Germany (13), and Spain (11). In addition, Australia (9), Brazil (6), Netherlands (5), Belgium (4), Japan (4), and Greece (4) are also worthy of note, as their number of publications exceeds 3.<figure class="figure text-xs" id=f0010><span><img src=data:null;base64, height=376 alt aria-describedby=cn0010><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr2_lrg.jpg target=_blank download title="Download high-res image (600KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (600KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr2.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0010><p id=sp0015><span class=label>Fig. 2</span>. Distribution of research institutions according to the country reported in the published paper.</p></span></span></figure></div><div><p id=p0130><span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/high-spatial-resolution title="Learn more about High spatial resolution from ScienceDirect's AI-generated Topic Pages" class=topic-link>High spatial resolution</a> remote-sensing imagery remains undoubtedly the most frequently employed data source amongst those utilized for object-based classification. Furthermore, the number of case studies with a resolution between 0 and 2</span>&nbsp;<span>m exceeds 90, mainly involving WorldView-2 (WV-2), <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/quickbird title="Learn more about QuickBird from ScienceDirect's AI-generated Topic Pages" class=topic-link>QuickBird</a><span><span> (QB), GeoEye-1, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/ikonos title="Learn more about IKONOS from ScienceDirect's AI-generated Topic Pages" class=topic-link>IKONOS</a>, </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/unmanned-aerial-vehicle title="Learn more about UAV from ScienceDirect's AI-generated Topic Pages" class=topic-link>UAV</a>, and Airborne images (</span></span><a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a><span>). In addition, more than 20 case studies employ SPOT images. In terms of remote sensing imagery with medium resolutions, the data sources for case studies mainly come from satellite remote-sensing images such as ASTER and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/landsat title="Learn more about Landsat from ScienceDirect's AI-generated Topic Pages" class=topic-link>Landsat</a> TM/ETM+/OLI. Moreover, 17 cases studies adopt ASTER and Landsat OLI remote-sensing images with a resolution of 15</span>&nbsp;m, and 28 studies employ Landsat TM/ETM+ remote-sensing imagery with a resolution of 30&nbsp;m.<figure class="figure text-xs" id=f0015><span><img src=data:null;base64, height=208 alt aria-describedby=cn0015><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr3_lrg.jpg target=_blank download title="Download high-res image (116KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (116KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr3.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0015><p id=sp0020><span class=label>Fig. 3</span>. Distribution of image spatial resolution used in the investigated case studies.</p></span></span></figure></div><div><p id=p0135>The spatial resolution of images used determines the scope and size of the study area. Generally, there is a positive correlation between the spatial resolution of images and the size of the study area (R<sup>2</sup>&nbsp;=&nbsp;0.22, p&nbsp;&lt;&nbsp;0.001) (<a name=bf0020 href=#f0020 class=workspace-trigger>Fig. 4</a>). The area of the remote-sensing images in most studies is less than 300&nbsp;<span>ha (95.6%). When <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/remote-sensing-image title="Learn more about remote sensing images from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing images</a> with a resolution of 15</span>&nbsp;m are used, the study area drops to between 0&nbsp;ha and 100&nbsp;ha. In the case of Landsat images with a resolution of 30&nbsp;m, the extent of the study area may exceed 100&nbsp;<span>ha. Moreover, the sparse use of imagery such as <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/modis title="Learn more about MODIS from ScienceDirect's AI-generated Topic Pages" class=topic-link>MODIS</a> images may result in a vast study area, e.g., study areas exceeding 1000</span>&nbsp;km<sup>2</sup> (<a name=bb0365 href=#b0365 class=workspace-trigger>Mohler and Goodin, 2012</a>), which are not considered here due to the small number of examples.<figure class="figure text-xs" id=f0020><span><img src=data:null;base64, height=243 alt aria-describedby=cn0020><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr4_lrg.jpg target=_blank download title="Download high-res image (92KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (92KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr4.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0020><p id=sp0025><span class=label>Fig. 4</span>. Correlation between resolution and scope of study area (R<sup>2</sup>&nbsp;=&nbsp;0.22, p&nbsp;&lt;&nbsp;0.001).</p></span></span></figure></div><p id=p0140>Statements regarding training samples are inconsistent among the 254 case studies. Some cases directly use the number of training sample objects, whereas others employ the proportion of training samples covered in all classification objects. By normalizing the descriptions regarding the number of training samples, the reported training samples vary from a minimum of 0.098% to a maximum of 80%. 121 case studies clearly delineate accuracy assessment methods, of which 26 studies adopt the area-based method, whereas 95 employ the point-based method.<p id=p0145><span>In addition, feature selection, which is an important classification step that could reduce <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-complexity title="Learn more about classification complexity from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification complexity</a> by removing redundant feature information (</span><a name=bb0335 href=#b0335 class=workspace-trigger>Ma et al., 2017</a><span>), still fails to attract adequate attention. Amongst all 254 classification case studies, those that explicitly employ the feature selection method only account for 22%. Moreover, 61% of studies fail to adopt the feature selection method or explicitly state whether the feature selection method is employed. The more frequently used feature selection methods consist of Feature Space Optimization (FSO) (8), DT (7), and Jeffries–Matusita (JM) distance (6), also comprising <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/random-decision-forest title="Learn more about RF from ScienceDirect's AI-generated Topic Pages" class=topic-link>RF</a>, CART, Correlation-Based Feature Selection (CFS), and Wrapper. In addition, for a small number of studies, manually identified feature combinations are adopted for subsequent classification. For example, feature combinations are manually identified using only spectrum or spectrum</span>&nbsp;+&nbsp;texture (<a name=bb0245 href=#b0245 class=workspace-trigger>Kim and Yeom, 2014</a>), spectrum&nbsp;+&nbsp;geometry (<a name=bb0355 href=#b0355 class=workspace-trigger>Maxwell et al., 2015</a>), or spectrum&nbsp;+&nbsp;texture&nbsp;+&nbsp;geometry (<a name=bb0125 href=#b0125 class=workspace-trigger>Đurić et al., 2014</a>).</p></section><section id=s0040><h3 id=st050 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.2. Classification performance by sensor type</h3><div><p id=p0150><span>Regarding supervised object-based image classification sensors, sensor types adopted in less than three case studies were ruled out, e.g., Gaofen-1, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/hyperion title="Learn more about Hyperion from ScienceDirect's AI-generated Topic Pages" class=topic-link>Hyperion</a><span>, Airborne Visible/Infrared Imaging Spectrometer (AVIRIS), <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/palsar title="Learn more about PALSAR from ScienceDirect's AI-generated Topic Pages" class=topic-link>PALSAR</a>, National Agriculture Imagery Program (NAIP), and TerraSAR-X, thus, the remaining 12 are predominantly frequently used sensors. </span></span><a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a> shows the mean classification accuracies of 12 different sensors. It is observed that, except for ASTER and Pléiades images, the mean classification accuracy is generally above 80%. The mean classification accuracy of UAV remains the highest at 86.33%, followed by SPOT-5, QuickBird, and IKONOS, for which the mean classification accuracies are 85.72%, 85.50%, and 85.49%, respectively (see <a name=bt0010 href=#t0010 class=workspace-trigger>Table 2</a>). Surprisingly, the mean classification accuracy of WorldView-2 only reaches 83.61%; however, its resolution is higher than SPOT-5 and IKONOS. This is likely to result from the extensive use of WorldView-2 in urban study areas. A total of 35 case studies adopting WorldView-2 imagery cover 24 urban area classification cases with a high proportion of up to 68.57%. Due to the complexity of land-cover types in urban study areas, its classification accuracy is typically lower than study areas with other land-cover types (e.g., agriculture areas) (refer to analysis results in Section <a name=bs0080 href=#s0080 class=workspace-trigger>3.6.2</a>). In addition, only six experimental case studies employ Pléiades imagery, which all focus on urban study areas (<a name=bb0300 href=#b0300 class=workspace-trigger>Li et al., 2015a</a>).<figure class="figure text-xs" id=f0025><span><img src=data:null;base64, height=340 alt aria-describedby=cn0025><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr5_lrg.jpg target=_blank download title="Download high-res image (200KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (200KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr5.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0025><p id=sp0030><span class=label>Fig. 5</span>. Distribution of overall accuracies for different sensor types.</p></span></span></figure><div class="tables frame-topbot rowsep-0 colsep-0" id=t0010><span class=captions><span id=cn0070><p id=sp0075><span class=label>Table 2</span>. Mean overall accuracy for different sensor types.</p></span></span><div class=groups><table><thead><tr class="rowsep-1 valign-top"><th scope=col class=align-left>Sensors<th scope=col class=align-left>Mean overall accuracy (%)<th scope=col class=align-left>Std. (%)<tbody><tr class=valign-top><td class=align-left>UAV<td class=align-left>86.33<td class=align-left>5.25<tr class=valign-top><td class=align-left>SPOT-5<td class=align-left>85.72<td class=align-left>6.43<tr class=valign-top><td class=align-left>QuickBird<td class=align-left>85.50<td class=align-left>7.40<tr class=valign-top><td class=align-left>IKONOS<td class=align-left>85.49<td class=align-left>7.02<tr class=valign-top><td class=align-left>GeoEye-1<td class=align-left>84.63<td class=align-left>6.22<tr class=valign-top><td class=align-left>Airborne<td class=align-left>84.14<td class=align-left>9.08<tr class=valign-top><td class=align-left>WorldView-2<td class=align-left>83.61<td class=align-left>7.72<tr class=valign-top><td class=align-left>Landsat<td class=align-left>83.28<td class=align-left>7.89<tr class=valign-top><td class=align-left>PolSAR<td class=align-left>82.93<td class=align-left>3.51<tr class=valign-top><td class=align-left>RapidEye<td class=align-left>82.42<td class=align-left>5.70<tr class=valign-top><td class=align-left>ASTER<td class=align-left>79.49<td class=align-left>5.45<tr class=valign-top><td class=align-left>Pléiades<td class=align-left>75.50<td class=align-left>5.32</table></div></div></div></section><section id=s0045><h3 id=st055 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.3. Segmentation scale and optimization</h3><div><p id=p0155>In terms of the segmentation software adopted in the 254 case studies, studies on segmentation using eCognition software account for 80.9% and those using ENVI software represent 4.4%. The remaining segmentation software mainly comprises SPRING and ERDAS. To better understand the relationship between segmentation scale and spatial resolution of the remote-sensing imagery, statistics are collected from all studies on multi-resolution segmentation using eCognition software. Furthermore, 92 case studies explicitly delineate the optimized segmentation scale and the image spatial resolution. Analysis of the correlation between scales and resolutions has found that a generally statistically significant correlation (p&nbsp;&lt;&nbsp;0.05) exists between the segmentation scale and the image resolution. In addition, the optimized segmentation scale is typically inversely proportional to the resolution (<a name=bf0030 href=#f0030 class=workspace-trigger>Fig. 6</a><span>). Generally, the higher the spatial resolution, the smaller the configured optimal segmentation scales. Conversely, the lower the spatial resolution, the greater the configured optimal segmentation scales. However, it can be difficult to determine the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/scale-optimization title="Learn more about optimization scale from ScienceDirect's AI-generated Topic Pages" class=topic-link>optimization scale</a> given the fact that the variability of the scale is affected by other image characteristics, for example, the size of the study area.</span><figure class="figure text-xs" id=f0030><span><img src=data:null;base64, height=251 alt aria-describedby=cn0030><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr6_lrg.jpg target=_blank download title="Download high-res image (107KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (107KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr6.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0030><p id=sp0035><span class=label>Fig. 6</span>. Correlation between spatial resolution and segmentation scale (R<sup>2</sup>&nbsp;=&nbsp;0.058, p&nbsp;&lt;&nbsp;0.05).</p></span></span></figure></div><p id=p0160>For multi-resolution segmentation, numerous studies demonstrate the even greater importance of the scale parameter because it controls the dimension and size of segmented objects, which may directly affect subsequent classification (<a name=bb0490 href=#b0490 class=workspace-trigger>Smith, 2010</a>, <a name=bb0255 href=#b0255 class=workspace-trigger>Kim et al., 2011</a>, <a name=bb0370 href=#b0370 class=workspace-trigger>Myint et al., 2011</a>, <a name=bb0225 href=#b0225 class=workspace-trigger>Hussain et al., 2013</a>, <a name=bb0095 href=#b0095 class=workspace-trigger>Drǎguţ et al., 2014</a>). The scale issue, therefore, has emerged as a major problem in OBIA, particularly with respect to OBIA studies conducted with multi-scale segmentation methods. It is therefore essential to determine the appropriate segmentation scale and obtain optimized segmentation results (<a name=bb0010 href=#b0010 class=workspace-trigger>Arbiol et al., 2006</a>). However, in numerous applied studies, land-cover extractions mainly rely on a trial-and-error approach, and segmentation scale parameters are determined based on previous experience (<a name=bb0280 href=#b0280 class=workspace-trigger>Laliberte and Rango, 2009</a>). This approach is highly inadvisable (<a name=bb0230 href=#b0230 class=workspace-trigger>Johnson and Xie, 2011</a>); therefore, many researchers have proposed methods to determine optimal segmentation scale parameters (<a name=bb0595 href=#b0595 class=workspace-trigger>Zhang et al., 2008</a>, <a name=bb0250 href=#b0250 class=workspace-trigger>Kim et al., 2008</a>, <a name=bb0105 href=#b0105 class=workspace-trigger>Drǎguţ et al., 2010</a>, <a name=bb0095 href=#b0095 class=workspace-trigger>Drǎguţ et al., 2014</a>, <a name=bb0230 href=#b0230 class=workspace-trigger>Johnson and Xie, 2011</a>, <a name=bb0350 href=#b0350 class=workspace-trigger>Martha et al., 2011</a>, <a name=bb0100 href=#b0100 class=workspace-trigger>Drǎguţ and Eisank, 2012</a>).<div><p id=p0165>A successful research result on scale optimization is to combine Local Variance (LV) and Rates of Change of LV (ROC-LV) to determine appropriate segmentation scales (<a name=bb0105 href=#b0105 class=workspace-trigger>Drǎguţ et al., 2010</a>), and the corresponding Estimate Scale Parameter (ESP) tool was made public for optimizing scale parameters. However, this method is only capable of processing single-band images. Therefore, they improved this method to enable the simultaneous processing of 30-band images, and developed the improved ESP2 tool, which has been successfully implemented in eCognition software (<a name=bb0095 href=#b0095 class=workspace-trigger>Drǎguţ et al., 2014</a>). <a name=bt0015 href=#t0015 class=workspace-trigger>Table 3</a> summarizes some representative segmentation scale optimization methods, which are mainly classified into two categories: supervised and unsupervised. The pros and cons of the above methods are also presented, which can be employed as required on a selective basis. Generally, supervised methods may obtain optimal segmentation results of different land-covers, but manual interpretation of reference objects for different land-covers is required as prior knowledge, such as the Fitting Equation (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a><span>) and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/euclidean-distance title="Learn more about Euclidean Distance from ScienceDirect's AI-generated Topic Pages" class=topic-link>Euclidean Distance</a> 2 (</span><a name=bb0565 href=#b0565 class=workspace-trigger>Witharana and Civco, 2014</a><span>). <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/unsupervised-method title="Learn more about Unsupervised methods from ScienceDirect's AI-generated Topic Pages" class=topic-link>Unsupervised methods</a> commonly suggest a single optimization scale (i.e., </span><a name=bb0140 href=#b0140 class=workspace-trigger>Espindola et al., 2006</a>), or require a difficult threshold (i.e., <a name=bb0230 href=#b0230 class=workspace-trigger>Johnson and Xie, 2011</a>).<div class="tables frame-topbot rowsep-0 colsep-0" id=t0015><span class=captions><span id=cn0075><p id=sp0080><span class=label>Table 3</span>. Representative studies on segmentation scale optimization.</p></span></span><div class=groups><table><thead><tr class="rowsep-1 valign-top"><td scope=col class=align-left><span class=screen-reader-only>Empty Cell</span><th scope=col class=align-left>Assessment indexes<th scope=col class=align-left>Methods<th scope=col class=align-left>Pros and cons<th scope=col class=align-left>Publication<tbody><tr class=valign-top><td class=align-left rowspan=2>Supervised<td class=align-left>Fitting equation<td class=align-left>Based on mean area of reference objects, substitute into fitting equation to compute optimized scales for different land-covers<td class=align-left>Optimal segmentation results of different land covers may be obtained. Manual interpretation of reference objects for different land covers is required, and the discriminant equation can then be derived<td class=align-left><a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al. (2015)</a><tr class=valign-top><td class=align-left>Euclidean Distance 2<td class=align-left>Optimize segmentation objects using the ED measure for the reference object and the actual object<td class=align-left>Manual interpretation of reference objects for different land-covers is required<td class=align-left><a name=bb0565 href=#b0565 class=workspace-trigger>Witharana and Civco (2014)</a><tr class=valign-top><td class=align-left colspan=5><br><tr class=valign-top><td class=align-left rowspan=3>Unsupervised<td class=align-left>Weighted variance&nbsp;+&nbsp;rate of change (ROC)<td class=align-left>Using the variation trend of two indices to determine the candidate optimization scale<td class=align-left>Plugins (ESP/ESP2) available that are embedded into eCognition software. Only multiple individual candidate optimization scales may be obtained. ROC bears no specific geometric significance<td class=align-left><a name=bb0105 href=#b0105 class=workspace-trigger>Drǎguţ et al., 2010</a>, <a name=bb0095 href=#b0095 class=workspace-trigger>Drǎguţ et al., 2014</a><tr class=valign-top><td class=align-left>Normalization (Weighted variance +Moran’s I), heterogeneity indexes of neighboring objects<td class=align-left>Using normalization heterogeneity indices amongst neighboring objects to determine object merging<td class=align-left>Redefine segmentation object to obtain optimal segmentation results for different land covers. Difficult to select merging/decomposition thresholds. Unstable test results with considerable regional limitations<td class=align-left><a name=bb0230 href=#b0230 class=workspace-trigger>Johnson and Xie (2011)</a><tr class=valign-top><td class=align-left>Weighted variance +Moran’s I<td class=align-left>Using the variation trend of the two indices to determine optimization scales<td class=align-left>Indices have geometric significance, and only a single optimization scale may be obtained<td class=align-left><a name=bb0140 href=#b0140 class=workspace-trigger>Espindola et al. (2006)</a></table></div></div></div></section><section id=s0050><h3 id=st060 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.4. The effect of training set size on accuracy</h3><div><p id=p0170><a name=bf0035 href=#f0035 class=workspace-trigger>Fig. 7</a> presents the statistical analysis of research results that clearly state the number of training samples. The descriptions of training samples in different case studies are inconsistent. For example, the description of some studies use the number of sample objects, whereas other studies employ the proportion of samples. For easy comparison and analyses, training sample proportion is uniformly used here to delineate the size of training samples in supervised classification, namely the proportion of training sample objects and classification objects. In general, with an increase in the size of training samples, the classification accuracy increases accordingly (<a name=bf0035 href=#f0035 class=workspace-trigger>Fig.&nbsp;7</a>(a)). Namely, a positive correlation exists between classification accuracy and size of the training sample, which has been consistently shown in many studies (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>). However, the strength of this positive correlation is very weak. Moreover, with an increase of training samples, classification accuracies exhibit a very limited increase (<a name=bf0035 href=#f0035 class=workspace-trigger>Fig.&nbsp;7</a><span>(a)–(c)), which is illogical. In particular, with respect to the point-based method, the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/correlation-coefficient title="Learn more about correlation coefficient from ScienceDirect's AI-generated Topic Pages" class=topic-link>correlation coefficient</a> (R</span><sup>2</sup>) for classification accuracy and size of the training samples is only 0.0042. This is likely to result from the considerable uncertainty of the point-based method.<figure class="figure text-xs" id=f0035><span><img src=data:null;base64, height=549 alt aria-describedby=cn0035><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr7_lrg.jpg target=_blank download title="Download high-res image (465KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (465KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr7.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0035><p id=sp0040><span class=label>Fig. 7</span>. <span>Correlation of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-accuracy title="Learn more about classification accuracy from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification accuracy</a> and training samples for: (a) all 82 studies that explicitly state the size of training samples; (b) 33 of the 82 case studies that fail to explicitly delineate accuracy assessment methods; (c) 40 of the remaining 49 studies that clearly state the accuracy assessment method and adopt the point-based accuracy assessment method; and (d) the other 9 studies, which employ the area-based accuracy assessment method (R</span><sup>2</sup>&nbsp;=&nbsp;0.618, p&nbsp;&lt;&nbsp;0.05).</p></span></span></figure></div><p id=p0175>For the case where segmentation objects comprise only homogeneous objects, this method exhibits advantages, and is characterized by fast processing speed and high accuracy; however, when segmentation objects consist of many mixed objects, then the classification accuracy may exhibit a marked fluctuation. This mainly stems from the considerable difficulty in identifying whether mixed objects are correctly classified. Consequently, in the case of the point-based method, classification accuracies exhibit a very limited increase with an increase of training samples. However, in terms of the area-based accuracy assessment method, a strong and significant positive correlation (R<sup>2</sup>&nbsp;=&nbsp;0.62, p&nbsp;&lt;&nbsp;0.05) may be observed between the classification accuracy and the size of the training sample (<a name=bf0035 href=#f0035 class=workspace-trigger>Fig.&nbsp;7</a>(d)). This is because the area-based accuracy assessment method is assumed to be more applicable to OBIA, which is capable of clearly defining correctly and incorrectly classified portions among mixed segmentation objects (<a name=bb0555 href=#b0555 class=workspace-trigger>Whiteside et al., 2014</a>). The statistical analysis presented here proves, from another perspective, the applicability of the area-based method to OBIA accuracy assessment. Unfortunately, many studies still fail to adopt this method for accuracy assessment. Thus, it is suggested that the area-based method is employed more frequently in future OBIA accuracy assessment.</p></section><section id=s0055><h3 id=st065 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.5. Supervised classification methods</h3><section id=s0060><h4 id=st070 class="u-margin-m-top u-margin-xs-bottom">3.5.1. Supervised methods and fuzzy methods</h4><div><p id=p0180>OBIA has been well verified for the classification of high or medium resolution images. In this supervised meta-analysis, we analyzed the distribution of 193 papers published since 2004 for different years. Moreover, our statistics reveal that, since 2003, there have been 140 publications dealing with fuzzy rule-based object-based image classification. <a name=bf0040 href=#f0040 class=workspace-trigger>Fig. 8</a><span> shows the number of studies published each year on fuzzy rule-based classification and supervised classification since 2003. Prior to 2010, the fuzzy rule-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-technique title="Learn more about classification technique from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification technique</a> appears more popular than the supervised classification technique, because of its ability to overcome the uncertainty of classes of segments and the strong transferability of rule sets based on the fuzzy concepts (</span><a name=bb0540 href=#b0540 class=workspace-trigger>Walker and Blaschke, 2008</a>, <a name=bb0195 href=#b0195 class=workspace-trigger>Hofmann et al., 2011</a>, <a name=bb0190 href=#b0190 class=workspace-trigger>Hofmann, 2016</a><span>). However, since 2008, the amount of published literature has increased slightly and the object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-classification-techniques title="Learn more about image classification technique from ScienceDirect's AI-generated Topic Pages" class=topic-link>image classification technique</a> has received more attention, primarily as a result of the first international conference on OBIA held in 2008 (</span><a name=bb0220 href=#b0220 class=workspace-trigger>Hay and Castilla, 2008</a>). Since 2010, the number of studies on supervised object-based classification has increased rapidly, although studies on the fuzzy rule-based classification technique were still increasing during this time. However, in 2015, studies on fuzzy rule-based classification began to decrease. In contrast, research on supervised classification exhibited a more rapid increase (<a name=bf0040 href=#f0040 class=workspace-trigger>Fig. 8</a>), perhaps because the fuzzy technique had become well known and well documented in the field of OBIA. Conversely, with the in-depth application of supervised classifiers in OBIA, a number of issues regarding supervised classifiers in OBIA began to attract attention (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>), for example, sample selection (<a name=bb0470 href=#b0470 class=workspace-trigger>Rougier et al., 2016</a>), feature selection (<a name=bb0275 href=#b0275 class=workspace-trigger>Laliberte et al., 2012</a>), the supervised classification technique (<a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>), accuracy assessment (<a name=bb0555 href=#b0555 class=workspace-trigger>Whiteside et al., 2014</a>), and so forth. Thus, it is considered that supervised object-based remote-sensing image classification techniques may be a key and dominant trend in future OBIA research.<figure class="figure text-xs" id=f0040><span><img src=data:null;base64, height=263 alt aria-describedby=cn0040><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr8_lrg.jpg target=_blank download title="Download high-res image (135KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (135KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr8.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0040><p id=sp0045><span class=label>Fig. 8</span>. Number of publications per year for object-based land-cover <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-classification title="Learn more about image classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>image classification</a> using either fuzzy rule-based or supervised methods.</p></span></span></figure></div></section><section id=s0065><h4 id=st075 class="u-margin-m-top u-margin-xs-bottom">3.5.2. Classification performance by supervised classifiers</h4><div><p id=p0185><span><span>Regarding the relationship between supervised classifiers and overall classification accuracies, we collected statistics for all 220 related studies, of which 64 studies adopt the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/neighbor-classifier title="Learn more about NN classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>NN classifier</a>, 55 the </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/support-vector-machine title="Learn more about SVM from ScienceDirect's AI-generated Topic Pages" class=topic-link>SVM</a><span><span> classifier, 44 the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/random-forest-classifier title="Learn more about RF classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>RF classifier</a>, 33 the </span><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/decision-tree-classifier title="Learn more about DT classifier from ScienceDirect's AI-generated Topic Pages" class=topic-link>DT classifier</a>, 14 the Maximum Likelihood Classifier (MLC) classifier, and 10 employ other classifiers. In terms of commonly used supervised classifiers, RF is generally characterized by the highest mean classification accuracy (85.81%), followed by the SVM classifier (85.19%) and the DT classifier (84.15%), whereas those of NN and MLC are relatively lower at 81.58% and 81.55%, respectively (</span></span><a name=bf0045 href=#f0045 class=workspace-trigger>Fig. 9</a>). Despite the fact that training samples or remote-sensing images may vary slightly, the accuracy assessment results of different commonly used classifiers are approximately consistent with most previous research conclusions (<a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a><span>). Moreover, compared to other factors, classifiers constitute a very important influential factor for supervised classification. The mean classification accuracies of other classifiers are generally higher, which is likely a result of improved classification methods adopted in these studies, including novel methods proposed by researchers, and excellent classification techniques such as the Adaboost and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/artificial-neural-network title="Learn more about Artificial Neural Network from ScienceDirect's AI-generated Topic Pages" class=topic-link>Artificial Neural Network</a> (ANN) (</span><a name=bb0590 href=#b0590 class=workspace-trigger>Zhang, 2015</a>). In addition, some more generic classifiers such as Naive Bayes are also included into the “Others” category; therefore, classification accuracies with other classifiers exhibit considerable fluctuations.<figure class="figure text-xs" id=f0045><span><img src=data:null;base64, height=297 alt aria-describedby=cn0045><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr9_lrg.jpg target=_blank download title="Download high-res image (108KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (108KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr9.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0045><p id=sp0050><span class=label>Fig. 9</span>. Overall accuracy vs. several supervised classifiers.</p></span></span></figure></div></section></section><section id=s0070><h3 id=st080 class="u-h4 u-margin-m-top u-margin-xs-bottom">3.6. Effect of classified types on classification performance</h3><p id=p0190>This section mainly analyzes the relationship between classification accuracies and the number of classes plus major land-cover types of study areas. Research results show that a correlation exists between classification accuracies and the number of classes. Furthermore, the classification accuracy will decrease with an increase in the number of classes. In addition, classification accuracy varies substantially with land-cover type.<section id=s0075><h4 id=st085 class="u-margin-m-top u-margin-xs-bottom">3.6.1. Effect of the number of classes on classification accuracy</h4><div><p id=p0195>Previous studies have described linear relationships between overall accuracy and targeted classes, and overall accuracy and plot size (<a name=bb0110 href=#b0110 class=workspace-trigger>Dronova, 2015</a>). However, the statistical results are derived only from studies of OBIA in wetland mapping. In this study, a full review is conducted on OBIA research results for all types of remote-sensing mapping, resulting in 225 case studies that satisfy the conditions, which far exceeds the 61 case studies of <a name=bb0110 href=#b0110 class=workspace-trigger>Dronova (2015)</a>. As for analyzing the effects of the number of classes on accuracy, 5 records on classification accuracy values are deleted where the number of classes are equal to or greater than 25, leaving 220 records, the statistics of which are shown in the scatter graph in <a name=bf0050 href=#f0050 class=workspace-trigger>Fig. 10</a>. <a name=bf0050 href=#f0050 class=workspace-trigger>Fig. 10</a> shows the relationship between the number of different classes and classification accuracies, with the number of classes mainly ranging from 2 to 11. In general, our results are consistent with the research conclusion of <a name=bb0110 href=#b0110 class=workspace-trigger>Dronova (2015)</a>; that is, there is a significant and weak negative correlation between the number of classes and classification accuracy, with p&nbsp;&lt;&nbsp;0.05 (<a name=bf0050 href=#f0050 class=workspace-trigger>Fig. 10</a>). Furthermore, the correlation test of 144 samples also indicates that there is no evident correlation between classification accuracies and the scope of study areas, as the p-value is greater than 0.1.<figure class="figure text-xs" id=f0050><span><img src=data:null;base64, height=255 alt aria-describedby=cn0050><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr10_lrg.jpg target=_blank download title="Download high-res image (139KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (139KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr10.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0050><p id=sp0055><span class=label>Fig. 10</span>. <span>Distribution of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-accuracy title="Learn more about classification accuracies from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification accuracies</a> with respect to the number of classes in the reviewed papers (R</span><sup>2</sup>&nbsp;=&nbsp;0.0259, p&nbsp;&lt;&nbsp;0.05).</p></span></span></figure></div></section><section id=s0080><h4 id=st090 class="u-margin-m-top u-margin-xs-bottom">3.6.2. Effect of land-cover type on classification accuracy</h4><div><p id=p0200>The 225 case studies comprise 5 major land-cover types, including agriculture (50 classification accuracies), forest (53 classification accuracies), urban (64 classification accuracies), vegetation (28 classification accuracies), and wetlands (9 classification accuracies). Other land-cover types in this study mainly consist of landslide, coral reef, flood, benthic habitat/seabed mapping, coal mining areas, and aquatic, totaling 20 classification accuracies. Results indicate that the mean overall classification accuracy of agriculture study areas is highest, at 86.04%, followed by that of wetland study areas at 84.46%. Urban, vegetation, and forest study areas are characterized by similar mean classification accuracies of 83.15%, 83.25%, and 82.45%, respectively (<a name=bf0055 href=#f0055 class=workspace-trigger>Fig. 11</a><span>). Study areas with a higher level of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/homogeneity title="Learn more about homogeneity from ScienceDirect's AI-generated Topic Pages" class=topic-link>homogeneity</a>, such as vegetation and forest types, exhibit lower classification accuracies. This is mainly due to the fact that vegetation and forest types were adopted as experimental areas in most earlier studies where NN classifiers are used extensively (</span><a name=bb0080 href=#b0080 class=workspace-trigger>Colditz et al., 2006</a>, <a name=bb0580 href=#b0580 class=workspace-trigger>Yu et al., 2006</a>, <a name=bb0170 href=#b0170 class=workspace-trigger>Gao, 2008</a>, <a name=bb0345 href=#b0345 class=workspace-trigger>Mallinis et al., 2008</a>), resulting in generally low classification accuracy. In recent years, however, more theoretical and methodological research has begun to involve the study of agriculture areas (<a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al., 2012a</a>, <a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>; <a name=bb0380 href=#b0380 class=workspace-trigger>O Connell et al., 2015</a>). Furthermore, excellent machine learning classifiers such as RF and SVM are now applied to the process of object-based image classification (<a name=bb0410 href=#b0410 class=workspace-trigger>Peña et al., 2014</a>, <a name=bb0445 href=#b0445 class=workspace-trigger>Qian et al., 2014</a>, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>), thereby enabling higher classification accuracies of agriculture and wetland study areas. However, the complex characteristics of urban land-cover, in contrast to areas such as highly homogeneous agricultural areas, pose intrinsic difficulties for classification (<a name=bb0370 href=#b0370 class=workspace-trigger>Myint et al., 2011</a>). Therefore, despite a legion of relevant studies (<a name=bb0570 href=#b0570 class=workspace-trigger>Xu et al., 2010</a>, <a name=bb0005 href=#b0005 class=workspace-trigger>Aguilar et al., 2013</a>, <a name=bb0120 href=#b0120 class=workspace-trigger>Du et al., 2015</a>) and the frequent use of advanced classification methods, the classification accuracy for urban study areas remains inferior to that of agriculture study areas.<figure class="figure text-xs" id=f0055><span><img src=data:null;base64, height=297 alt aria-describedby=cn0055><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr11_lrg.jpg target=_blank download title="Download high-res image (113KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (113KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr11.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0055><p id=sp0060><span class=label>Fig. 11</span>. Overall accuracy vs. study area land-cover type.</p></span></span></figure></div></section></section></section><section id=s0085><h2 id=st095 class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Methodological advances: issues and future prospects</h2><p id=p0205>Meta-analyses and reviews are useful for documenting the scope, range, geographic distribution, and history of supervised object-based image classification, as well as the interaction effect of the multiple factors on classification accuracy, i.e., spatial resolution, sensors, scale, classes, and methods. In the following sections, an in-depth exploration is carried out with respect to the results of the meta-analyses, a detailed review is conducted of the methods related to key steps in the process of supervised object-based land-cover image classification, and predictions are made regarding the future advances of supervised object-based land-cover image classification.<section id=s0090><h3 id=st100 class="u-h4 u-margin-m-top u-margin-xs-bottom">4.1. Development of classification methods</h3><div><p id=p0210>The results of meta-analyses indicate that supervised classifiers substantially affect the performance of object-based land-cover image classification (see Section <a name=bs0065 href=#s0065 class=workspace-trigger>3.5.2</a>). The most frequently cited papers were summarized to find which classifier is given more attention (<a name=bt0020 href=#t0020 class=workspace-trigger>Table 4</a>), and to extrapolate the significant research results from existing studies on supervised object-based classification. Results show that the most popular classifiers are still the traditional NN and DT classifiers. This mainly results from the extensive use of eCognition software in most supervised object-based classifications, where NN is the only supervised classifier. Moreover, owing to the unique advantage for rule construction featured by eCognition, DT is preferred by many researchers. In addition, since 2011, RF and SVM classifiers have also attracted great attention owing to their excellent classification performance (<a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al., 2012a</a>, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>).<div class="tables frame-topbot rowsep-0 colsep-0" id=t0020><span class=captions><span id=cn0080><p id=sp0085><span class=label>Table 4</span>. Supervised classification articles ranked by number of citations normalized by years (as of April 15, 2016).</p></span></span><div class=groups><table><thead><tr class="rowsep-1 valign-top"><th scope=col class=align-left>Rank<th scope=col class=align-left>Average number of citations per year<th scope=col class=align-left>Classifiers used<th scope=col class=align-left>Publication<tbody><tr class=valign-top><td class=align-left>1<td class=align-left>41<td class=align-left>NN<td class=align-left><a name=bb0370 href=#b0370 class=workspace-trigger>Myint et al. (2011)</a><tr class=valign-top><td class=align-left>2<td class=align-left>27<td class=align-left>NN<td class=align-left><a name=bb0580 href=#b0580 class=workspace-trigger>Yu et al. (2006)</a><tr class=valign-top><td class=align-left>3<td class=align-left>22<td class=align-left>RF, SVM, DT<td class=align-left><a name=bb0130 href=#b0130 class=workspace-trigger>Duro et al. (2012a)</a><tr class=valign-top><td class=align-left>4<td class=align-left>19<td class=align-left>NN<td class=align-left><a name=bb0285 href=#b0285 class=workspace-trigger>Laliberte et al. (2004)</a><tr class=valign-top><td class=align-left>5<td class=align-left>18<td class=align-left>DT<td class=align-left><a name=bb0405 href=#b0405 class=workspace-trigger>Peña-Barragán et al. (2011)</a><tr class=valign-top><td class=align-left>6<td class=align-left>17<td class=align-left>RF<td class=align-left><a name=bb0505 href=#b0505 class=workspace-trigger>Stumpf and Kerle (2011)</a><tr class=valign-top><td class=align-left>7<td class=align-left>15<td class=align-left>DT, NN<td class=align-left><a name=bb0345 href=#b0345 class=workspace-trigger>Mallinis et al. (2008)</a><tr class=valign-top><td class=align-left>8<td class=align-left>14<td class=align-left>NN<td class=align-left><a name=bb0545 href=#b0545 class=workspace-trigger>Wang et al. (2004)</a><tr class=valign-top><td class=align-left>9<td class=align-left>14<td class=align-left>DT<td class=align-left><a name=bb0235 href=#b0235 class=workspace-trigger>Ke et al. (2010)</a><tr class=valign-top><td class=align-left>10<td class=align-left>13<td class=align-left>DT<td class=align-left><a name=bb0435 href=#b0435 class=workspace-trigger>Qi et al. (2012)</a><tr class=valign-top><td class=align-left>11<td class=align-left>12<td class=align-left>NN<td class=align-left><a name=bb0575 href=#b0575 class=workspace-trigger>Yan et al. (2006)</a><tr class=valign-top><td class=align-left>12<td class=align-left>11<td class=align-left>DT<td class=align-left><a name=bb0280 href=#b0280 class=workspace-trigger>Laliberte and Rango (2009)</a><tr class=valign-top><td class=align-left>13<td class=align-left>11<td class=align-left>LDA, DT<td class=align-left><a name=bb0425 href=#b0425 class=workspace-trigger>Pu and Landry (2012)</a><tr class=valign-top><td class=align-left>14<td class=align-left>10<td class=align-left>NN<td class=align-left><a name=bb0070 href=#b0070 class=workspace-trigger>Cleve et al. (2008)</a></table></div></div></div><p id=p0215>The application of supervised classifiers in OBIA has attracted widespread attention in recent years. Nevertheless, owing to the effects of various factors such as segmentation scale, that affect which supervised classifier is more applicable to supervised object-based classification, the conclusions from many studies are inconsistent (<a name=bb0355 href=#b0355 class=workspace-trigger>Maxwell et al., 2015</a>, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>). For example, some studies concluded that the overall classification accuracies of DT were better than those of the K-NN algorithm (<a name=bb0270 href=#b0270 class=workspace-trigger>Laliberte et al., 2006</a>, <a name=bb0345 href=#b0345 class=workspace-trigger>Mallinis et al., 2008</a>), while the others suggested that K-NN generally performed better for land-cover mapping (<a name=bb0515 href=#b0515 class=workspace-trigger>Tehrany et al., 2014</a>). Until recently, by combining a series of uncertainties in OBIA such as segmentation scales, features, and mixed objects, <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al. (2016)</a><span> systematically analyzed the performance of various commonly-used supervised classifiers under different conditions, and deemed that, as a whole, RF was the supervised classifier most suitable to OBIA. Moreover, they admitted that the advantages of various classifiers differ under different conditions, for example, the DT and <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/random-forest-classifier title="Learn more about RF classifiers from ScienceDirect's AI-generated Topic Pages" class=topic-link>RF classifiers</a> perform better at processing redundant features, while other classifiers benefit more from feature selection (</span><a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>).<p id=p0220>Deep learning is an excellent classification technique developed in recent years, and therefore has been paid more attention in object-based classification framework (<a name=bb0260 href=#b0260 class=workspace-trigger>Längkvist et al., 2016</a>, <a name=bb0605 href=#b0605 class=workspace-trigger>Zhao et al., 2017</a>). It is believed that, in future studies, it will be necessary to introduce it into supervised object-based classification and conduct an in-depth examination on the interactive effects between deep learning and varied factors in OBIA. Thus, it is expected to further promote the development of supervised object-based classification techniques. In addition, type-2 fuzzy techniques may offer another opportunity for object-based image classification, because the development of fuzzy rule-based classification already encounters a bottleneck in the field of object-based image classification (<a name=bf0040 href=#f0040 class=workspace-trigger>Fig. 8</a>). Compared to type-1 fuzzy techniques, type-2 fuzzy techniques, which reveal the depth of uncertainty, are a much softer classifier (<a name=bb0155 href=#b0155 class=workspace-trigger>Fisher, 2010</a>), and are thus expected to overcome all the effects of the variations inherent in supervised object-based image classification (<a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>).</p></section><section id=s0095><h3 id=st105 class="u-h4 u-margin-m-top u-margin-xs-bottom">4.2. Development of sampling methods</h3><p id=p0225><span>Based on the statistics of 254 case studies, only 88 clearly described the sampling methods, of which 45 studies adopted the traditional simple random sampling method, 37 employed the <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/stratified-random title="Learn more about stratified random from ScienceDirect's AI-generated Topic Pages" class=topic-link>stratified random</a> sampling method, and the other 6 employed other sampling methods, including some new methods. In terms of object-based image classification, the sampling method undoubtedly constitutes a crucial step. Furthermore, the varied sizes of segmentation objects pose sampling difficulties and specificity in the process of object-based classification (</span><a name=bb0085 href=#b0085 class=workspace-trigger>Corcoran et al., 2015</a>). However, relevant theoretical research on the sampling process of object-based classification is still insufficient. For example, is remains unknown how the discrepancies will be reflected among training sample objects with different sizes, how the training samples of homogeneous objects better represent corresponding mixed objects with identical classes, and how the mixed objects express corresponding homogeneous objects with identical classes. Subsequently, little attention has been devoted to optimizing sampled training objects (<a name=bb0115 href=#b0115 class=workspace-trigger>Dronova et al., 2011</a>, <a name=bb0390 href=#b0390 class=workspace-trigger>Pérez-Ortiz et al., 2016</a>).<p id=p0230>Generally, it is considered that mixed objects offer the opportunity to devise novel sampling schemes for supervised object-based image classification. Because mixed objects in object-based classification are the same as mixed pixels in pixel-based classification, numerous sampling schemes have already been devised with respect to this process. The effects of a sampling scheme combining mixed pixels on classification results are assessed (<a name=bb0475 href=#b0475 class=workspace-trigger>Samat et al., 2015</a>), whereas for supervised object-based image classification, more studies still focus on assessing the effects of training sample size and commonly used sampling methods (<a name=bb0600 href=#b0600 class=workspace-trigger>Zhen et al., 2013</a>, <a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>), thus falling short of designs and studies that exploit advanced sampling schemes.<p id=p0235>Therefore, for supervised object-based image classification, we suggest that future studies take advantage of relatively advanced techniques within the existing sampling hierarchy, such as active learning (<a name=bb0520 href=#b0520 class=workspace-trigger>Tuia et al., 2011</a>, <a name=bb0510 href=#b0510 class=workspace-trigger>Stumpf et al., 2014</a>, <a name=bb0470 href=#b0470 class=workspace-trigger>Rougier et al., 2016</a>) and semi-supervised learning (<a name=bb0025 href=#b0025 class=workspace-trigger>Bioucas-Dias et al., 2013</a>), by combining characteristics of mixed objects, and conduct studies on the optimization of training sample objects. Thus, this would address the problem of reduced classification accuracies due to the small number of coarse-scale sample objects during the sampling process, as well as the discrepancies between mixed and homogeneous objects that were labeled as identical classes. Moreover, the sample training bias would also be addressed, which results from the different sizes of segmentation objects for different land covers within the same scene.</p></section><section id=s0100><h3 id=st110 class="u-h4 u-margin-m-top u-margin-xs-bottom">4.3. Study of feature selection methods</h3><div><p id=p0240>With the in-depth application of supervised classification in the field of OBIA, feature selection is also gradually attracting attention. <a name=bt0025 href=#t0025 class=workspace-trigger>Table 5</a> summarizes the applied studies into feature selection methods in supervised object-based classification. Based on different feature selection results, feature selection methods mainly comprise the feature importance assessment method, the feature subset assessment method, and other selection methods (<a name=bb0335 href=#b0335 class=workspace-trigger>Ma et al., 2017</a>). The feature importance assessment method may obtain a sorted ranking of importance (e.g., GINI (<a name=bb0280 href=#b0280 class=workspace-trigger>Laliberte and Rango, 2009</a>, <a name=bb0055 href=#b0055 class=workspace-trigger>Cánovas-García and Alonso-Sarría, 2015</a>), RF (<a name=bb0375 href=#b0375 class=workspace-trigger>Novack et al., 2011</a>), Chi-square (<a name=bb0405 href=#b0405 class=workspace-trigger>Peña-Barragán et al., 2011</a>), and SVM-RFE (<a name=bb0505 href=#b0505 class=workspace-trigger>Stumpf and Kerle, 2011</a>, <a name=bb0480 href=#b0480 class=workspace-trigger>Schultz et al., 2015</a>)), whereas the feature subset method can directly attain optimized feature subsets (e.g., Wrapper (<a name=bb0135 href=#b0135 class=workspace-trigger>Duro et al., 2012b</a>, <a name=bb0305 href=#b0305 class=workspace-trigger>Li et al., 2015b</a>) and CFS (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>)), thereby achieving the highest classification accuracy.<div class="tables frame-topbot rowsep-0 colsep-0" id=t0025><span class=captions><span id=cn0085><p id=sp0090><span class=label>Table 5</span>. Feature selection methods used in OBIA.</p></span></span><div class=groups><table><thead><tr class="rowsep-1 valign-top"><td scope=col class=align-left><span class=screen-reader-only>Empty Cell</span><th scope=col class=align-left>Methods<th scope=col class=align-left>Pros and cons<th scope=col class=align-left>Applications and representative literature<tbody><tr class=valign-top><td class=align-left rowspan=6>Feature importance assessment methods<td class=align-left>RF<td class=align-left>Easily adapt to RF classifier, ensuring classification accuracy of the feature selection. Able to generate sorted features<td class=align-left>Classification of urban land use (<a name=bb0375 href=#b0375 class=workspace-trigger>Novack et al., 2011</a>). Classification of land use in agricultural areas in combination with RF classifier (<a name=bb0380 href=#b0380 class=workspace-trigger>O Connell et al., 2015</a>). Forest habitat type classification (<a name=bb0450 href=#b0450 class=workspace-trigger>Räsänen et al., 2013</a>)<tr class=valign-top><td class=align-left>GINI<td class=align-left>Higher feature selection efficiency, able to generate sorted features and classification rules. Easily combined with decision tree classifier<td class=align-left>Classification of pasture land-cover types, combining DT classifier (<a name=bb0280 href=#b0280 class=workspace-trigger>Laliberte and Rango, 2009</a>). GINI index is considered to perform best based on the comparison of multiple feature selection methods (<a name=bb0055 href=#b0055 class=workspace-trigger>Cánovas-García and Alonso-Sarría, 2015</a>)<tr class=valign-top><td class=align-left>SVM/RF-RFE<td class=align-left>Generate feature sorting results while achieving best classification accuracy, normally combined with SVM and RF<td class=align-left>Landslide detection (<a name=bb0505 href=#b0505 class=workspace-trigger>Stumpf and Kerle, 2011</a>) and classification of agricultural land-use types (<a name=bb0480 href=#b0480 class=workspace-trigger>Schultz et al., 2015</a>), combined with RF classifier<tr class=valign-top><td class=align-left>Relief-F<td class=align-left>Able to generate sorted features; independent of classification model, difficult to determine optimal subsets<td class=align-left>Classification of urban land-use, in combination with RF, SVM, and DT classifiers (<a name=bb0375 href=#b0375 class=workspace-trigger>Novack et al., 2011</a>)<tr class=valign-top><td class=align-left>Chi-square<td class=align-left>Able to generate sorted features and classification rules<td class=align-left><a name=bb0405 href=#b0405 class=workspace-trigger>Peña-Barragán et al. (2011)</a> applied Chi-square test index to split the decision tree and achieved agricultural area classification and mapping<tr class=valign-top><td class=align-left>Information Gain (IG)<td class=align-left>Able to generate sorted features and classification rules, difficult to determine optimal subsets<td class=align-left><a name=bb0530 href=#b0530 class=workspace-trigger>Vieira et al. (2012)</a> applied IG index to split the decision tree, and achieved sugarcane land-cover classification and mapping; <a name=bb0390 href=#b0390 class=workspace-trigger>Pérez-Ortiz et al. (2016)</a> applied IG index to screen out top 10 features for weed mapping<tr class=valign-top><td class=align-left colspan=4><br><tr class=valign-top><td class=align-left rowspan=2>Feature subset assessment methods<td class=align-left>CFS<td class=align-left>Directly generate feature subsets, independent of classification model, featuring fast processing speed<td class=align-left>Classification of agricultural land-uses in combination with RF classifier (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>)<tr class=valign-top><td class=align-left>Wrapper (RF/SVM)<td class=align-left>Easy adaptation to classifiers. Point-based cross-validation is adopted in most cases, susceptible to over-fitting or over-adapting to classifiers. Time consuming. Normally applied in combination with RF and SVM<td class=align-left>Classification of agricultural landscape in combination with RF classifier (<a name=bb0135 href=#b0135 class=workspace-trigger>Duro et al., 2012b</a>); Landslide classification and detection using RF and SVM (<a name=bb0305 href=#b0305 class=workspace-trigger>Li et al., 2015b</a>)<tr class=valign-top><td class=align-left colspan=4><br><tr class=valign-top><td class=align-left rowspan=3>Miscellaneous<td class=align-left>JM distance<td class=align-left>Able to generate classification distances amongst classes. Generally used for rule-based classification or NN classifier<td class=align-left>Rule-based classification of pasture land cover (<a name=bb0255 href=#b0255 class=workspace-trigger>Kim et al., 2011</a>); JM index is considered to perform best based on the comparison of multiple feature selection methods (<a name=bb0275 href=#b0275 class=workspace-trigger>Laliberte et al., 2012</a>)<tr class=valign-top><td class=align-left>FSO<td class=align-left>Integrated in eCognition with easy use. Black-box manipulation, lack of feature importance sorting. Normally used for NN classifier<td class=align-left>Mangrove forest classification using NN classifier (<a name=bb0495 href=#b0495 class=workspace-trigger>Son et al., 2015</a>); Comparison of multiple feature selection methods (<a name=bb0275 href=#b0275 class=workspace-trigger>Laliberte et al., 2012</a>); <a name=bb0145 href=#b0145 class=workspace-trigger>Evans et al. (2014)</a> employ it to define optimal feature set for wetland mapping with NN classifier<tr class=valign-top><td class=align-left>GA<td class=align-left>Black-box manipulation, lack of feature sorting results. Normally used for ANN classifier<td class=align-left>Combining neural network for forest mapping (<a name=bb0075 href=#b0075 class=workspace-trigger>Van Coillie et al., 2007</a>)</table></div></div></div><p id=p0245><span>Because feature selection may reduce <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-complexity title="Learn more about classification complexity from ScienceDirect's AI-generated Topic Pages" class=topic-link>classification complexity</a> or improve classification accuracies, the above studies generally show that feature selection can improve the process of object-based remote sensing image classification. However, not all studies guarantee that feature selection improves classification accuracy, mainly due to uncertainties introduced in the process of object-based imagery classification, e.g., changes of segmentation scales, and the diversity of classifiers. Additionally, studies of other high-dimensional data (e.g., high spectral data) showed that feature selection was characterized by considerable uncertainties in terms of different supervised classification methods (</span><a name=bb0395 href=#b0395 class=workspace-trigger>Pal and Foody, 2010</a>, <a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>). With respect to the SVM classifier, some studies suggest that SVM is insensitive to the number of data dimensions (<a name=bb0200 href=#b0200 class=workspace-trigger>Melgani and Bruzzone, 2004</a>, <a name=bb0400 href=#b0400 class=workspace-trigger>Pal and Mather, 2006</a>), namely the increase or decrease in the number of data dimensions may not affect the classification accuracies of SVM. Conversely, SVM classification accuracy improved was found with the decreased number of dimensions by <a name=bb0550 href=#b0550 class=workspace-trigger>Weston et al. (2000)</a>. Therefore, in the process of SVM-based classification, some uncertainties still exist in feature selection. The same issues exist with the RF classifier, as it has already been applied extensively to object-based remote sensing image classification (<a name=bb0505 href=#b0505 class=workspace-trigger>Stumpf and Kerle, 2011</a>, <a name=bb0430 href=#b0430 class=workspace-trigger>Puissant et al., 2014</a>). For example, some studies found that feature selection could improve the performance of the RF classifier for agricultural area mapping (<a name=bb0135 href=#b0135 class=workspace-trigger>Duro et al., 2012b</a>), while the others proved that the RF classifier is a more stable object-based remote sensing image classification method with or without feature selection (<a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>). Hence, in the process of object-based remote sensing imagery classification, due to the diversity of input data or a series of variations introduced during the process of object-based classification (e.g., different classifiers), feature selection requires substantial further research.<p id=p0250>Some earlier studies simply stated that the number of employed feature dimensions should be limited to avoid overfitting and a potential decrease in classification accuracy (<a name=bb0295 href=#b0295 class=workspace-trigger>Leon and Woodroffe, 2011</a>). However, This study in particular shows that, in terms of RF or DT classifiers, an increase in features may not lead to a decrease in object-based classification accuracies (<a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>). Although the number of features required for different machine learning classifiers to achieve higher and stable accuracy varies slightly (<a name=bb0560 href=#b0560 class=workspace-trigger>Wieland and Pittore, 2014</a>), previous studies have demonstrated that a higher classification accuracy may be achieved when the number of features is less than 30 (<a name=bb0205 href=#b0205 class=workspace-trigger>Guan et al., 2013</a>, <a name=bb0180 href=#b0180 class=workspace-trigger>Ghosh and Joshi, 2014</a>) and, generally, it is not advisable to use too many features in OBIA. This is probably because most features in OBIA belong to secondary derivative features with comparatively high correlation. Therefore, calculating a limited number of features in OBIA not only avoids large computational burdens, but also ensures classification accuracies. However, due to the fact that the performance of different machine learning classifiers varies with different feature selection methods, further exploration should be made into which feature selection method should be used in specific practical applications of OBIA.</p></section><section id=s0105><h3 id=st115 class="u-h4 u-margin-m-top u-margin-xs-bottom">4.4. Labeling and accuracy assessment</h3><p id=p0255>This study also involves a discussion of the labeling of candidate samples during supervised object-based image classification because it is related to whether the correct training samples are obtained. Unfortunately, very few studies have explored this aspect to date. Moreover, the labeling process is not clearly stated in numerous studies on supervised classification, which, we suggest, is essential. Because it was already explicitly demonstrated that accuracy performance benefits more from homogeneous objects (<a name=bb0310 href=#b0310 class=workspace-trigger>Li et al., 2016</a>), which signifies when the area-based method is used in accuracy assessment, the more homogeneous objects that are selected as training samples or test samples, the higher the classification accuracies. In current studies, the segmentation object will, in most cases, be labeled directly using the class that accounts for a large percentage of the segmentation object (<a name=bb0525 href=#b0525 class=workspace-trigger>Verbeeck et al., 2012</a>, <a name=bb0330 href=#b0330 class=workspace-trigger>Ma et al., 2015</a>).<p id=p0260>Labels not only influence the sampling process, but are also closely related to the process of accuracy assessment, thereby affecting the assessment of classification accuracy performance. This mainly results from the following two reasons: (1) the actual interpreted image layer objects cannot fully coincide with the segmentation objects and (2) segmentation objects consist of mixed land-cover types. In object-based high-resolution remote-sensing image classification, because the point-based accuracy assessment method (viewing an individual segmentation object as an independent point) remains simple and straightforward, researchers are more inclined to view the object as an individual point, thus the classification of this object is either correct or incorrect (see Section <a name=bs0035 href=#s0035 class=workspace-trigger>3.1</a>). However, its classification accuracy increased with increasing scale (<a name=bb0280 href=#b0280 class=workspace-trigger>Laliberte and Rango, 2009</a>), which is not reasonable over large scales, because an increase in the number of mixed objects may reduce classification accuracy. With advances in OBIA technology, OBIA accuracy assessment methods are emerging as a new research field (<a name=bb0030 href=#b0030 class=workspace-trigger>Blaschke, 2010</a>, <a name=bb0460 href=#b0460 class=workspace-trigger>Radoux et al., 2011</a>, <a name=bb0555 href=#b0555 class=workspace-trigger>Whiteside et al., 2014</a>, <a name=bb0455 href=#b0455 class=workspace-trigger>Radoux and Bogaert, 2014</a>) as point-based assessment methods gradually exhibit drawbacks (<a name=bb0585 href=#b0585 class=workspace-trigger>Zhan et al., 2005</a>, <a name=bb0465 href=#b0465 class=workspace-trigger>Recio et al., 2013</a>, <a name=bb0185 href=#b0185 class=workspace-trigger>Goodin et al., 2015</a>).<p id=p0265>The area-based validation method essentially assesses classification accuracy based on the scope and spatial distribution of segmentation objects (<a name=bb0165 href=#b0165 class=workspace-trigger>Freire et al., 2014</a>). The main issue is that the unit of accuracy assessment is no longer a regular pixel unit and each object is a geographical object of differing sizes (<a name=bb0340 href=#b0340 class=workspace-trigger>MacLean and Congalton, 2012</a>). Therefore, researchers had to determine: (1) which unit (point or polygon) should be used to evaluate the classification accuracy (<a name=bb0500 href=#b0500 class=workspace-trigger>Stehman and Wickham, 2011</a>); and (2) which rule should be used to label the class of object according to the reference layer (<a name=bb0455 href=#b0455 class=workspace-trigger>Radoux and Bogaert, 2014</a>). Generally, it is considered that accuracy assessment benefits more from a polygon unit. Therefore, a theoretical model based on the area-based accuracy assessment method is proposed by <a name=bb0555 href=#b0555 class=workspace-trigger>Whiteside et al. (2014)</a>, which is, so far, a set of theoretical systems with practical significance for accuracy assessment in OBIA. Subsequently, it is deemed that accuracy assessment studies of object-based classification should first focus on the object labeling issue, because it will not only optimize training sample objects, but also promote the development of accuracy assessment methods.</p></section></section><section id=s0110><h2 id=st120 class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Uncertainty in object-based supervised classification</h2><div><p id=p0270>Based on the meta-analysis and review, we summarize the sources of major uncertainties on supervised object-based classification (<a name=bf0060 href=#f0060 class=workspace-trigger>Fig. 12</a>). Uncertainties in object-based classification mainly stem from the diversity of techniques and methods used in various processing phases, which have already been reviewed in previous sections of this paper (Sections <a name=bs0090 href=#s0090 class=workspace-trigger>4.1 Development of classification methods</a>, <a name=bs0095 href=#s0095 class=workspace-trigger>4.2 Development of sampling methods</a>, <a name=bs0100 href=#s0100 class=workspace-trigger>4.3 Study of feature selection methods</a>, <a name=bs0105 href=#s0105 class=workspace-trigger>4.4 Labeling and accuracy assessment</a>). In-depth studies have also been carried out on, for example, the uncertainty of the classification result obtained from a single classifier by <a name=bb0210 href=#b0210 class=workspace-trigger>Hao et al. (2015)</a> and <a name=bb0310 href=#b0310 class=workspace-trigger>Li et al. (2016)</a>. Therefore, method uncertainties will not be described here; instead, we focus on the effects of uncertainties for parameters involved in various methods and different phases, e.g., spatial scale, segmentation parameters, and data sources.<figure class="figure text-xs" id=f0060><span><img src=data:null;base64, height=281 alt aria-describedby=cn0060><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr12_lrg.jpg target=_blank download title="Download high-res image (265KB)"><span class=anchor-text>Download : <span class=download-link-title>Download high-res image (265KB)</span></span></a><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S092427161630661X-gr12.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span id=cn0060><p id=sp0065><span class=label>Fig. 12</span>. Factors introducing uncertainty into the process of supervised object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-classification title="Learn more about image classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>image classification</a>.</p></span></span></figure></div><p id=p0275>Scale uncertainties may be classified into the following two categories: firstly, different spatial resolutions leading to uncertainty of classification; secondly, different segmentation scale parameters leading to differing segmentation objects. In terms of different spatial resolutions, <a name=bb0420 href=#b0420 class=workspace-trigger>Powers et al. (2012)</a><span> used a re-sampling approach to compare the performance of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/classification-analysis title="Learn more about OBIA classification from ScienceDirect's AI-generated Topic Pages" class=topic-link>OBIA classification</a> for images with different spatial resolutions (5, 10, 15, 20, 25, and 30</span>&nbsp;m) and found that better classification performance can be obtained using a spatial resolution of 10&nbsp;m. However, based on our statistical analyses of published case studies on supervised classification, studies on OBIA classification are generally more inclined to employ images with high spatial resolutions ranging from 0 to 2&nbsp;m (<a name=bf0015 href=#f0015 class=workspace-trigger>Fig. 3</a>). In addition, by means of statistical analyses of classification accuracies for different sensors (<a name=bf0025 href=#f0025 class=workspace-trigger>Fig. 5</a><span>), no correlation is observed between better classification accuracies and spatial resolutions in OBIA. Therefore, it remains unknown why this technique applies to images with high spatial resolution, and not to images with low spatial resolution. Hence, related <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/quantitative-assessment title="Learn more about quantitative assessments from ScienceDirect's AI-generated Topic Pages" class=topic-link>quantitative assessments</a><span> should be conducted. Furthermore, many studies have been performed on the uncertainties of segmentation scale, and related studies on <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/scale-optimization title="Learn more about scale optimization from ScienceDirect's AI-generated Topic Pages" class=topic-link>scale optimization</a> are also detailed in Section </span></span><a name=bs0045 href=#s0045 class=workspace-trigger>3.3</a>; therefore, they will not be repeated here.<p id=p0280>From the perspective of data sources, the effects of various factors related to acquired imagery are encompassed, including sensors (Section <a name=bs0040 href=#s0040 class=workspace-trigger>3.2</a>), types of land-cover (Section <a name=bs0080 href=#s0080 class=workspace-trigger>3.6.2</a><span>), and temporal information. Concerning to the uncertainty derived from varied temporal information, the classification accuracies in different seasons or periods exhibit considerable discrepancies because of the changes in <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/earth-and-planetary-sciences/phenology title="Learn more about phenology from ScienceDirect's AI-generated Topic Pages" class=topic-link>phenology</a> (</span><a name=bb0440 href=#b0440 class=workspace-trigger>Qi et al., 2015</a>), and therefore multi-temporal datasets enable to decrease classification uncertainty compared to single data sets (<a name=bb0265 href=#b0265 class=workspace-trigger>Löw et al., 2015</a>).<p id=p0285>Most of the above assessment and analyses reflect the uncertainties caused by various factors related to overall classification accuracy. Overall accuracy is highly recommended for assessing uncertainties, because the ultimate goal of OBIA is to obtain better classification accuracies. Furthermore, the concept of uncertainty has been extensively applied to remote-sensing data processing and quality assessment (<a name=bb0160 href=#b0160 class=workspace-trigger>Foody and Atkinson, 2002</a>, <a name=bb0320 href=#b0320 class=workspace-trigger>Loosvelt et al., 2012</a>, <a name=bb0385 href=#b0385 class=workspace-trigger>Olofsson et al., 2013</a>, <a name=bb0265 href=#b0265 class=workspace-trigger>Löw et al., 2015</a><span>) because processes such as the acquisition, processing, analysis, and conversion of <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/remote-sensing-data title="Learn more about remote sensing data from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing data</a> may all pose certain degrees and types of uncertainties. Consequently, because OBIA remains a newly emerging remote-sensing image processing paradigm (</span><a name=bb0045 href=#b0045 class=workspace-trigger>Blaschke and Strobl, 2001</a>, <a name=bb0040 href=#b0040 class=workspace-trigger>Blaschke et al., 2014</a><span>), we highly recommend further study on the uncertainties of object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/remote-sensing-image title="Learn more about remote sensing image from ScienceDirect's AI-generated Topic Pages" class=topic-link>remote sensing image</a> classification.</span></p></section><section id=s0115><h2 id=st125 class="u-h3 u-margin-l-top u-margin-xs-bottom">6. Conclusions</h2><p id=p0290>The study was motivated by the popularity of supervised object-based image classification for land-cover mapping. Using information available in 173 scientific publications, a database containing typical fields related to supervised object-based land-cover classification was constructed to serve as a basis for meta-analysis. Subsequently, several conclusions were drawn.<dl class=list><dt class=list-label>(1)<dd class=list-description><p id=p0295>High spatial resolution remote-sensing imagery remains the most frequently used data source for supervised object-based land-cover image classification, and the dominant image resolutions are 0–2&nbsp;<span>m. Moreover, due to good availability and accessibility, <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/landsat-series title="Learn more about Landsat series from ScienceDirect's AI-generated Topic Pages" class=topic-link>Landsat series</a> remote-sensing images are often employed in supervised object-based classification.</span></p><dt class=list-label>(2)<dd class=list-description><p id=p0300>In the studies conducted, the size of most study areas is less than 300&nbsp;<span>ha (95.6%), which suggests the need for larger study areas in future research, thereby verifying the applicability of the object-based <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/image-classification-techniques title="Learn more about image classification technique from ScienceDirect's AI-generated Topic Pages" class=topic-link>image classification technique</a> over wider areas. Also, a positive correlation exists between the size of study areas and the spatial resolutions used.</span></p><dt class=list-label>(3)<dd class=list-description><p id=p0305>With respect to segmentation algorithms, more studies are inclined to adopt the multi-scale <a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/topics/computer-science/segmentation-technique title="Learn more about segmentation technique from ScienceDirect's AI-generated Topic Pages" class=topic-link>segmentation technique</a>. A negative correlation exists between the optimal segmentation scale and the spatial resolution of imagery.</p><dt class=list-label>(4)<dd class=list-description><p id=p0310>Unsurprisingly, high spatial resolution remote-sensing imagery, such as UAV, is advantageous for obtaining higher overall classification accuracy. However, there are exceptions. For example, Pléiades images are mainly applied to urban areas, which result in anomalously low classification accuracy. Hence, it is necessary to employ more extensive remote-sensing imagery or land-cover types to further verify object-based image classification technique.</p><dt class=list-label>(5)<dd class=list-description><p id=p0315>A strong positive correlation exists between classification accuracies attained using area-based accuracy assessment methods and the size of training samples, whereas those obtained using the point-based method are extremely unstable.</p><dt class=list-label>(6)<dd class=list-description><p id=p0320>The fuzzy rule-based classification technique encounters a bottleneck in object-based classification, whereas supervised object-based classification is experiencing a peak in development. To better address related issues, we strongly encourage scientists using the supervised object-based image classification technique to report results obtained for all the multiple configurations considered during the optimization phase.</p><dt class=list-label>(7)<dd class=list-description><p id=p0325>RF exhibits the best performance in object-based classification, and has attracted significant attention in recent years, followed by SVM, with MLC performing the worst. Furthermore, NN appears unsuitable for more extensive use in object-based classification. Thus, the use of NN should be reduced, even though it was the most frequently employed classifier for object-based classification.</p><dt class=list-label>(8)<dd class=list-description><p id=p0330>A negative correlation exists between overall classification accuracy and the number of classes defined.</p><dt class=list-label>(9)<dd class=list-description><p id=p0335>Regarding the land-cover type of study areas, object-based classification proves more advantageous for land-cover mapping of agriculture study areas. Moreover, it is essential to explore object-based classification methods that are applicable to urban areas.</p></dl><p><p id=p0340>Although RF and SVM classifiers have also attracted great attention owing to their excellent classification performance, deep learning, which is an excellent classification technique developed in recent years, is still expected to further promote the development of supervised object-based classification techniques. In addition, it seems that rule-based classification already encounters a bottleneck in the field of object-based image classification, and thus type-2 fuzzy techniques are expected to offer another opportunity for object-based image classification.<p id=p0345>Mixed objects may offer the opportunity to devise novel sampling schemes for supervised object-based image classification, and therefore future studies should take advantage of relatively advanced techniques within the existing sampling hierarchy, such as active learning and semi-supervised learning, by combining characteristics of mixed objects, and conduct studies on the optimization of training sample objects.<p id=p0350>Not all studies can ensure feature selection to improve classification accuracy because of uncertainties introduced in the process of object-based imagery classification (e.g., different classifiers). Hence, feature selection requires substantial further research. Furthermore, it is not advisable to use too many features in OBIA and the number of features should be limited to 30, because most features in OBIA belong to secondary derivative features with comparatively high correlation.<p id=p0355>Successfully labeled objects or well-designed labeling rules will not only optimize training sample objects, but also promote the development of accuracy assessment methods. Therefore, we also suggest that accuracy assessment studies of object-based classification should first focus on the object labeling issue.<p id=p0360>As mentioned above, the sources of the major uncertainties in supervised object-based classification stem from the diversity of techniques and methods and the variability of parameters (spatial scale, segmentation parameters, and data sources). Due to the lack of such knowledge, we highly recommend further study on the types and effects of uncertainties related to supervised object-based remote sensing image classification.</p></section></div><section id=ak005><h2 id=st130 class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><p id=p0365>This work is supported by Project funded by <span id=gp005>China Postdoctoral Science Foundation</span> (No. <a href=#gp005>2016M600392</a>), the <span id=gp010>Program B for Outstanding PhD Candidate of Nanjing University</span> (No. <a href=#gp010>201502B008</a>), the <span id=gp015>Special Research Fund of the Ministry of Land and Resources for NonProfit Sector</span> (No. <a href=#gp015>201411014-03</a>), the <span id=gp020>Fundamental Research Funds for the Central Universities</span> (No. <a href=#gp020>020914380031</a>). Sincere thanks are given for the comments and contributions of anonymous reviewers and members of the editorial team.</p></section></div><div class="related-content-links u-hide-from-md sf-hidden"></div><div class=Tail></div><section class="bibliography u-font-serif text-s" id=bi005><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">References</h2><section class=bibliography-sec id=bs005><dl class=references id=reference-links-bs005><dt class=label><a href=#bb0005 id=ref-id-b0005 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Aguilar et al., 2013</a><dd class=reference id=h0005><div class=contribution>M.A. Aguilar, M.M. Saldaña, F.J. Aguilar<div id=ref-id-h0005><strong class=title>GeoEye-1 and WorldView-2 pan-sharpened imagery for object-based classification in urban environments</strong></div></div><div class=host>Int. J. Remote Sens., 34 (2013), pp. 2583-2606</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2012.747018></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2012.747018 aria-describedby=ref-id-h0005>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84872347823&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0005>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=GeoEye-1%20and%20WorldView-2%20pan-sharpened%20imagery%20for%20object-based%20classification%20in%20urban%20environments&amp;publication_year=2013&amp;author=M.A.%20Aguilar&amp;author=M.M.%20Salda%C3%B1a&amp;author=F.J.%20Aguilar" aria-describedby=ref-id-h0005>Google Scholar</a></div><dt class=label><a href=#bb0010 id=ref-id-b0010 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Arbiol et al., 2006</a><dd class=reference id=h0010><span>Arbiol, R., Zhang, Y., Palà, V., 2006. Advanced classification techniques: a review. In: ISPRS Commission VII Mid-term Symposium Remote Sensing: From Pixels to Processes, Enschede, pp. 292–296.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Arbiol,%20R.,%20Zhang,%20Y.,%20Pal%C3%A0,%20V.,%202006.%20Advanced%20classification%20techniques:%20a%20review.%20In:%20ISPRS%20Commission%20VII%20Mid-term%20Symposium%20Remote%20Sensing:%20From%20Pixels%20to%20Processes,%20Enschede,%20pp.%20292296.">Google Scholar</a></div><dt class=label><a href=#bb0015 id=ref-id-b0015 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Arvor et al., 2013</a><dd class=reference id=h0015><div class=contribution>D. Arvor, L. Durieux, S. Andrés, M.A. Laporte<div id=ref-id-h0015><strong class=title>Advances in Geographic Object-Based Image Analysis with ontologies: a review of main contributions and limitations from a remote sensing perspective</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 82 (2013), pp. 125-137</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161300124X aria-describedby=ref-id-h0015>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161300124X/pdfft?md5=912159784596142990a64f4db27c8814&amp;pid=1-s2.0-S092427161300124X-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84879590467&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0015>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Advances%20in%20Geographic%20Object-Based%20Image%20Analysis%20with%20ontologies%3A%20a%20review%20of%20main%20contributions%20and%20limitations%20from%20a%20remote%20sensing%20perspective&amp;publication_year=2013&amp;author=D.%20Arvor&amp;author=L.%20Durieux&amp;author=S.%20Andr%C3%A9s&amp;author=M.A.%20Laporte" aria-describedby=ref-id-h0015>Google Scholar</a></div><dt class=label><a href=#bb0020 id=ref-id-b0020 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Belward and Skøien, 2015</a><dd class=reference id=h0020><div class=contribution>A.S. Belward, J.O. Skøien<div id=ref-id-h0020><strong class=title>Who launched what, when and why; trends in global land-cover observation capacity from civilian earth observation satellites</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 103 (2015), pp. 115-128</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271614000720 aria-describedby=ref-id-h0020>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271614000720/pdfft?md5=940a6f24c30815a8b267dc67f8932a3c&amp;pid=1-s2.0-S0924271614000720-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84927804350&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0020>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Who%20launched%20what%2C%20when%20and%20why%3B%20trends%20in%20global%20land-cover%20observation%20capacity%20from%20civilian%20earth%20observation%20satellites&amp;publication_year=2015&amp;author=A.S.%20Belward&amp;author=J.O.%20Sk%C3%B8ien" aria-describedby=ref-id-h0020>Google Scholar</a></div><dt class=label><a href=#bb0025 id=ref-id-b0025 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Bioucas-Dias et al., 2013</a><dd class=reference id=h0025><div class=contribution>J.M. Bioucas-Dias, A. Plaza, G. Camps-Valls, P. Scheunders<div id=ref-id-h0025><strong class=title>Hyperspectral remote sensing data analysis and future challenges</strong></div></div><div class=host>IEEE Geosci. Remote Sens. Mag., 1 (2013), pp. 6-36</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-84888349041></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84888349041&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0025>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Hyperspectral%20remote%20sensing%20data%20analysis%20and%20future%20challenges&amp;publication_year=2013&amp;author=J.M.%20Bioucas-Dias&amp;author=A.%20Plaza&amp;author=G.%20Camps-Valls&amp;author=P.%20Scheunders" aria-describedby=ref-id-h0025>Google Scholar</a></div><dt class=label><a href=#bb0030 id=ref-id-b0030 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Blaschke, 2010</a><dd class=reference id=h0030><div class=contribution>T. Blaschke<div id=ref-id-h0030><strong class=title>Object based image analysis for remote sensing</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 65 (2010), pp. 2-16</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271609000884 aria-describedby=ref-id-h0030>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271609000884/pdfft?md5=4098d24776d67b6dce49e54703c5dbcf&amp;pid=1-s2.0-S0924271609000884-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-73249139477&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0030>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object%20based%20image%20analysis%20for%20remote%20sensing&amp;publication_year=2010&amp;author=T.%20Blaschke" aria-describedby=ref-id-h0030>Google Scholar</a></div><dt class=label><a href=#bb0035 id=ref-id-b0035 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Blaschke et al., 2004</a><dd class=reference id=h0035><div class=contribution>T. Blaschke, C. Burnett, A. Pekkarinen<div id=ref-id-h0035><strong class=title>Image segmentation methods for object-based analysis and classification</strong></div></div><div class=host>F. De Meer, S. de Jong (Eds.), Remote Sensing Image Analysis: Including the Spatial Domain, Kluwer Academic Publishers, Dordrecht (2004), pp. 211-236</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/978-1-4020-2560-0_12></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/978-1-4020-2560-0_12 aria-describedby=ref-id-h0035>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Image%20segmentation%20methods%20for%20object-based%20analysis%20and%20classification&amp;publication_year=2004&amp;author=T.%20Blaschke&amp;author=C.%20Burnett&amp;author=A.%20Pekkarinen" aria-describedby=ref-id-h0035>Google Scholar</a></div><dt class=label><a href=#bb0040 id=ref-id-b0040 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Blaschke et al., 2014</a><dd class=reference id=h0040><div class=contribution>T. Blaschke, G.J. Hay, M. Kelly, S. Lang, P. Hofmann, E. Addink, R.Q. Feitosa, F.V.D. Meer, H.V.D. Werff, F.V. Coillie<div id=ref-id-h0040><strong class=title>Geographic object-based image analysis – towards a new paradigm</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 87 (2014), pp. 180-191</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002220 aria-describedby=ref-id-h0040>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002220/pdfft?md5=8bda8ea91a7e57ca60490ef3d5c6b2e0&amp;pid=1-s2.0-S0924271613002220-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84890209110&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0040>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Geographic%20object-based%20image%20analysis%20%20towards%20a%20new%20paradigm" aria-describedby=ref-id-h0040>Google Scholar</a></div><dt class=label><a href=#bb0045 id=ref-id-b0045 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Blaschke and Strobl, 2001</a><dd class=reference id=h0045><span>Blaschke, T., Strobl, J., 2001. What’s wrong with pixels? Some recent developments interfacing remote sensing and GIS. GeoBIT/GIS 6(1), 12–17.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Blaschke,%20T.,%20Strobl,%20J.,%202001.%20Whats%20wrong%20with%20pixels%20Some%20recent%20developments%20interfacing%20remote%20sensing%20and%20GIS.%20GeoBITGIS%206,%201217.">Google Scholar</a></div><dt class=label><a href=#bb0050 id=ref-id-b0050 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Borra-Serrano et al., 2015</a><dd class=reference id=h0050><div class=contribution>I. Borra-Serrano, J.M. Peña, J. Torres-Sánchez, F.J. Mesas-Carrascosa, F. López-Granados<div id=ref-id-h0050><strong class=title>Spatial quality evaluation of resampled unmanned aerial vehicle-imagery for weed mapping</strong></div></div><div class=host>Sensors, 15 (2015), pp. 19688-19708</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/s150819688></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/s150819688 aria-describedby=ref-id-h0050>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84939230880&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0050>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Spatial%20quality%20evaluation%20of%20resampled%20unmanned%20aerial%20vehicle-imagery%20for%20weed%20mapping&amp;publication_year=2015&amp;author=I.%20Borra-Serrano&amp;author=J.M.%20Pe%C3%B1a&amp;author=J.%20Torres-S%C3%A1nchez&amp;author=F.J.%20Mesas-Carrascosa&amp;author=F.%20L%C3%B3pez-Granados" aria-describedby=ref-id-h0050>Google Scholar</a></div><dt class=label><a href=#bb0055 id=ref-id-b0055 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Cánovas-García and Alonso-Sarría, 2015</a><dd class=reference id=h0055><div class=contribution>F. Cánovas-García, F. Alonso-Sarría<div id=ref-id-h0055><strong class=title>Optimal combination of classification algorithms and feature ranking methods for object-based classification of submeter resolution Z/I-Imaging DMC imagery</strong></div></div><div class=host>Remote Sens., 7 (2015), pp. 4651-4677</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs70404651></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs70404651 aria-describedby=ref-id-h0055>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84937837804&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0055>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Optimal%20combination%20of%20classification%20algorithms%20and%20feature%20ranking%20methods%20for%20object-based%20classification%20of%20submeter%20resolution%20ZI-Imaging%20DMC%20imagery" aria-describedby=ref-id-h0055>Google Scholar</a></div><dt class=label><a href=#bb0060 id=ref-id-b0060 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Charoenjit et al., 2015</a><dd class=reference id=h0060><div class=contribution>K. Charoenjit, P. Zuddas, P. Allemand, S. Pattanakiat, K. Pachana<div id=ref-id-h0060><strong class=title>Estimation of biomass and carbon stock in Para rubber plantations using object-based classification from Thaichote satellite data in Eastern Thailand</strong></div></div><div class=host>J. Appl. Remote Sens., 9 (2015), p. 096072</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1117/1.JRS.9.096072></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1117/1.JRS.9.096072 aria-describedby=ref-id-h0060>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84925423615&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0060>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Estimation%20of%20biomass%20and%20carbon%20stock%20in%20Para%20rubber%20plantations%20using%20object-based%20classification%20from%20Thaichote%20satellite%20data%20in%20Eastern%20Thailand&amp;publication_year=2015&amp;author=K.%20Charoenjit&amp;author=P.%20Zuddas&amp;author=P.%20Allemand&amp;author=S.%20Pattanakiat&amp;author=K.%20Pachana" aria-describedby=ref-id-h0060>Google Scholar</a></div><dt class=label><a href=#bb0065 id=ref-id-b0065 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Chirici et al., 2016</a><dd class=reference id=h0065><div class=contribution>G. Chirici, M. Mura, D. Mcinerney, N. Py, E.O. Tomppo, L.T. Waser, D. Travaglini, R.E. Mcroberts<div id=ref-id-h0065><strong class=title>A meta-analysis and review of the literature on the k-Nearest Neighbors technique for forestry applications that use remotely sensed data</strong></div></div><div class=host>Remote Sens. Environ., 176 (2016), pp. 282-294</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716300293 aria-describedby=ref-id-h0065>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716300293/pdfft?md5=22a579a8b933c79138368b2a707292f8&amp;pid=1-s2.0-S0034425716300293-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84957681049&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0065>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20meta-analysis%20and%20review%20of%20the%20literature%20on%20the%20k-Nearest%20Neighbors%20technique%20for%20forestry%20applications%20that%20use%20remotely%20sensed%20data&amp;publication_year=2016&amp;author=G.%20Chirici&amp;author=M.%20Mura&amp;author=D.%20Mcinerney&amp;author=N.%20Py&amp;author=E.O.%20Tomppo&amp;author=L.T.%20Waser&amp;author=D.%20Travaglini&amp;author=R.E.%20Mcroberts" aria-describedby=ref-id-h0065>Google Scholar</a></div><dt class=label><a href=#bb0070 id=ref-id-b0070 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Cleve et al., 2008</a><dd class=reference id=h0070><div class=contribution>C. Cleve, M. Kelly, F.R. Kearns, M. Moritz<div id=ref-id-h0070><strong class=title>Classification of the wildland–urban interface: a comparison of pixel- and object-based classifications using high-resolution aerial photography</strong></div></div><div class=host>Comput. Environ. Urban Syst., 32 (2008), pp. 317-326</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0198971507000737 aria-describedby=ref-id-h0070>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0198971507000737/pdfft?md5=7d46e4603db0cdddfa1cbe6fb12a5495&amp;pid=1-s2.0-S0198971507000737-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-49449103785&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0070>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Classification%20of%20the%20wildlandurban%20interface:%20a%20comparison%20of%20pixel-%20and%20object-based%20classifications%20using%20high-resolution%20aerial%20photography" aria-describedby=ref-id-h0070>Google Scholar</a></div><dt class=label><a href=#bb0075 id=ref-id-b0075 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Van Coillie et al., 2007</a><dd class=reference id=h0075><div class=contribution>F. Van Coillie, L. Verbeke, R.R. De Wulf<div id=ref-id-h0075><strong class=title>Feature selection by genetic algorithms in object-based classification of IKONOS imagery for forest mapping in Flanders, Belgium</strong></div></div><div class=host>Remote Sens. Environ., 110 (2007), pp. 476-487</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425707001782 aria-describedby=ref-id-h0075>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425707001782/pdfft?md5=f438b16753301b5c8f99500c538bfb23&amp;pid=1-s2.0-S0034425707001782-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-34548435161&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0075>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Feature%20selection%20by%20genetic%20algorithms%20in%20object-based%20classification%20of%20IKONOS%20imagery%20for%20forest%20mapping%20in%20Flanders%2C%20Belgium&amp;publication_year=2007&amp;author=F.%20Van%20Coillie&amp;author=L.%20Verbeke&amp;author=R.R.%20De%20Wulf" aria-describedby=ref-id-h0075>Google Scholar</a></div><dt class=label><a href=#bb0080 id=ref-id-b0080 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Colditz et al., 2006</a><dd class=reference id=h0080><div class=contribution>R.R. Colditz, T. Wehrmann, M. Bachmann, K. Steinnocher, M. Schmidt, G. Strunz, S. Dech<div id=ref-id-h0080><strong class=title>Influence of image fusion approaches on classification accuracy – a case study</strong></div></div><div class=host>Int. J. Remote Sens., 27 (2006), pp. 3311-3335</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160600649254></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160600649254 aria-describedby=ref-id-h0080>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33746998063&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0080>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Influence%20of%20image%20fusion%20approaches%20on%20classification%20accuracy%20%20a%20case%20study" aria-describedby=ref-id-h0080>Google Scholar</a></div><dt class=label><a href=#bb0085 id=ref-id-b0085 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Corcoran et al., 2015</a><dd class=reference id=h0085><div class=contribution>J. Corcoran, J. Knight, K. Pelletier, L. Rampi, Y. Wang<div id=ref-id-h0085><strong class=title>The effects of point or polygon based training data on RandomForest classification accuracy of wetlands</strong></div></div><div class=host>Remote Sens., 7 (2015), pp. 4002-4025</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs70404002></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs70404002 aria-describedby=ref-id-h0085>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84937941472&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0085>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20effects%20of%20point%20or%20polygon%20based%20training%20data%20on%20RandomForest%20classification%20accuracy%20of%20wetlands&amp;publication_year=2015&amp;author=J.%20Corcoran&amp;author=J.%20Knight&amp;author=K.%20Pelletier&amp;author=L.%20Rampi&amp;author=Y.%20Wang" aria-describedby=ref-id-h0085>Google Scholar</a></div><dt class=label><a href=#bb0090 id=ref-id-b0090 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Costa et al., 2014</a><dd class=reference id=h0090><div class=contribution>H. Costa, H. Carrão, F. Bação, M. Caetano<div id=ref-id-h0090><strong class=title>Combining per-pixel and object-based classifications for mapping land cover over large areas</strong></div></div><div class=host>Int. J. Remote Sens., 35 (2014), pp. 738-753</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2013.873151></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2013.873151 aria-describedby=ref-id-h0090>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84892975526&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0090>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Combining%20per-pixel%20and%20object-based%20classifications%20for%20mapping%20land%20cover%20over%20large%20areas&amp;publication_year=2014&amp;author=H.%20Costa&amp;author=H.%20Carr%C3%A3o&amp;author=F.%20Ba%C3%A7%C3%A3o&amp;author=M.%20Caetano" aria-describedby=ref-id-h0090>Google Scholar</a></div><dt class=label><a href=#bb0095 id=ref-id-b0095 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Drǎguţ et al., 2014</a><dd class=reference id=h0095><div class=contribution>L. Drǎguţ, O. Csillik, C. Eisank, D. Tiede<div id=ref-id-h0095><strong class=title>Automated parameterisation for multi-scale image segmentation on multiple layers</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 88 (2014), pp. 119-127</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002803 aria-describedby=ref-id-h0095>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002803/pdfft?md5=c4cd3c390396302df359a152aaa6a690&amp;pid=1-s2.0-S0924271613002803-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84891136260&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0095>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Automated%20parameterisation%20for%20multi-scale%20image%20segmentation%20on%20multiple%20layers&amp;publication_year=2014&amp;author=L.%20Dr%C7%8Egu%C5%A3&amp;author=O.%20Csillik&amp;author=C.%20Eisank&amp;author=D.%20Tiede" aria-describedby=ref-id-h0095>Google Scholar</a></div><dt class=label><a href=#bb0100 id=ref-id-b0100 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Drǎguţ and Eisank, 2012</a><dd class=reference id=h0100><div class=contribution>L. Drǎguţ, C. Eisank<div id=ref-id-h0100><strong class=title>Automated object-based classification of topography from SRTM data</strong></div></div><div class=host>Geomorphology, 141 (142) (2012), pp. 21-33</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0169555X11006143 aria-describedby=ref-id-h0100>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0169555X11006143/pdfft?md5=6ebac352ecd3e6ca962c9b1dabd41770&amp;pid=1-s2.0-S0169555X11006143-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84856418204&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0100>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Automated%20object-based%20classification%20of%20topography%20from%20SRTM%20data&amp;publication_year=2012&amp;author=L.%20Dr%C7%8Egu%C5%A3&amp;author=C.%20Eisank" aria-describedby=ref-id-h0100>Google Scholar</a></div><dt class=label><a href=#bb0105 id=ref-id-b0105 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Drǎguţ et al., 2010</a><dd class=reference id=h0105><div class=contribution>L. Drǎguţ, D. Tiede, S.R. Levick<div id=ref-id-h0105><strong class=title>ESP: a tool to estimate scale parameter for multiresolution image segmentation of remotely sensed data</strong></div></div><div class=host>Int. J. Geogr. Inf. Sci., 24 (2010), pp. 859-871</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/13658810903174803></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/13658810903174803 aria-describedby=ref-id-h0105>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-77951189897&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0105>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=ESP%3A%20a%20tool%20to%20estimate%20scale%20parameter%20for%20multiresolution%20image%20segmentation%20of%20remotely%20sensed%20data&amp;publication_year=2010&amp;author=L.%20Dr%C7%8Egu%C5%A3&amp;author=D.%20Tiede&amp;author=S.R.%20Levick" aria-describedby=ref-id-h0105>Google Scholar</a></div><dt class=label><a href=#bb0110 id=ref-id-b0110 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Dronova, 2015</a><dd class=reference id=h0110><div class=contribution>I. Dronova<div id=ref-id-h0110><strong class=title>Object-based image analysis in wetland research: a review</strong></div></div><div class=host>Remote Sens., 7 (2015), pp. 6380-6413</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs70506380></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs70506380 aria-describedby=ref-id-h0110>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84930017658&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0110>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20image%20analysis%20in%20wetland%20research%3A%20a%20review&amp;publication_year=2015&amp;author=I.%20Dronova" aria-describedby=ref-id-h0110>Google Scholar</a></div><dt class=label><a href=#bb0115 id=ref-id-b0115 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Dronova et al., 2011</a><dd class=reference id=h0115><div class=contribution>I. Dronova, G. Peng, L. Wang<div id=ref-id-h0115><strong class=title>Object-based analysis and change detection of major wetland cover types and their classification uncertainty during the low water period at Poyang Lake, China</strong></div></div><div class=host>Remote Sens. Environ., 115 (2011), pp. 3220-3236</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002501 aria-describedby=ref-id-h0115>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002501/pdfft?md5=87c8fc5fbb0f6e8eb5ef6d49c398733b&amp;pid=1-s2.0-S0034425711002501-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-81355138692&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0115>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20analysis%20and%20change%20detection%20of%20major%20wetland%20cover%20types%20and%20their%20classification%20uncertainty%20during%20the%20low%20water%20period%20at%20Poyang%20Lake%2C%20China&amp;publication_year=2011&amp;author=I.%20Dronova&amp;author=G.%20Peng&amp;author=L.%20Wang" aria-describedby=ref-id-h0115>Google Scholar</a></div><dt class=label><a href=#bb0120 id=ref-id-b0120 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Du et al., 2015</a><dd class=reference id=h0120><div class=contribution>S. Du, F. Zhang, X. Zhang<div id=ref-id-h0120><strong class=title>Semantic classification of urban buildings combining VHR image and GIS data: an improved random forest approach</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 105 (2015), pp. 107-119</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161500091X aria-describedby=ref-id-h0120>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427161500091X/pdfft?md5=7bcadde5d5353609cc8b3d4faab04915&amp;pid=1-s2.0-S092427161500091X-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84927672061&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0120>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Semantic%20classification%20of%20urban%20buildings%20combining%20VHR%20image%20and%20GIS%20data%3A%20an%20improved%20random%20forest%20approach&amp;publication_year=2015&amp;author=S.%20Du&amp;author=F.%20Zhang&amp;author=X.%20Zhang" aria-describedby=ref-id-h0120>Google Scholar</a></div><dt class=label><a href=#bb0125 id=ref-id-b0125 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Đurić et al., 2014</a><dd class=reference id=h0125><div class=contribution>N. Đurić, P. Pehani, K. Oštir<div id=ref-id-h0125><strong class=title>Application of in-segment multiple sampling in object-based classification</strong></div></div><div class=host>Remote Sens., 6 (2014), pp. 12138-12165</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs61212138></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs61212138 aria-describedby=ref-id-h0125>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84920089298&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0125>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Application%20of%20in-segment%20multiple%20sampling%20in%20object-based%20classification&amp;publication_year=2014&amp;author=N.%20%C4%90uri%C4%87&amp;author=P.%20Pehani&amp;author=K.%20O%C5%A1tir" aria-describedby=ref-id-h0125>Google Scholar</a></div><dt class=label><a href=#bb0130 id=ref-id-b0130 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Duro et al., 2012a</a><dd class=reference id=h0130><div class=contribution>D.C. Duro, S.E. Franklin, M.G. Dubé<div id=ref-id-h0130><strong class=title>A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using SPOT-5 HRG imagery</strong></div></div><div class=host>Remote Sens. Environ., 118 (2012), pp. 259-272</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711004172 aria-describedby=ref-id-h0130>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711004172/pdfft?md5=0ef375565045bd6fd517ca47f687b9be&amp;pid=1-s2.0-S0034425711004172-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84455200427&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0130>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20comparison%20of%20pixel-based%20and%20object-based%20image%20analysis%20with%20selected%20machine%20learning%20algorithms%20for%20the%20classification%20of%20agricultural%20landscapes%20using%20SPOT-5%20HRG%20imagery&amp;publication_year=2012&amp;author=D.C.%20Duro&amp;author=S.E.%20Franklin&amp;author=M.G.%20Dub%C3%A9" aria-describedby=ref-id-h0130>Google Scholar</a></div><dt class=label><a href=#bb0135 id=ref-id-b0135 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Duro et al., 2012b</a><dd class=reference id=h0135><div class=contribution>D.C. Duro, S.E. Franklin, M.G. Dubé<div id=ref-id-h0135><strong class=title>Multi-scale object-based image analysis and feature selection of multi-sensor earth observation imagery using random forests</strong></div></div><div class=host>Int. J. Remote Sens., 33 (2012), pp. 4502-4526</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2011.649864></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2011.649864 aria-describedby=ref-id-h0135>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84856982478&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0135>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Multi-scale%20object-based%20image%20analysis%20and%20feature%20selection%20of%20multi-sensor%20earth%20observation%20imagery%20using%20random%20forests&amp;publication_year=2012&amp;author=D.C.%20Duro&amp;author=S.E.%20Franklin&amp;author=M.G.%20Dub%C3%A9" aria-describedby=ref-id-h0135>Google Scholar</a></div><dt class=label><a href=#bb0140 id=ref-id-b0140 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Espindola et al., 2006</a><dd class=reference id=h0140><div class=contribution>G.M. Espindola, G. Camara, I.A. Reis, L.S. Bins, A.M. Monteiro<div id=ref-id-h0140><strong class=title>Parameter selection for region-growing image segmentation algorithms using spatial autocorrelation</strong></div></div><div class=host>Int. J. Remote Sens., 27 (2006), pp. 3035-3040</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160600617194></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160600617194 aria-describedby=ref-id-h0140>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33747105856&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0140>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Parameter%20selection%20for%20region-growing%20image%20segmentation%20algorithms%20using%20spatial%20autocorrelation&amp;publication_year=2006&amp;author=G.M.%20Espindola&amp;author=G.%20Camara&amp;author=I.A.%20Reis&amp;author=L.S.%20Bins&amp;author=A.M.%20Monteiro" aria-describedby=ref-id-h0140>Google Scholar</a></div><dt class=label><a href=#bb0145 id=ref-id-b0145 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Evans et al., 2014</a><dd class=reference id=h0145><div class=contribution>T.L. Evans, M. Costa, W.M. Tomas, A.R. Camilo<div id=ref-id-h0145><strong class=title>Large-scale habitat mapping of the Brazilian Pantanal wetland: a synthetic aperture radar approach</strong></div></div><div class=host>Remote Sens. Environ., 155 (2014), pp. 89-108</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425714001576 aria-describedby=ref-id-h0145>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425714001576/pdfft?md5=be7fbb2a0af32f75f31f94f189d1e15c&amp;pid=1-s2.0-S0034425714001576-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84909582564&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0145>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Large-scale%20habitat%20mapping%20of%20the%20Brazilian%20Pantanal%20wetland%3A%20a%20synthetic%20aperture%20radar%20approach&amp;publication_year=2014&amp;author=T.L.%20Evans&amp;author=M.%20Costa&amp;author=W.M.%20Tomas&amp;author=A.R.%20Camilo" aria-describedby=ref-id-h0145>Google Scholar</a></div><dt class=label><a href=#bb0150 id=ref-id-b0150 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Fernandes et al., 2014</a><dd class=reference id=h0150><div class=contribution>M.R. Fernandes, F.C. Aguiar, J.M.N. Silva, M.T. Ferreira, J.M.C. Pereira<div id=ref-id-h0150><strong class=title>Optimal attributes for the object based detection of giant reed in riparian habitats: a comparative study between Airborne High Spatial Resolution and WorldView-2 imagery</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 32 (2014), pp. 79-91</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243414000841 aria-describedby=ref-id-h0150>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243414000841/pdfft?md5=9d2c75d1decc8f76acf973d636c2c5de&amp;pid=1-s2.0-S0303243414000841-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.12660/bre.v34n12014.18106 aria-describedby=ref-id-h0150>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84904514055&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0150>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Optimal%20attributes%20for%20the%20object%20based%20detection%20of%20giant%20reed%20in%20riparian%20habitats%3A%20a%20comparative%20study%20between%20Airborne%20High%20Spatial%20Resolution%20and%20WorldView-2%20imagery&amp;publication_year=2014&amp;author=M.R.%20Fernandes&amp;author=F.C.%20Aguiar&amp;author=J.M.N.%20Silva&amp;author=M.T.%20Ferreira&amp;author=J.M.C.%20Pereira" aria-describedby=ref-id-h0150>Google Scholar</a></div><dt class=label><a href=#bb0155 id=ref-id-b0155 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Fisher, 2010</a><dd class=reference id=h0155><div class=contribution>P.F. Fisher<div id=ref-id-h0155><strong class=title>Remote sensing of land cover classes as type 2 fuzzy sets</strong></div></div><div class=host>Remote Sens. Environ., 114 (2010), pp. 309-321</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425709002764 aria-describedby=ref-id-h0155>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425709002764/pdfft?md5=0bc376d37e5b3b0a7bd81463b989b1e9&amp;pid=1-s2.0-S0034425709002764-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-71349086465&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0155>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Remote%20sensing%20of%20land%20cover%20classes%20as%20type%202%20fuzzy%20sets&amp;publication_year=2010&amp;author=P.F.%20Fisher" aria-describedby=ref-id-h0155>Google Scholar</a></div><dt class=label><a href=#bb0160 id=ref-id-b0160 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody and Atkinson, 2002</a><dd class=reference id=h0160><div class=contribution>G.M. Foody, P.M. Atkinson<div id=ref-id-h0160><strong class=title>Uncertainty in Remote Sensing and GIS</strong></div></div><div class=host>John Wiley &amp; Sons Ltd, Chichester, England (2002)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Uncertainty%20in%20Remote%20Sensing%20and%20GIS&amp;publication_year=2002&amp;author=G.M.%20Foody&amp;author=P.M.%20Atkinson" aria-describedby=ref-id-h0160>Google Scholar</a></div><dt class=label><a href=#bb0165 id=ref-id-b0165 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Freire et al., 2014</a><dd class=reference id=h0165><div class=contribution>S. Freire, T. Santos, A. Navarro, F. Soares, J.D. Silva, N. Afonso, A. Fonseca, J. Tenedório<div id=ref-id-h0165><strong class=title>Introducing mapping standards in the quality assessment of buildings extracted from very high resolution satellite imagery</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 90 (2014), pp. 1-9</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613003031 aria-describedby=ref-id-h0165>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613003031/pdfft?md5=bcf6f6e44a113b12f7d4fd5760f091ff&amp;pid=1-s2.0-S0924271613003031-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Introducing%20mapping%20standards%20in%20the%20quality%20assessment%20of%20buildings%20extracted%20from%20very%20high%20resolution%20satellite%20imagery&amp;publication_year=2014&amp;author=S.%20Freire&amp;author=T.%20Santos&amp;author=A.%20Navarro&amp;author=F.%20Soares&amp;author=J.D.%20Silva&amp;author=N.%20Afonso&amp;author=A.%20Fonseca&amp;author=J.%20Tened%C3%B3rio" aria-describedby=ref-id-h0165>Google Scholar</a></div><dt class=label><a href=#bb0170 id=ref-id-b0170 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Gao, 2008</a><dd class=reference id=h0170><div class=contribution>J. Gao<div id=ref-id-h0170><strong class=title>Mapping of land degradation from ASTER data: a comparison of object-based and pixel-based methods</strong></div></div><div class=host>Gisci. Remote Sens., 45 (2008), pp. 149-166</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.2747/1548-1603.45.2.149></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.2747/1548-1603.45.2.149 aria-describedby=ref-id-h0170>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-44649142555&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0170>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Mapping%20of%20land%20degradation%20from%20ASTER%20data%3A%20a%20comparison%20of%20object-based%20and%20pixel-based%20methods&amp;publication_year=2008&amp;author=J.%20Gao" aria-describedby=ref-id-h0170>Google Scholar</a></div><dt class=label><a href=#bb0175 id=ref-id-b0175 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Gavazzi et al., 2016</a><dd class=reference id=h0175><div class=contribution>G.M. Gavazzi, F. Madricardo, L. Janowski, A. Kruss, P. Blondel, M. Sigovini, F. Foglini<div id=ref-id-h0175><strong class=title>Evaluation of seabed mapping methods for fine-scale classification of extremely shallow benthic habitats – application to the Venice Lagoon, Italy</strong></div></div><div class=host>Estuarine Coast. Shelf Sci., 170 (2016), pp. 45-60</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Evaluation%20of%20seabed%20mapping%20methods%20for%20fine-scale%20classification%20of%20extremely%20shallow%20benthic%20habitats%20%20application%20to%20the%20Venice%20Lagoon,%20Italy" aria-describedby=ref-id-h0175>Google Scholar</a></div><dt class=label><a href=#bb0180 id=ref-id-b0180 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ghosh and Joshi, 2014</a><dd class=reference id=h0180><div class=contribution>A. Ghosh, P.K. Joshi<div id=ref-id-h0180><strong class=title>A comparison of selected classification algorithms for mapping bamboo patches in lower Gangetic plains using very high resolution WorldView 2 imagery</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 26 (2014), pp. 298-311</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413000974 aria-describedby=ref-id-h0180>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413000974/pdfft?md5=c7c6c1d7cbaeed0853c02d8e45c3313a&amp;pid=1-s2.0-S0303243413000974-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84897586103&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0180>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20comparison%20of%20selected%20classification%20algorithms%20for%20mapping%20bamboo%20patches%20in%20lower%20Gangetic%20plains%20using%20very%20high%20resolution%20WorldView%202%20imagery&amp;publication_year=2014&amp;author=A.%20Ghosh&amp;author=P.K.%20Joshi" aria-describedby=ref-id-h0180>Google Scholar</a></div><dt class=label><a href=#bb0185 id=ref-id-b0185 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Goodin et al., 2015</a><dd class=reference id=h0185><div class=contribution>D.G. Goodin, K.L. Anibas, M. Bezymennyi<div id=ref-id-h0185><strong class=title>Mapping land cover and land use from object-based classification: an example from a complex agricultural landscape</strong></div></div><div class=host>Int. J. Remote Sens., 36 (2015), pp. 4702-4723</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2015.1088674></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2015.1088674 aria-describedby=ref-id-h0185>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84943374021&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0185>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Mapping%20land%20cover%20and%20land%20use%20from%20object-based%20classification%3A%20an%20example%20from%20a%20complex%20agricultural%20landscape&amp;publication_year=2015&amp;author=D.G.%20Goodin&amp;author=K.L.%20Anibas&amp;author=M.%20Bezymennyi" aria-describedby=ref-id-h0185>Google Scholar</a></div><dt class=label><a href=#bb0190 id=ref-id-b0190 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hofmann, 2016</a><dd class=reference id=h0190><div class=contribution>P. Hofmann<div id=ref-id-h0190><strong class=title>Defuzzification strategies for fuzzy classifications of remote sensing data</strong></div></div><div class=host>Remote Sens., 8 (2016), p. 467</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs8060467></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs8060467 aria-describedby=ref-id-h0190>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84974849295&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0190>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Defuzzification%20strategies%20for%20fuzzy%20classifications%20of%20remote%20sensing%20data&amp;publication_year=2016&amp;author=P.%20Hofmann" aria-describedby=ref-id-h0190>Google Scholar</a></div><dt class=label><a href=#bb0195 id=ref-id-b0195 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hofmann et al., 2011</a><dd class=reference id=h0195><div class=contribution>P. Hofmann, T. Blaschke, J. Strobl<div id=ref-id-h0195><strong class=title>Quantifying the robustness of fuzzy rule sets in object-based image analysis</strong></div></div><div class=host>Int. J. Remote Sens., 32 (2011), pp. 7359-7381</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2010.523727></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2010.523727 aria-describedby=ref-id-h0195>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-82155168615&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0195>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Quantifying%20the%20robustness%20of%20fuzzy%20rule%20sets%20in%20object-based%20image%20analysis&amp;publication_year=2011&amp;author=P.%20Hofmann&amp;author=T.%20Blaschke&amp;author=J.%20Strobl" aria-describedby=ref-id-h0195>Google Scholar</a></div><dt class=label><a href=#bb0200 id=ref-id-b0200 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Melgani and Bruzzone, 2004</a><dd class=reference id=h0200><div class=contribution>F. Melgani, L. Bruzzone<div id=ref-id-h0200><strong class=title>Classification of hyperspectral remote sensing images with support vector machines</strong></div></div><div class=host>IEEE Trans. Geosci. Remote Sens., 42 (2004), pp. 1778-1790</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-4344614511></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-4344614511&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0200>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Classification%20of%20hyperspectral%20remote%20sensing%20images%20with%20support%20vector%20machines&amp;publication_year=2004&amp;author=F.%20Melgani&amp;author=L.%20Bruzzone" aria-describedby=ref-id-h0200>Google Scholar</a></div><dt class=label><a href=#bb0205 id=ref-id-b0205 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Guan et al., 2013</a><dd class=reference id=h0205><div class=contribution>H. Guan, J. Li, M. Chapman, F. Deng, Z. Ji, X. Yang<div id=ref-id-h0205><strong class=title>Integration of orthoimagery and lidar data for object-based urban thematic mapping using random forests</strong></div></div><div class=host>Int. J. Remote Sens., 34 (2013), pp. 5166-5186</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2013.788261></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2013.788261 aria-describedby=ref-id-h0205>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84876300410&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0205>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Integration%20of%20orthoimagery%20and%20lidar%20data%20for%20object-based%20urban%20thematic%20mapping%20using%20random%20forests&amp;publication_year=2013&amp;author=H.%20Guan&amp;author=J.%20Li&amp;author=M.%20Chapman&amp;author=F.%20Deng&amp;author=Z.%20Ji&amp;author=X.%20Yang" aria-describedby=ref-id-h0205>Google Scholar</a></div><dt class=label><a href=#bb0210 id=ref-id-b0210 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hao et al., 2015</a><dd class=reference id=h0210><div class=contribution>P. Hao, L. Wang, Z. Niu<div id=ref-id-h0210><strong class=title>Comparison of hybrid classifiers for crop classification using normalized difference vegetation index time series: a case study for major crops in North Xinjiang, China</strong></div></div><div class=host>Plos One, 10 (2015), p. e0137748</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1371/journal.pone.0137748></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1371/journal.pone.0137748 aria-describedby=ref-id-h0210>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84945281405&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0210>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Comparison%20of%20hybrid%20classifiers%20for%20crop%20classification%20using%20normalized%20difference%20vegetation%20index%20time%20series%3A%20a%20case%20study%20for%20major%20crops%20in%20North%20Xinjiang%2C%20China&amp;publication_year=2015&amp;author=P.%20Hao&amp;author=L.%20Wang&amp;author=Z.%20Niu" aria-describedby=ref-id-h0210>Google Scholar</a></div><dt class=label><a href=#bb0215 id=ref-id-b0215 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hay and Blaschke, 2010</a><dd class=reference id=h0215><div class=contribution>G.J. Hay, T. Blaschke<div id=ref-id-h0215><strong class=title>Special issue: geographic object-based image analysis (GEOBIA)</strong></div></div><div class=host>Photogramm. Eng. Remote Sens., 76 (2010), pp. 121-122</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-77649094470></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-77649094470&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0215>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Special%20issue:%20geographic%20object-based%20image%20analysis" aria-describedby=ref-id-h0215>Google Scholar</a></div><dt class=label><a href=#bb0220 id=ref-id-b0220 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hay and Castilla, 2008</a><dd class=reference id=h0220><div class=contribution>G.J. Hay, G. Castilla<div id=ref-id-h0220><strong class=title>Geographic Object-Based Image Analysis (GEOBIA): A New Name for a New Discipline</strong></div></div><div class=host>Springer, Berlin (2008)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Geographic%20Object-Based%20Image%20Analysis%20:%20A%20New%20Name%20for%20a%20New%20Discipline" aria-describedby=ref-id-h0220>Google Scholar</a></div><dt class=label><a href=#bb0225 id=ref-id-b0225 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hussain et al., 2013</a><dd class=reference id=h0225><div class=contribution>M. Hussain, D. Chen, A. Cheng, H. Wei, D. Stanley<div id=ref-id-h0225><strong class=title>Change detection from remotely sensed images: from pixel-based to object-based approaches</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 80 (2013), pp. 91-106</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613000804 aria-describedby=ref-id-h0225>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613000804/pdfft?md5=b603242e59fd181b58c7008b5ab09795&amp;pid=1-s2.0-S0924271613000804-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84876726723&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0225>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Change%20detection%20from%20remotely%20sensed%20images%3A%20from%20pixel-based%20to%20object-based%20approaches&amp;publication_year=2013&amp;author=M.%20Hussain&amp;author=D.%20Chen&amp;author=A.%20Cheng&amp;author=H.%20Wei&amp;author=D.%20Stanley" aria-describedby=ref-id-h0225>Google Scholar</a></div><dt class=label><a href=#bb0230 id=ref-id-b0230 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Johnson and Xie, 2011</a><dd class=reference id=h0230><div class=contribution>B. Johnson, Z. Xie<div id=ref-id-h0230><strong class=title>Unsupervised image segmentation evaluation and refinement using a multi-scale approach</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 66 (2011), pp. 473-483</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271611000293 aria-describedby=ref-id-h0230>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271611000293/pdfft?md5=7da60999302b5baf957cfe3bcc7f90dd&amp;pid=1-s2.0-S0924271611000293-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79957819255&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0230>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Unsupervised%20image%20segmentation%20evaluation%20and%20refinement%20using%20a%20multi-scale%20approach&amp;publication_year=2011&amp;author=B.%20Johnson&amp;author=Z.%20Xie" aria-describedby=ref-id-h0230>Google Scholar</a></div><dt class=label><a href=#bb0235 id=ref-id-b0235 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ke et al., 2010</a><dd class=reference id=h0235><div class=contribution>Y. Ke, L.J. Quackenbush, J. Im<div id=ref-id-h0235><strong class=title>Synergistic use of QuickBird multispectral imagery and LIDAR data for object-based forest species classification</strong></div></div><div class=host>Remote Sens. Environ., 114 (2010), pp. 1141-1154</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425710000246 aria-describedby=ref-id-h0235>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425710000246/pdfft?md5=7c022a3e795810ef761910a535b9026c&amp;pid=1-s2.0-S0034425710000246-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-77949657728&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0235>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Synergistic%20use%20of%20QuickBird%20multispectral%20imagery%20and%20LIDAR%20data%20for%20object-based%20forest%20species%20classification&amp;publication_year=2010&amp;author=Y.%20Ke&amp;author=L.J.%20Quackenbush&amp;author=J.%20Im" aria-describedby=ref-id-h0235>Google Scholar</a></div><dt class=label><a href=#bb0240 id=ref-id-b0240 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Khatami et al., 2016</a><dd class=reference id=h0240><div class=contribution>R. Khatami, G. Mountrakis, S.V. Stehman<div id=ref-id-h0240><strong class=title>A meta-analysis of remote sensing research on supervised pixel-based land-cover image classification processes: general guidelines for practitioners and future research</strong></div></div><div class=host>Remote Sens. Environ., 177 (2016), pp. 89-100</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716300578 aria-describedby=ref-id-h0240>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716300578/pdfft?md5=c2fceb40299f72283bc7ad9624ffa10f&amp;pid=1-s2.0-S0034425716300578-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84958236853&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0240>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20meta-analysis%20of%20remote%20sensing%20research%20on%20supervised%20pixel-based%20land-cover%20image%20classification%20processes%3A%20general%20guidelines%20for%20practitioners%20and%20future%20research&amp;publication_year=2016&amp;author=R.%20Khatami&amp;author=G.%20Mountrakis&amp;author=S.V.%20Stehman" aria-describedby=ref-id-h0240>Google Scholar</a></div><dt class=label><a href=#bb0245 id=ref-id-b0245 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kim and Yeom, 2014</a><dd class=reference id=h0245><div class=contribution>H.O. Kim, J.M. Yeom<div id=ref-id-h0245><strong class=title>Effect of red-edge and texture features for object-based paddy rice crop classification using RapidEye multi-spectral satellite image data</strong></div></div><div class=host>Int. J. Remote Sens., 35 (2014), pp. 7046-7068</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-84908079629></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84908079629&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0245>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Effect%20of%20red-edge%20and%20texture%20features%20for%20object-based%20paddy%20rice%20crop%20classification%20using%20RapidEye%20multi-spectral%20satellite%20image%20data&amp;publication_year=2014&amp;author=H.O.%20Kim&amp;author=J.M.%20Yeom" aria-describedby=ref-id-h0245>Google Scholar</a></div><dt class=label><a href=#bb0250 id=ref-id-b0250 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kim et al., 2008</a><dd class=reference id=h0250><div class=contribution>M. Kim, M. Madden, T. Warner<div id=ref-id-h0250><strong class=title>Estimation of Optimal Image Object Size for the Segmentation of Forest Stands with Multispectral IKONOS Imagery</strong></div></div><div class=host>Springer, Berlin (2008)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Estimation%20of%20Optimal%20Image%20Object%20Size%20for%20the%20Segmentation%20of%20Forest%20Stands%20with%20Multispectral%20IKONOS%20Imagery&amp;publication_year=2008&amp;author=M.%20Kim&amp;author=M.%20Madden&amp;author=T.%20Warner" aria-describedby=ref-id-h0250>Google Scholar</a></div><dt class=label><a href=#bb0255 id=ref-id-b0255 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kim et al., 2011</a><dd class=reference id=h0255><div class=contribution>M. Kim, T.A. Warner, M. Madden, D.S. Atkinson<div id=ref-id-h0255><strong class=title>Multi-scale GEOBIA with very high spatial resolution digital aerial imagery: scale, texture and image objects</strong></div></div><div class=host>Int. J. Remote Sens., 32 (2011), pp. 2825-2850</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161003745608></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161003745608 aria-describedby=ref-id-h0255>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79957608758&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0255>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Multi-scale%20GEOBIA%20with%20very%20high%20spatial%20resolution%20digital%20aerial%20imagery%3A%20scale%2C%20texture%20and%20image%20objects&amp;publication_year=2011&amp;author=M.%20Kim&amp;author=T.A.%20Warner&amp;author=M.%20Madden&amp;author=D.S.%20Atkinson" aria-describedby=ref-id-h0255>Google Scholar</a></div><dt class=label><a href=#bb0260 id=ref-id-b0260 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Längkvist et al., 2016</a><dd class=reference id=h0260><div class=contribution>M. Längkvist, A. Kiselev, M. Alirezaie, A. Loutfi<div id=ref-id-h0260><strong class=title>Classification and segmentation of satellite orthoimagery using convolutional neural networks</strong></div></div><div class=host>Remote Sens., 8 (2016), p. 329</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs8040329></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs8040329 aria-describedby=ref-id-h0260>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84971612769&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0260>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Classification%20and%20segmentation%20of%20satellite%20orthoimagery%20using%20convolutional%20neural%20networks&amp;publication_year=2016&amp;author=M.%20L%C3%A4ngkvist&amp;author=A.%20Kiselev&amp;author=M.%20Alirezaie&amp;author=A.%20Loutfi" aria-describedby=ref-id-h0260>Google Scholar</a></div><dt class=label><a href=#bb0265 id=ref-id-b0265 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Löw et al., 2015</a><dd class=reference id=h0265><div class=contribution>F. Löw, P. Knöfel, C. Conrad<div id=ref-id-h0265><strong class=title>Analysis of uncertainty in multi-temporal object-based classification</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 105 (2015), pp. 91-106</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000635 aria-describedby=ref-id-h0265>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000635/pdfft?md5=2b46d0fc665e45567f709ee7dc756fd6&amp;pid=1-s2.0-S0924271615000635-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84927598104&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0265>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Analysis%20of%20uncertainty%20in%20multi-temporal%20object-based%20classification&amp;publication_year=2015&amp;author=F.%20L%C3%B6w&amp;author=P.%20Kn%C3%B6fel&amp;author=C.%20Conrad" aria-describedby=ref-id-h0265>Google Scholar</a></div><dt class=label><a href=#bb0270 id=ref-id-b0270 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Laliberte et al., 2006</a><dd class=reference id=h0270><span>Laliberte, A., Koppa, J., Fredrickson, E., Rango, A., 2006. Comparison of nearest neighbor and rule-based decision tree classification in an object-oriented environment. In: Geoscience and Remote Sensing Symposium Proceedings (IGARSS), Denver, 2006 IEEE International, July, pp. 3923–3926.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Laliberte,%20A.,%20Koppa,%20J.,%20Fredrickson,%20E.,%20Rango,%20A.,%202006.%20Comparison%20of%20nearest%20neighbor%20and%20rule-based%20decision%20tree%20classification%20in%20an%20object-oriented%20environment.%20In:%20Geoscience%20and%20Remote%20Sensing%20Symposium%20Proceedings%20,%20Denver,%202006%20IEEE%20International,%20July,%20pp.%2039233926.">Google Scholar</a></div><dt class=label><a href=#bb0275 id=ref-id-b0275 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Laliberte et al., 2012</a><dd class=reference id=h0275><div class=contribution>A.S. Laliberte, D.M. Browning, A. Rango<div id=ref-id-h0275><strong class=title>A comparison of three feature selection methods for object-based classification of sub-decimeter resolution UltraCam-L imagery</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 15 (2012), pp. 70-78</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243411000742 aria-describedby=ref-id-h0275>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243411000742/pdfft?md5=285412ed6c6c9786ea76034d89ceb7ae&amp;pid=1-s2.0-S0303243411000742-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84864510265&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0275>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20comparison%20of%20three%20feature%20selection%20methods%20for%20object-based%20classification%20of%20sub-decimeter%20resolution%20UltraCam-L%20imagery&amp;publication_year=2012&amp;author=A.S.%20Laliberte&amp;author=D.M.%20Browning&amp;author=A.%20Rango" aria-describedby=ref-id-h0275>Google Scholar</a></div><dt class=label><a href=#bb0280 id=ref-id-b0280 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Laliberte and Rango, 2009</a><dd class=reference id=h0280><div class=contribution>A.S. Laliberte, A. Rango<div id=ref-id-h0280><strong class=title>Texture and scale in object-based analysis of subdecimeter resolution Unmanned Aerial Vehicle (UAV) imagery</strong></div></div><div class=host>IEEE Trans. Geosci. Remote Sens., 47 (2009), pp. 761-770</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-61349099437></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-61349099437&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0280>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Texture%20and%20scale%20in%20object-based%20analysis%20of%20subdecimeter%20resolution%20Unmanned%20Aerial%20Vehicle%20%20imagery" aria-describedby=ref-id-h0280>Google Scholar</a></div><dt class=label><a href=#bb0285 id=ref-id-b0285 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Laliberte et al., 2004</a><dd class=reference id=h0285><div class=contribution>A.S. Laliberte, A. Rango, K.M. Havstad, J.F. Paris, R.F. Beck, R. Mcneely, A.L. Gonzalez<div id=ref-id-h0285><strong class=title>Object-oriented image analysis for mapping shrub encroachment from 1937 to 2003 in southern New Mexico</strong></div></div><div class=host>Remote Sens. Environ., 93 (2004), pp. 198-210</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425704002147 aria-describedby=ref-id-h0285>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425704002147/pdfft?md5=f9498bf22728675771641f32764c5309&amp;pid=1-s2.0-S0034425704002147-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-4544310502&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0285>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-oriented%20image%20analysis%20for%20mapping%20shrub%20encroachment%20from%201937%20to%202003%20in%20southern%20New%20Mexico&amp;publication_year=2004&amp;author=A.S.%20Laliberte&amp;author=A.%20Rango&amp;author=K.M.%20Havstad&amp;author=J.F.%20Paris&amp;author=R.F.%20Beck&amp;author=R.%20Mcneely&amp;author=A.L.%20Gonzalez" aria-describedby=ref-id-h0285>Google Scholar</a></div><dt class=label><a href=#bb0290 id=ref-id-b0290 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Langner et al., 2014</a><dd class=reference id=h0290><div class=contribution>A. Langner, Y. Hirata, H. Saito, H. Sokh, C. Leng, C. Pak, R. Raši<div id=ref-id-h0290><strong class=title>Spectral normalization of SPOT 4 data to adjust for changing leaf phenology within seasonal forests in Cambodia</strong></div></div><div class=host>Remote Sens. Environ., 143 (2014), pp. 122-130</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425713004525 aria-describedby=ref-id-h0290>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425713004525/pdfft?md5=3a0a52c4469a0cf7183b1cf48265f9a1&amp;pid=1-s2.0-S0034425713004525-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84892858702&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0290>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Spectral%20normalization%20of%20SPOT%204%20data%20to%20adjust%20for%20changing%20leaf%20phenology%20within%20seasonal%20forests%20in%20Cambodia&amp;publication_year=2014&amp;author=A.%20Langner&amp;author=Y.%20Hirata&amp;author=H.%20Saito&amp;author=H.%20Sokh&amp;author=C.%20Leng&amp;author=C.%20Pak&amp;author=R.%20Ra%C5%A1i" aria-describedby=ref-id-h0290>Google Scholar</a></div><dt class=label><a href=#bb0295 id=ref-id-b0295 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Leon and Woodroffe, 2011</a><dd class=reference id=h0295><div class=contribution>J. Leon, C.D. Woodroffe<div id=ref-id-h0295><strong class=title>Improving the synoptic mapping of coral reef geomorphology using object-based image analysis</strong></div></div><div class=host>Int. J. Geogr. Inf. Sci., 25 (2011), pp. 949-969</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/13658816.2010.513980></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/13658816.2010.513980 aria-describedby=ref-id-h0295>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79960684595&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0295>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Improving%20the%20synoptic%20mapping%20of%20coral%20reef%20geomorphology%20using%20object-based%20image%20analysis&amp;publication_year=2011&amp;author=J.%20Leon&amp;author=C.D.%20Woodroffe" aria-describedby=ref-id-h0295>Google Scholar</a></div><dt class=label><a href=#bb0300 id=ref-id-b0300 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Li et al., 2015a</a><dd class=reference id=h0300><div class=contribution>M. Li, W. Bijker, A. Stein<div id=ref-id-h0300><strong class=title>Use of Binary Partition Tree and energy minimization for object-based classification of urban land cover</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 102 (2015), pp. 48-61</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000167 aria-describedby=ref-id-h0300>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000167/pdfft?md5=71df499b3971d9813400d1ec2d4c8161&amp;pid=1-s2.0-S0924271615000167-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84922350962&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0300>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Use%20of%20Binary%20Partition%20Tree%20and%20energy%20minimization%20for%20object-based%20classification%20of%20urban%20land%20cover&amp;publication_year=2015&amp;author=M.%20Li&amp;author=W.%20Bijker&amp;author=A.%20Stein" aria-describedby=ref-id-h0300>Google Scholar</a></div><dt class=label><a href=#bb0305 id=ref-id-b0305 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Li et al., 2015b</a><dd class=reference id=h0305><div class=contribution>X. Li, X. Cheng, W. Chen, G. Chen, S. Liu<div id=ref-id-h0305><strong class=title>Identification of forested landslides using LiDar data, object-based image analysis, and machine learning algorithms</strong></div></div><div class=host>Remote Sens., 7 (2015), pp. 9705-9726</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs70809705></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs70809705 aria-describedby=ref-id-h0305>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84939432156&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0305>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Identification%20of%20forested%20landslides%20using%20LiDar%20data%2C%20object-based%20image%20analysis%2C%20and%20machine%20learning%20algorithms&amp;publication_year=2015&amp;author=X.%20Li&amp;author=X.%20Cheng&amp;author=W.%20Chen&amp;author=G.%20Chen&amp;author=S.%20Liu" aria-describedby=ref-id-h0305>Google Scholar</a></div><dt class=label><a href=#bb0310 id=ref-id-b0310 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Li et al., 2016</a><dd class=reference id=h0310><div class=contribution>M. Li, L. Ma, T. Blaschke, L. Cheng, D. Tiede<div id=ref-id-h0310><strong class=title>A systematic comparison of different object-based classification techniques using high spatial resolution imagery in agricultural environments</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 49 (2016), pp. 87-98</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243416300125 aria-describedby=ref-id-h0310>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243416300125/pdfft?md5=5c00f13bbe3f6598bfa8769ceef0e9a5&amp;pid=1-s2.0-S0303243416300125-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-85017406334&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0310>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20systematic%20comparison%20of%20different%20object-based%20classification%20techniques%20using%20high%20spatial%20resolution%20imagery%20in%20agricultural%20environments&amp;publication_year=2016&amp;author=M.%20Li&amp;author=L.%20Ma&amp;author=T.%20Blaschke&amp;author=L.%20Cheng&amp;author=D.%20Tiede" aria-describedby=ref-id-h0310>Google Scholar</a></div><dt class=label><a href=#bb0315 id=ref-id-b0315 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Liu et al., 2006</a><dd class=reference id=h0315><div class=contribution>Y. Liu, L.I. Manchun, M. Liang, X.U. Feifei, S. Huang<div id=ref-id-h0315><strong class=title>Review of remotely sensed imagery classification patterns based on object-oriented image analysis</strong></div></div><div class=host>Chin. Geogr. Sci., 16 (2006), pp. 282-288</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/s11769-006-0282-0></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/s11769-006-0282-0 aria-describedby=ref-id-h0315>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33845417588&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0315>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Review%20of%20remotely%20sensed%20imagery%20classification%20patterns%20based%20on%20object-oriented%20image%20analysis&amp;publication_year=2006&amp;author=Y.%20Liu&amp;author=L.I.%20Manchun&amp;author=M.%20Liang&amp;author=X.U.%20Feifei&amp;author=S.%20Huang" aria-describedby=ref-id-h0315>Google Scholar</a></div><dt class=label><a href=#bb0320 id=ref-id-b0320 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Loosvelt et al., 2012</a><dd class=reference id=h0320><div class=contribution>L. Loosvelt, J. Peters, H. Skriver, H. Lievens, F.M.B.V. Coillie, B.D. Baets, N.E.C. Verhoest<div id=ref-id-h0320><strong class=title>Random Forests as a tool for estimating uncertainty at pixel-level in SAR image classification</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 19 (2012), pp. 173-184</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243412001195 aria-describedby=ref-id-h0320>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243412001195/pdfft?md5=0707fedea8204d823fa2579d67fe848f&amp;pid=1-s2.0-S0303243412001195-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84872776258&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0320>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Random%20Forests%20as%20a%20tool%20for%20estimating%20uncertainty%20at%20pixel-level%20in%20SAR%20image%20classification&amp;publication_year=2012&amp;author=L.%20Loosvelt&amp;author=J.%20Peters&amp;author=H.%20Skriver&amp;author=H.%20Lievens&amp;author=F.M.B.V.%20Coillie&amp;author=B.D.%20Baets&amp;author=N.E.C.%20Verhoest" aria-describedby=ref-id-h0320>Google Scholar</a></div><dt class=label><a href=#bb0325 id=ref-id-b0325 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ma et al., 2014</a><dd class=reference id=h0325><div class=contribution>L. Ma, L. Cheng, W. Han, L. Zhong, M. Li<div id=ref-id-h0325><strong class=title>Cultivated land information extraction from high-resolution unmanned aerial vehicle imagery data</strong></div></div><div class=host>J. Appl. Remote Sens., 8 (2014), pp. 1-25</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Cultivated%20land%20information%20extraction%20from%20high-resolution%20unmanned%20aerial%20vehicle%20imagery%20data&amp;publication_year=2014&amp;author=L.%20Ma&amp;author=L.%20Cheng&amp;author=W.%20Han&amp;author=L.%20Zhong&amp;author=M.%20Li" aria-describedby=ref-id-h0325>Google Scholar</a></div><dt class=label><a href=#bb0330 id=ref-id-b0330 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ma et al., 2015</a><dd class=reference id=h0330><div class=contribution>L. Ma, L. Cheng, M. Li, Y. Liu, X. Ma<div id=ref-id-h0330><strong class=title>Training set size, scale, and features in Geographic Object-Based Image Analysis of very high resolution unmanned aerial vehicle imagery</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 102 (2015), pp. 14-27</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000192 aria-describedby=ref-id-h0330>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615000192/pdfft?md5=cca441ae474b106aaed050d2b262d54c&amp;pid=1-s2.0-S0924271615000192-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Training%20set%20size%2C%20scale%2C%20and%20features%20in%20Geographic%20Object-Based%20Image%20Analysis%20of%20very%20high%20resolution%20unmanned%20aerial%20vehicle%20imagery&amp;publication_year=2015&amp;author=L.%20Ma&amp;author=L.%20Cheng&amp;author=M.%20Li&amp;author=Y.%20Liu&amp;author=X.%20Ma" aria-describedby=ref-id-h0330>Google Scholar</a></div><dt class=label><a href=#bb0335 id=ref-id-b0335 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ma et al., 2017</a><dd class=reference id=h0335><div class=contribution>L. Ma, T. Fu, T. Blaschke, M. Li, D. Tiede, Z. Zhou, X. Ma, D. Chen<div id=ref-id-h0335><strong class=title>Evaluation of feature selection methods for object-based land cover mapping of Unmanned Aerial Vehicle imagery using random forest and support vector machine classifiers</strong></div></div><div class=host>ISPRS Int. J. Geo-Inf., 6 (2017), p. 51</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/ijgi6020051></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/ijgi6020051 aria-describedby=ref-id-h0335>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Evaluation%20of%20feature%20selection%20methods%20for%20object-based%20land%20cover%20mapping%20of%20Unmanned%20Aerial%20Vehicle%20imagery%20using%20random%20forest%20and%20support%20vector%20machine%20classifiers&amp;publication_year=2017&amp;author=L.%20Ma&amp;author=T.%20Fu&amp;author=T.%20Blaschke&amp;author=M.%20Li&amp;author=D.%20Tiede&amp;author=Z.%20Zhou&amp;author=X.%20Ma&amp;author=D.%20Chen" aria-describedby=ref-id-h0335>Google Scholar</a></div><dt class=label><a href=#bb0340 id=ref-id-b0340 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Maclean and Congalton, 2012</a><dd class=reference id=h0340><span>Maclean, M.G., Congalton, R.G., 2012. Map accuracy assessment issues when using an object-oriented approach. In: ASPRS 2012 Annual Conference, Sacramento, CA, 19–23 March. &lt;<a href=http://info.asprs.org/publications/proceedings/Sacramento2012/files/MacLean.pdf target=_blank rel="noreferrer noopener">http://info.asprs.org/publications/proceedings/Sacramento2012/files/MacLean.pdf</a>&gt;.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Maclean,%20M.G.,%20Congalton,%20R.G.,%202012.%20Map%20accuracy%20assessment%20issues%20when%20using%20an%20object-oriented%20approach.%20In:%20ASPRS%202012%20Annual%20Conference,%20Sacramento,%20CA,%201923%20March.%20http:info.asprs.orgpublicationsproceedingsSacramento2012filesMacLean.pdf.">Google Scholar</a></div><dt class=label><a href=#bb0345 id=ref-id-b0345 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mallinis et al., 2008</a><dd class=reference id=h0345><div class=contribution>G. Mallinis, N. Koutsias, M. Tsakiri-Strati, M. Karteris<div id=ref-id-h0345><strong class=title>Object-based classification using Quickbird imagery for delineating forest vegetation polygons in a Mediterranean test site</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 63 (2008), pp. 237-250</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427160700086X aria-describedby=ref-id-h0345>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S092427160700086X/pdfft?md5=5ff9f92c9883b8cafe08ee8993477031&amp;pid=1-s2.0-S092427160700086X-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-39749162214&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0345>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20classification%20using%20Quickbird%20imagery%20for%20delineating%20forest%20vegetation%20polygons%20in%20a%20Mediterranean%20test%20site&amp;publication_year=2008&amp;author=G.%20Mallinis&amp;author=N.%20Koutsias&amp;author=M.%20Tsakiri-Strati&amp;author=M.%20Karteris" aria-describedby=ref-id-h0345>Google Scholar</a></div><dt class=label><a href=#bb0350 id=ref-id-b0350 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Martha et al., 2011</a><dd class=reference id=h0350><div class=contribution>T.R. Martha, N. Kerle, C.J. Van Westen, V. Jetten, K.V. Kumar<div id=ref-id-h0350><strong class=title>Segment optimization and data-driven thresholding for knowledge-based landslide detection by object-based image analysis</strong></div></div><div class=host>IEEE Trans. Geosci. Remote Sens., 49 (2011), pp. 4928-4943</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-82155192323></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-82155192323&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0350>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Segment%20optimization%20and%20data-driven%20thresholding%20for%20knowledge-based%20landslide%20detection%20by%20object-based%20image%20analysis&amp;publication_year=2011&amp;author=T.R.%20Martha&amp;author=N.%20Kerle&amp;author=C.J.%20Van%20Westen&amp;author=V.%20Jetten&amp;author=K.V.%20Kumar" aria-describedby=ref-id-h0350>Google Scholar</a></div><dt class=label><a href=#bb0355 id=ref-id-b0355 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Maxwell et al., 2015</a><dd class=reference id=h0355><div class=contribution>A.E. Maxwell, T.A. Warner, M.P. Strager, J.F. Conley, A.L. Sharp<div id=ref-id-h0355><strong class=title>Assessing machine-learning algorithms and image-and lidar-derived variables for GEOBIA classification of mining and mine reclamation</strong></div></div><div class=host>Int. J. Remote Sens., 36 (2015), pp. 954-978</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2014.1001086></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2014.1001086 aria-describedby=ref-id-h0355>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84923357893&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0355>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Assessing%20machine-learning%20algorithms%20and%20image-and%20lidar-derived%20variables%20for%20GEOBIA%20classification%20of%20mining%20and%20mine%20reclamation&amp;publication_year=2015&amp;author=A.E.%20Maxwell&amp;author=T.A.%20Warner&amp;author=M.P.%20Strager&amp;author=J.F.%20Conley&amp;author=A.L.%20Sharp" aria-describedby=ref-id-h0355>Google Scholar</a></div><dt class=label><a href=#bb0360 id=ref-id-b0360 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Moher et al., 2009</a><dd class=reference id=h0360><span>Moher, D., Liberati, A., Tetzlaff, J., Altman, D.G., Group, T.P., 2009. Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement. Ann. Int. Med. 151, 264–269.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Moher,%20D.,%20Liberati,%20A.,%20Tetzlaff,%20J.,%20Altman,%20D.G.,%20Group,%20T.P.,%202009.%20Preferred%20reporting%20items%20for%20systematic%20reviews%20and%20meta-analyses:%20the%20PRISMA%20statement.%20Ann.%20Int.%20Med.%20151,%20264269.">Google Scholar</a></div><dt class=label><a href=#bb0365 id=ref-id-b0365 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mohler and Goodin, 2012</a><dd class=reference id=h0365><div class=contribution>R.L. Mohler, D.G. Goodin<div id=ref-id-h0365><strong class=title>Identifying a suitable combination of classification technique and bandwidth(s) for burned area mapping in tallgrass prairie with MODIS imagery</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 14 (2012), pp. 103-111</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243411001164 aria-describedby=ref-id-h0365>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243411001164/pdfft?md5=936f3dcaae9dfe9ce16df3880310587c&amp;pid=1-s2.0-S0303243411001164-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84864411194&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0365>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Identifying%20a%20suitable%20combination%20of%20classification%20technique%20and%20bandwidth%20for%20burned%20area%20mapping%20in%20tallgrass%20prairie%20with%20MODIS%20imagery" aria-describedby=ref-id-h0365>Google Scholar</a></div><dt class=label><a href=#bb0370 id=ref-id-b0370 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Myint et al., 2011</a><dd class=reference id=h0370><div class=contribution>S.W. Myint, P. Gober, A. Brazel, S. Grossman-Clarke, Q. Weng<div id=ref-id-h0370><strong class=title>Per-pixel vs. object-based classification of urban land cover extraction using high spatial resolution imagery</strong></div></div><div class=host>Remote Sens. Environ., 115 (2011), pp. 1145-1161</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711000034 aria-describedby=ref-id-h0370>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711000034/pdfft?md5=ddbe76028628e7c16f07b70201f0fbae&amp;pid=1-s2.0-S0034425711000034-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Per-pixel%20vs.%20object-based%20classification%20of%20urban%20land%20cover%20extraction%20using%20high%20spatial%20resolution%20imagery&amp;publication_year=2011&amp;author=S.W.%20Myint&amp;author=P.%20Gober&amp;author=A.%20Brazel&amp;author=S.%20Grossman-Clarke&amp;author=Q.%20Weng" aria-describedby=ref-id-h0370>Google Scholar</a></div><dt class=label><a href=#bb0375 id=ref-id-b0375 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Novack et al., 2011</a><dd class=reference id=h0375><div class=contribution>T. Novack, T. Esch, H. Kux, U. Stilla<div id=ref-id-h0375><strong class=title>Machine learning comparison between WorldView-2 and QuickBird-2-simulated imagery regarding object-based urban land cover classification</strong></div></div><div class=host>Remote Sens., 3 (2011), pp. 2263-2282</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs3102263></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs3102263 aria-describedby=ref-id-h0375>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84856742245&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0375>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Machine%20learning%20comparison%20between%20WorldView-2%20and%20QuickBird-2-simulated%20imagery%20regarding%20object-based%20urban%20land%20cover%20classification&amp;publication_year=2011&amp;author=T.%20Novack&amp;author=T.%20Esch&amp;author=H.%20Kux&amp;author=U.%20Stilla" aria-describedby=ref-id-h0375>Google Scholar</a></div><dt class=label><a href=#bb0380 id=ref-id-b0380 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">O Connell et al., 2015</a><dd class=reference id=h0380><div class=contribution>J. O Connell, U. Bradter, T.G. Benton<div id=ref-id-h0380><strong class=title>Wide-area mapping of small-scale features in agricultural landscapes using airborne remote sensing</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 109 (2015), pp. 165-177</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615002087 aria-describedby=ref-id-h0380>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271615002087/pdfft?md5=8580d8ff52d06d512bb8f377d2a8e8d7&amp;pid=1-s2.0-S0924271615002087-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84943617428&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0380>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Wide-area%20mapping%20of%20small-scale%20features%20in%20agricultural%20landscapes%20using%20airborne%20remote%20sensing&amp;publication_year=2015&amp;author=J.%20O%20Connell&amp;author=U.%20Bradter&amp;author=T.G.%20Benton" aria-describedby=ref-id-h0380>Google Scholar</a></div><dt class=label><a href=#bb0385 id=ref-id-b0385 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Olofsson et al., 2013</a><dd class=reference id=h0385><div class=contribution>P. Olofsson, S.V. Stehman, C.E. Woodcock, G.M. Foody<div id=ref-id-h0385><strong class=title>Making better use of accuracy data in land change studies: estimating accuracy and area and quantifying uncertainty using stratified estimation</strong></div></div><div class=host>Remote Sens. Environ., 129 (2013), pp. 122-131</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712004191 aria-describedby=ref-id-h0385>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712004191/pdfft?md5=7761fcc76155d3c5ef24dbe395948d21&amp;pid=1-s2.0-S0034425712004191-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84870202759&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0385>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Making%20better%20use%20of%20accuracy%20data%20in%20land%20change%20studies%3A%20estimating%20accuracy%20and%20area%20and%20quantifying%20uncertainty%20using%20stratified%20estimation&amp;publication_year=2013&amp;author=P.%20Olofsson&amp;author=S.V.%20Stehman&amp;author=C.E.%20Woodcock&amp;author=G.M.%20Foody" aria-describedby=ref-id-h0385>Google Scholar</a></div><dt class=label><a href=#bb0390 id=ref-id-b0390 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pérez-Ortiz et al., 2016</a><dd class=reference id=h0390><div class=contribution>M. Pérez-Ortiz, J.M. Peña, P.A. Gutiérrez, J. Torres-Sánchez, C. Hervás-Martínez, F. López-Granados<div id=ref-id-h0390><strong class=title>Selecting patterns and features for between- and within- crop-row weed mapping using UAV-imagery</strong></div></div><div class=host>Expert Syst. Appl., 47 (2016), pp. 85-94</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0957417415007472 aria-describedby=ref-id-h0390>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0957417415007472/pdfft?md5=636d40c318455dbfad483e113061387d&amp;pid=1-s2.0-S0957417415007472-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84949520283&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0390>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Selecting%20patterns%20and%20features%20for%20between-%20and%20within-%20crop-row%20weed%20mapping%20using%20UAV-imagery&amp;publication_year=2016&amp;author=M.%20P%C3%A9rez-Ortiz&amp;author=J.M.%20Pe%C3%B1a&amp;author=P.A.%20Guti%C3%A9rrez&amp;author=J.%20Torres-S%C3%A1nchez&amp;author=C.%20Herv%C3%A1s-Mart%C3%ADnez&amp;author=F.%20L%C3%B3pez-Granados" aria-describedby=ref-id-h0390>Google Scholar</a></div><dt class=label><a href=#bb0395 id=ref-id-b0395 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pal and Foody, 2010</a><dd class=reference id=h0395><div class=contribution>M. Pal, G.M. Foody<div id=ref-id-h0395><strong class=title>Feature selection for classification of hyperspectral data by SVM</strong></div></div><div class=host>IEEE Trans. Geosci. Remote Sens., 48 (2010), pp. 2297-2307</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-77951295936></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-77951295936&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0395>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Feature%20selection%20for%20classification%20of%20hyperspectral%20data%20by%20SVM&amp;publication_year=2010&amp;author=M.%20Pal&amp;author=G.M.%20Foody" aria-describedby=ref-id-h0395>Google Scholar</a></div><dt class=label><a href=#bb0400 id=ref-id-b0400 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pal and Mather, 2006</a><dd class=reference id=h0400><div class=contribution>M. Pal, P.M. Mather<div id=ref-id-h0400><strong class=title>Some issues in the classification of DAIS hyperspectral data</strong></div></div><div class=host>Int. J. Remote Sens., 105 (2006), pp. 2895-2916</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160500185227></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160500185227 aria-describedby=ref-id-h0400>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33747086525&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0400>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Some%20issues%20in%20the%20classification%20of%20DAIS%20hyperspectral%20data&amp;publication_year=2006&amp;author=M.%20Pal&amp;author=P.M.%20Mather" aria-describedby=ref-id-h0400>Google Scholar</a></div><dt class=label><a href=#bb0405 id=ref-id-b0405 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Peña-Barragán et al., 2011</a><dd class=reference id=h0405><div class=contribution>J.M. Peña-Barragán, M.K. Ngugi, R.E. Plant, J. Six<div id=ref-id-h0405><strong class=title>Object-based crop identification using multiple vegetation indices, textural features and crop phenology</strong></div></div><div class=host>Remote Sens. Environ., 115 (2011), pp. 1301-1316</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711000290 aria-describedby=ref-id-h0405>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711000290/pdfft?md5=0c919c3c6645cfcdd33eeb48bec0071a&amp;pid=1-s2.0-S0034425711000290-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79953182966&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0405>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20crop%20identification%20using%20multiple%20vegetation%20indices%2C%20textural%20features%20and%20crop%20phenology&amp;publication_year=2011&amp;author=J.M.%20Pe%C3%B1a-Barrag%C3%A1n&amp;author=M.K.%20Ngugi&amp;author=R.E.%20Plant&amp;author=J.%20Six" aria-describedby=ref-id-h0405>Google Scholar</a></div><dt class=label><a href=#bb0410 id=ref-id-b0410 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Peña et al., 2014</a><dd class=reference id=h0410><div class=contribution>J. Peña, P. Gutiérrez, C. Hervásmartínez, J. Six, R. Plant, F. Lópezgranados<div id=ref-id-h0410><strong class=title>Object-based image classification of summer crops with machine learning methods</strong></div></div><div class=host>Remote Sens., 6 (2014), pp. 5019-5041</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs6065019></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs6065019 aria-describedby=ref-id-h0410>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84908572688&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0410>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20image%20classification%20of%20summer%20crops%20with%20machine%20learning%20methods&amp;publication_year=2014&amp;author=J.%20Pe%C3%B1a&amp;author=P.%20Guti%C3%A9rrez&amp;author=C.%20Herv%C3%A1smart%C3%ADnez&amp;author=J.%20Six&amp;author=R.%20Plant&amp;author=F.%20L%C3%B3pezgranados" aria-describedby=ref-id-h0410>Google Scholar</a></div><dt class=label><a href=#bb0415 id=ref-id-b0415 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pereira Júnior et al., 2014</a><dd class=reference id=h0415><div class=contribution>A.C. Pereira Júnior, S.L. Oliveira, J.M. Pereira, M.A. Turkman<div id=ref-id-h0415><strong class=title>Modelling fire frequency in a cerrado savanna protected area</strong></div></div><div class=host>PLoS ONE, 9 (7) (2014), p. e102380</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1371/journal.pone.0102380></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1371/journal.pone.0102380 aria-describedby=ref-id-h0415>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Modelling%20fire%20frequency%20in%20a%20cerrado%20savanna%20protected%20area&amp;publication_year=2014&amp;author=A.C.%20Pereira%20J%C3%BAnior&amp;author=S.L.%20Oliveira&amp;author=J.M.%20Pereira&amp;author=M.A.%20Turkman" aria-describedby=ref-id-h0415>Google Scholar</a></div><dt class=label><a href=#bb0420 id=ref-id-b0420 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Powers et al., 2012</a><dd class=reference id=h0420><div class=contribution>R.P. Powers, G.J. Hay, G. Chen<div id=ref-id-h0420><strong class=title>How wetland type and area differ through scale: a GEOBIA case study in Alberta's Boreal Plains</strong></div></div><div class=host>Remote Sens. Environ., 117 (2012), pp. 135-145</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002537 aria-describedby=ref-id-h0420>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002537/pdfft?md5=ff272822bc79a4843404f19265d2e2de&amp;pid=1-s2.0-S0034425711002537-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84855442605&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0420>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=How%20wetland%20type%20and%20area%20differ%20through%20scale:%20a%20GEOBIA%20case%20study%20in%20Albertas%20Boreal%20Plains" aria-describedby=ref-id-h0420>Google Scholar</a></div><dt class=label><a href=#bb0425 id=ref-id-b0425 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pu and Landry, 2012</a><dd class=reference id=h0425><div class=contribution>R. Pu, S. Landry<div id=ref-id-h0425><strong class=title>A comparative analysis of high spatial resolution IKONOS and WorldView-2 imagery for mapping urban tree species</strong></div></div><div class=host>Remote Sens. Environ., 124 (2012), pp. 516-533</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712002477 aria-describedby=ref-id-h0425>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712002477/pdfft?md5=3c6a7be27edab4e7a5738fdc0d99bf87&amp;pid=1-s2.0-S0034425712002477-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84863422450&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0425>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20comparative%20analysis%20of%20high%20spatial%20resolution%20IKONOS%20and%20WorldView-2%20imagery%20for%20mapping%20urban%20tree%20species&amp;publication_year=2012&amp;author=R.%20Pu&amp;author=S.%20Landry" aria-describedby=ref-id-h0425>Google Scholar</a></div><dt class=label><a href=#bb0430 id=ref-id-b0430 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Puissant et al., 2014</a><dd class=reference id=h0430><div class=contribution>A. Puissant, S. Rougier, A. Stumpf<div id=ref-id-h0430><strong class=title>Object-oriented mapping of urban trees using Random Forest classifiers</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 26 (2014), pp. 235-245</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413000822 aria-describedby=ref-id-h0430>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413000822/pdfft?md5=5256d6262415995ff9116f55dd02e293&amp;pid=1-s2.0-S0303243413000822-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84897879307&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0430>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-oriented%20mapping%20of%20urban%20trees%20using%20Random%20Forest%20classifiers&amp;publication_year=2014&amp;author=A.%20Puissant&amp;author=S.%20Rougier&amp;author=A.%20Stumpf" aria-describedby=ref-id-h0430>Google Scholar</a></div><dt class=label><a href=#bb0435 id=ref-id-b0435 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Qi et al., 2012</a><dd class=reference id=h0435><div class=contribution>Z. Qi, G.O. Yeh, X. Li, Z. Lin<div id=ref-id-h0435><strong class=title>A novel algorithm for land use and land cover classification using RADARSAT-2 polarimetric SAR data</strong></div></div><div class=host>Remote Sens. Environ., 118 (2012), pp. 21-39</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711003877 aria-describedby=ref-id-h0435>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711003877/pdfft?md5=9ff24ea18a3c924178c159100062a632&amp;pid=1-s2.0-S0034425711003877-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-82655178425&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0435>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20novel%20algorithm%20for%20land%20use%20and%20land%20cover%20classification%20using%20RADARSAT-2%20polarimetric%20SAR%20data&amp;publication_year=2012&amp;author=Z.%20Qi&amp;author=G.O.%20Yeh&amp;author=X.%20Li&amp;author=Z.%20Lin" aria-describedby=ref-id-h0435>Google Scholar</a></div><dt class=label><a href=#bb0440 id=ref-id-b0440 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Qi et al., 2015</a><dd class=reference id=h0440><div class=contribution>Z. Qi, G.O. Yeh, X. Li, S. Xian, X. Zhang<div id=ref-id-h0440><strong class=title>Monthly short-term detection of land development using RADARSAT-2 polarimetric SAR imagery</strong></div></div><div class=host>Remote Sens. Environ., 164 (2015), pp. 179-196</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715001601 aria-describedby=ref-id-h0440>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715001601/pdfft?md5=0538388ae52fdd7b102c891c7332762e&amp;pid=1-s2.0-S0034425715001601-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84951910279&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0440>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Monthly%20short-term%20detection%20of%20land%20development%20using%20RADARSAT-2%20polarimetric%20SAR%20imagery&amp;publication_year=2015&amp;author=Z.%20Qi&amp;author=G.O.%20Yeh&amp;author=X.%20Li&amp;author=S.%20Xian&amp;author=X.%20Zhang" aria-describedby=ref-id-h0440>Google Scholar</a></div><dt class=label><a href=#bb0445 id=ref-id-b0445 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Qian et al., 2014</a><dd class=reference id=h0445><div class=contribution>Y. Qian, W. Zhou, J. Yan, W. Li, L. Han<div id=ref-id-h0445><strong class=title>Comparing machine learning classifiers for object-based land cover classification using very high resolution imagery</strong></div></div><div class=host>Remote Sens., 7 (2014), pp. 153-168</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Comparing%20machine%20learning%20classifiers%20for%20object-based%20land%20cover%20classification%20using%20very%20high%20resolution%20imagery&amp;publication_year=2014&amp;author=Y.%20Qian&amp;author=W.%20Zhou&amp;author=J.%20Yan&amp;author=W.%20Li&amp;author=L.%20Han" aria-describedby=ref-id-h0445>Google Scholar</a></div><dt class=label><a href=#bb0450 id=ref-id-b0450 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Räsänen et al., 2013</a><dd class=reference id=h0450><div class=contribution>A. Räsänen, A. Rusanen, M. Kuitunen, A. Lensu<div id=ref-id-h0450><strong class=title>What makes segmentation good? A case study in boreal forest habitat mapping</strong></div></div><div class=host>Int. J. Remote Sens., 34 (2013), pp. 8603-8627</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2013.845318></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2013.845318 aria-describedby=ref-id-h0450>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84886930214&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0450>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=What%20makes%20segmentation%20good%20A%20case%20study%20in%20boreal%20forest%20habitat%20mapping" aria-describedby=ref-id-h0450>Google Scholar</a></div><dt class=label><a href=#bb0455 id=ref-id-b0455 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Radoux and Bogaert, 2014</a><dd class=reference id=h0455><div class=contribution>J. Radoux, P. Bogaert<div id=ref-id-h0455><strong class=title>Accounting for the area of polygon sampling units for the prediction of primary accuracy assessment indices</strong></div></div><div class=host>Remote Sens. Environ., 142 (2014), pp. 9-19</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425713004070 aria-describedby=ref-id-h0455>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425713004070/pdfft?md5=5d386f7b34273dcbe4aa10ef92a9dd8c&amp;pid=1-s2.0-S0034425713004070-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84889598340&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0455>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Accounting%20for%20the%20area%20of%20polygon%20sampling%20units%20for%20the%20prediction%20of%20primary%20accuracy%20assessment%20indices&amp;publication_year=2014&amp;author=J.%20Radoux&amp;author=P.%20Bogaert" aria-describedby=ref-id-h0455>Google Scholar</a></div><dt class=label><a href=#bb0460 id=ref-id-b0460 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Radoux et al., 2011</a><dd class=reference id=h0460><div class=contribution>J. Radoux, P. Bogaert, D. Fasbender, P. Defourny<div id=ref-id-h0460><strong class=title>Thematic accuracy assessment of geographic object-based image classification</strong></div></div><div class=host>Int. J. Geogr. Inf. Sci., 25 (2011), pp. 895-911</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/13658816.2010.498378></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/13658816.2010.498378 aria-describedby=ref-id-h0460>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79960692760&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0460>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Thematic%20accuracy%20assessment%20of%20geographic%20object-based%20image%20classification&amp;publication_year=2011&amp;author=J.%20Radoux&amp;author=P.%20Bogaert&amp;author=D.%20Fasbender&amp;author=P.%20Defourny" aria-describedby=ref-id-h0460>Google Scholar</a></div><dt class=label><a href=#bb0465 id=ref-id-b0465 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Recio et al., 2013</a><dd class=reference id=h0465><div class=contribution>M.R. Recio, R. Mathieu, G.B. Hall, A.B. Moore, P.J. Seddon<div id=ref-id-h0465><strong class=title>Landscape resource mapping for wildlife research using very high resolution satellite imagery</strong></div></div><div class=host>Methods Ecol. Evol., 4 (2013), pp. 982-992</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-84885601486></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84885601486&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0465>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Landscape%20resource%20mapping%20for%20wildlife%20research%20using%20very%20high%20resolution%20satellite%20imagery&amp;publication_year=2013&amp;author=M.R.%20Recio&amp;author=R.%20Mathieu&amp;author=G.B.%20Hall&amp;author=A.B.%20Moore&amp;author=P.J.%20Seddon" aria-describedby=ref-id-h0465>Google Scholar</a></div><dt class=label><a href=#bb0470 id=ref-id-b0470 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Rougier et al., 2016</a><dd class=reference id=h0470><div class=contribution>S. Rougier, A. Puissant, A. Stumpf, N. Lachiche<div id=ref-id-h0470><strong class=title>Comparison of sampling strategies for object-based classification of urban vegetation from Very High Resolution satellite images</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 51 (2016), pp. 60-73</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243416300642 aria-describedby=ref-id-h0470>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243416300642/pdfft?md5=4066071827ff35488030cd4691c04942&amp;pid=1-s2.0-S0303243416300642-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84997523905&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0470>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Comparison%20of%20sampling%20strategies%20for%20object-based%20classification%20of%20urban%20vegetation%20from%20Very%20High%20Resolution%20satellite%20images&amp;publication_year=2016&amp;author=S.%20Rougier&amp;author=A.%20Puissant&amp;author=A.%20Stumpf&amp;author=N.%20Lachiche" aria-describedby=ref-id-h0470>Google Scholar</a></div><dt class=label><a href=#bb0475 id=ref-id-b0475 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Samat et al., 2015</a><dd class=reference id=h0475><div class=contribution>A. Samat, J. Li, S. Liu, P. Du, Z. Miao, J. Luo<div id=ref-id-h0475><strong class=title>Improved hyperspectral image classification by active learning using pre-designed mixed pixels</strong></div></div><div class=host>Pattern Recognit., 51 (2015), pp. 43-58</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Improved%20hyperspectral%20image%20classification%20by%20active%20learning%20using%20pre-designed%20mixed%20pixels&amp;publication_year=2015&amp;author=A.%20Samat&amp;author=J.%20Li&amp;author=S.%20Liu&amp;author=P.%20Du&amp;author=Z.%20Miao&amp;author=J.%20Luo" aria-describedby=ref-id-h0475>Google Scholar</a></div><dt class=label><a href=#bb0480 id=ref-id-b0480 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Schultz et al., 2015</a><dd class=reference id=h0480><div class=contribution>B. Schultz, M. Immitzer, A. Formaggio, I. Sanches, A. Luiz, C. Atzberger<div id=ref-id-h0480><strong class=title>Self-guided segmentation and classification of multi-temporal Landsat 8 images for crop type mapping in Southeastern Brazil</strong></div></div><div class=host>Remote Sens., 7 (2015), pp. 14482-14508</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs71114482></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs71114482 aria-describedby=ref-id-h0480>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84950112053&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0480>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Self-guided%20segmentation%20and%20classification%20of%20multi-temporal%20Landsat%208%20images%20for%20crop%20type%20mapping%20in%20Southeastern%20Brazil&amp;publication_year=2015&amp;author=B.%20Schultz&amp;author=M.%20Immitzer&amp;author=A.%20Formaggio&amp;author=I.%20Sanches&amp;author=A.%20Luiz&amp;author=C.%20Atzberger" aria-describedby=ref-id-h0480>Google Scholar</a></div><dt class=label><a href=#bb0485 id=ref-id-b0485 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Seto et al., 2011</a><dd class=reference id=h0485><div class=contribution>K.C. Seto, F. Michail, G. Burak, M.K. Reilly<div id=ref-id-h0485><strong class=title>A meta-analysis of global urban land expansion</strong></div></div><div class=host>PLoS ONE, 6 (2011), p. e23777</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1371/journal.pone.0023777></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1371/journal.pone.0023777 aria-describedby=ref-id-h0485>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84865296592&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0485>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20meta-analysis%20of%20global%20urban%20land%20expansion&amp;publication_year=2011&amp;author=K.C.%20Seto&amp;author=F.%20Michail&amp;author=G.%20Burak&amp;author=M.K.%20Reilly" aria-describedby=ref-id-h0485>Google Scholar</a></div><dt class=label><a href=#bb0490 id=ref-id-b0490 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Smith, 2010</a><dd class=reference id=h0490><div class=contribution>A. Smith<div id=ref-id-h0490><strong class=title>Image segmentation scale parameter optimization and land cover classification using the Random Forest algorithm</strong></div></div><div class=host>Spat. Sci., 55 (2010), pp. 69-79</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/14498596.2010.487851></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/14498596.2010.487851 aria-describedby=ref-id-h0490>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Image%20segmentation%20scale%20parameter%20optimization%20and%20land%20cover%20classification%20using%20the%20Random%20Forest%20algorithm&amp;publication_year=2010&amp;author=A.%20Smith" aria-describedby=ref-id-h0490>Google Scholar</a></div><dt class=label><a href=#bb0495 id=ref-id-b0495 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Son et al., 2015</a><dd class=reference id=h0495><div class=contribution>N.T. Son, C.F. Chen, N.B. Chang, C.R. Chen<div id=ref-id-h0495><strong class=title>Mangrove mapping and change detection in Ca Mau Peninsula, Vietnam, using Landsat data and object-based image analysis</strong></div></div><div class=host>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens., 8 (2015), pp. 503-510</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Mangrove%20mapping%20and%20change%20detection%20in%20Ca%20Mau%20Peninsula%2C%20Vietnam%2C%20using%20Landsat%20data%20and%20object-based%20image%20analysis&amp;publication_year=2015&amp;author=N.T.%20Son&amp;author=C.F.%20Chen&amp;author=N.B.%20Chang&amp;author=C.R.%20Chen" aria-describedby=ref-id-h0495>Google Scholar</a></div><dt class=label><a href=#bb0500 id=ref-id-b0500 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Stehman and Wickham, 2011</a><dd class=reference id=h0500><div class=contribution>S.V. Stehman, J.D. Wickham<div id=ref-id-h0500><strong class=title>Pixels, blocks of pixels, and polygons: choosing a spatial unit for thematic accuracy assessment</strong></div></div><div class=host>Remote Sens. Environ., 115 (2011), pp. 3044-3055</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002318 aria-describedby=ref-id-h0500>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711002318/pdfft?md5=ca4cbfda6109b55b7704586964c026d7&amp;pid=1-s2.0-S0034425711002318-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-81355138730&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0500>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Pixels%2C%20blocks%20of%20pixels%2C%20and%20polygons%3A%20choosing%20a%20spatial%20unit%20for%20thematic%20accuracy%20assessment&amp;publication_year=2011&amp;author=S.V.%20Stehman&amp;author=J.D.%20Wickham" aria-describedby=ref-id-h0500>Google Scholar</a></div><dt class=label><a href=#bb0505 id=ref-id-b0505 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Stumpf and Kerle, 2011</a><dd class=reference id=h0505><div class=contribution>A. Stumpf, N. Kerle<div id=ref-id-h0505><strong class=title>Object-oriented mapping of landslides using Random Forests</strong></div></div><div class=host>Remote Sens. Environ., 115 (2011), pp. 2564-2577</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711001969 aria-describedby=ref-id-h0505>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425711001969/pdfft?md5=0e1d26ae72d1e1114d61b8829e9c9ffa&amp;pid=1-s2.0-S0034425711001969-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79960743609&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0505>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-oriented%20mapping%20of%20landslides%20using%20Random%20Forests&amp;publication_year=2011&amp;author=A.%20Stumpf&amp;author=N.%20Kerle" aria-describedby=ref-id-h0505>Google Scholar</a></div><dt class=label><a href=#bb0510 id=ref-id-b0510 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Stumpf et al., 2014</a><dd class=reference id=h0510><div class=contribution>A. Stumpf, N. Lachiche, J.P. Malet, N. Kerle, A. Puissant<div id=ref-id-h0510><strong class=title>Active learning in the spatial domain for remote sensing image classification</strong></div></div><div class=host>IEEE Trans. Geosci. Remote Sens., 52 (2014), pp. 2492-2507</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-84898598578></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84898598578&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0510>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Active%20learning%20in%20the%20spatial%20domain%20for%20remote%20sensing%20image%20classification&amp;publication_year=2014&amp;author=A.%20Stumpf&amp;author=N.%20Lachiche&amp;author=J.P.%20Malet&amp;author=N.%20Kerle&amp;author=A.%20Puissant" aria-describedby=ref-id-h0510>Google Scholar</a></div><dt class=label><a href=#bb0515 id=ref-id-b0515 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tehrany et al., 2014</a><dd class=reference id=h0515><div class=contribution>M.S. Tehrany, B. Pradhan, M.N. Jebuv<div id=ref-id-h0515><strong class=title>A comparative assessment between object and pixel-based classification approaches for land use/land cover mapping using SPOT 5 imagery</strong></div></div><div class=host>Geocarto Int., 29 (2014), pp. 351-369</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/10106049.2013.768300></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/10106049.2013.768300 aria-describedby=ref-id-h0515>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84903538782&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0515>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=A%20comparative%20assessment%20between%20object%20and%20pixel-based%20classification%20approaches%20for%20land%20useland%20cover%20mapping%20using%20SPOT%205%20imagery" aria-describedby=ref-id-h0515>Google Scholar</a></div><dt class=label><a href=#bb0520 id=ref-id-b0520 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tuia et al., 2011</a><dd class=reference id=h0520><div class=contribution>D. Tuia, M. Volpi, L. Copa, M. Kanevski, J. Muñoz-Marí<div id=ref-id-h0520><strong class=title>A survey of active learning algorithms for supervised remote sensing image classification</strong></div></div><div class=host>IEEE J. Sel. Top. Signal Process., 5 (2011), pp. 606-617</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-79957456032></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-79957456032&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0520>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20survey%20of%20active%20learning%20algorithms%20for%20supervised%20remote%20sensing%20image%20classification&amp;publication_year=2011&amp;author=D.%20Tuia&amp;author=M.%20Volpi&amp;author=L.%20Copa&amp;author=M.%20Kanevski&amp;author=J.%20Mu%C3%B1oz-Mar%C3%AD" aria-describedby=ref-id-h0520>Google Scholar</a></div><dt class=label><a href=#bb0525 id=ref-id-b0525 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Verbeeck et al., 2012</a><dd class=reference id=h0525><div class=contribution>K. Verbeeck, M. Hermy, J.V. Orshoven<div id=ref-id-h0525><strong class=title>External geo-information in the segmentation of VHR imagery improves the detection of imperviousness in urban neighborhoods</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 18 (2012), pp. 428-435</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243412000591 aria-describedby=ref-id-h0525>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243412000591/pdfft?md5=543fb22844e175ed35857b4aeb6fb216&amp;pid=1-s2.0-S0303243412000591-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84864502305&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0525>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=External%20geo-information%20in%20the%20segmentation%20of%20VHR%20imagery%20improves%20the%20detection%20of%20imperviousness%20in%20urban%20neighborhoods&amp;publication_year=2012&amp;author=K.%20Verbeeck&amp;author=M.%20Hermy&amp;author=J.V.%20Orshoven" aria-describedby=ref-id-h0525>Google Scholar</a></div><dt class=label><a href=#bb0530 id=ref-id-b0530 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Vieira et al., 2012</a><dd class=reference id=h0530><div class=contribution>M.A. Vieira, A.R. Formaggio, C.D. Rennó, C. Atzberger, D.A. Aguiar, M.P. Mello<div id=ref-id-h0530><strong class=title>Object Based Image Analysis and Data Mining applied to a remotely sensed Landsat time-series to map sugarcane over large areas</strong></div></div><div class=host>Remote Sens. Environ., 123 (2012), pp. 553-562</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712001800 aria-describedby=ref-id-h0530>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712001800/pdfft?md5=7dfb53a5760ed3e884167b1be3211172&amp;pid=1-s2.0-S0034425712001800-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84861335868&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0530>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object%20Based%20Image%20Analysis%20and%20Data%20Mining%20applied%20to%20a%20remotely%20sensed%20Landsat%20time-series%20to%20map%20sugarcane%20over%20large%20areas&amp;publication_year=2012&amp;author=M.A.%20Vieira&amp;author=A.R.%20Formaggio&amp;author=C.D.%20Renn%C3%B3&amp;author=C.%20Atzberger&amp;author=D.A.%20Aguiar&amp;author=M.P.%20Mello" aria-describedby=ref-id-h0530>Google Scholar</a></div><dt class=label><a href=#bb0535 id=ref-id-b0535 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Walter, 2004</a><dd class=reference id=h0535><div class=contribution>V. Walter<div id=ref-id-h0535><strong class=title>Object-based classification of remote sensing data for change detection</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 58 (2004), pp. 225-238</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271603000595 aria-describedby=ref-id-h0535>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271603000595/pdfft?md5=9ba760d4746a8737291f587ad1dc826a&amp;pid=1-s2.0-S0924271603000595-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-1642528357&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0535>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20classification%20of%20remote%20sensing%20data%20for%20change%20detection&amp;publication_year=2004&amp;author=V.%20Walter" aria-describedby=ref-id-h0535>Google Scholar</a></div><dt class=label><a href=#bb0540 id=ref-id-b0540 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Walker and Blaschke, 2008</a><dd class=reference id=h0540><div class=contribution>J.S. Walker, T. Blaschke<div id=ref-id-h0540><strong class=title>Object-based land-cover classification for the Phoenix metropolitan area: optimization vs. transportability</strong></div></div><div class=host>Int. J. Remote Sens., 29 (2008), pp. 2021-2040</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160701408337></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160701408337 aria-describedby=ref-id-h0540>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-41549153831&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0540>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20land-cover%20classification%20for%20the%20Phoenix%20metropolitan%20area%3A%20optimization%20vs.%20transportability&amp;publication_year=2008&amp;author=J.S.%20Walker&amp;author=T.%20Blaschke" aria-describedby=ref-id-h0540>Google Scholar</a></div><dt class=label><a href=#bb0545 id=ref-id-b0545 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wang et al., 2004</a><dd class=reference id=h0545><div class=contribution>L. Wang, W.P. Sousa, P. Gong<div id=ref-id-h0545><strong class=title>Integration of object-based and pixel-based classification for mapping mangroves with IKONOS imagery</strong></div></div><div class=host>Int. J. Remote Sens., 25 (2004), pp. 5655-5668</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-10844220846></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-10844220846&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0545>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Integration%20of%20object-based%20and%20pixel-based%20classification%20for%20mapping%20mangroves%20with%20IKONOS%20imagery&amp;publication_year=2004&amp;author=L.%20Wang&amp;author=W.P.%20Sousa&amp;author=P.%20Gong" aria-describedby=ref-id-h0545>Google Scholar</a></div><dt class=label><a href=#bb0550 id=ref-id-b0550 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Weston et al., 2000</a><dd class=reference id=h0550><div class=contribution>J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, V. Vapnik<div id=ref-id-h0550><strong class=title>Feature selection for SVMs</strong></div></div><div class=host>Adv. Neural. Inf. Process. Syst., 13 (2000), pp. 668-674</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-33645427310></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33645427310&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0550>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Feature%20selection%20for%20SVMs&amp;publication_year=2000&amp;author=J.%20Weston&amp;author=S.%20Mukherjee&amp;author=O.%20Chapelle&amp;author=M.%20Pontil&amp;author=T.%20Poggio&amp;author=V.%20Vapnik" aria-describedby=ref-id-h0550>Google Scholar</a></div><dt class=label><a href=#bb0555 id=ref-id-b0555 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Whiteside et al., 2014</a><dd class=reference id=h0555><div class=contribution>T.G. Whiteside, S.W. Maier, G.S. Boggs<div id=ref-id-h0555><strong class=title>Area-based and location-based validation of classified image objects</strong></div></div><div class=host>Int. J. Appl. Earth Obs. Geoinf., 28 (2014), pp. 117-130</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413001621 aria-describedby=ref-id-h0555>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243413001621/pdfft?md5=fd8abca9421f68ab83055941fd316fc3&amp;pid=1-s2.0-S0303243413001621-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84897470278&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0555>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Area-based%20and%20location-based%20validation%20of%20classified%20image%20objects&amp;publication_year=2014&amp;author=T.G.%20Whiteside&amp;author=S.W.%20Maier&amp;author=G.S.%20Boggs" aria-describedby=ref-id-h0555>Google Scholar</a></div><dt class=label><a href=#bb0560 id=ref-id-b0560 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wieland and Pittore, 2014</a><dd class=reference id=h0560><div class=contribution>M. Wieland, M. Pittore<div id=ref-id-h0560><strong class=title>Performance evaluation of machine learning algorithms for urban pattern recognition from multi-spectral satellite images</strong></div></div><div class=host>Remote Sens., 6 (2014), pp. 2912-2939</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.3390/rs6042912></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.3390/rs6042912 aria-describedby=ref-id-h0560>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84898072545&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0560>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Performance%20evaluation%20of%20machine%20learning%20algorithms%20for%20urban%20pattern%20recognition%20from%20multi-spectral%20satellite%20images&amp;publication_year=2014&amp;author=M.%20Wieland&amp;author=M.%20Pittore" aria-describedby=ref-id-h0560>Google Scholar</a></div><dt class=label><a href=#bb0565 id=ref-id-b0565 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Witharana and Civco, 2014</a><dd class=reference id=h0565><div class=contribution>C. Witharana, D.L. Civco<div id=ref-id-h0565><strong class=title>Optimizing multi-resolution segmentation scale using empirical methods: exploring the sensitivity of the supervised discrepancy measure Euclidean distance 2 (ED2)</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 87 (2014), pp. 108-121</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002669 aria-describedby=ref-id-h0565>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271613002669/pdfft?md5=a41c969dc9eaf0e5d251cbd7a1b9f24f&amp;pid=1-s2.0-S0924271613002669-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84896962668&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0565>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Optimizing%20multi-resolution%20segmentation%20scale%20using%20empirical%20methods:%20exploring%20the%20sensitivity%20of%20the%20supervised%20discrepancy%20measure%20Euclidean%20distance%202" aria-describedby=ref-id-h0565>Google Scholar</a></div><dt class=label><a href=#bb0570 id=ref-id-b0570 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Xu et al., 2010</a><dd class=reference id=h0570><div class=contribution>H.Q. Xu, P.J. Li, Y. Zhang, J.F. Wang, N.C. Coops<div id=ref-id-h0570><strong class=title>Urban land cover classification from very high resolution imagery using spectral and invariant moment shape information</strong></div></div><div class=host>Can. J. Remote. Sens., 36 (2010), pp. 248-260</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.5589/m10-042></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.5589/m10-042 aria-describedby=ref-id-h0570>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-78649988569&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0570>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Urban%20land%20cover%20classification%20from%20very%20high%20resolution%20imagery%20using%20spectral%20and%20invariant%20moment%20shape%20information&amp;publication_year=2010&amp;author=H.Q.%20Xu&amp;author=P.J.%20Li&amp;author=Y.%20Zhang&amp;author=J.F.%20Wang&amp;author=N.C.%20Coops" aria-describedby=ref-id-h0570>Google Scholar</a></div><dt class=label><a href=#bb0575 id=ref-id-b0575 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Yan et al., 2006</a><dd class=reference id=h0575><div class=contribution>G. Yan, J.F. Mas, B.H.P. Maathuis, Z. Xiangmin, P.M.V. Dijk<div id=ref-id-h0575><strong class=title>Comparison of pixel-based and object-oriented image classification approaches – a case study in a coal fire area, Wuda, Inner Mongolia, China</strong></div></div><div class=host>Int. J. Remote Sens., 27 (2006), pp. 4039-4055</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160600702632></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160600702632 aria-describedby=ref-id-h0575>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33750084387&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0575>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Comparison%20of%20pixel-based%20and%20object-oriented%20image%20classification%20approaches%20%20a%20case%20study%20in%20a%20coal%20fire%20area,%20Wuda,%20Inner%20Mongolia,%20China" aria-describedby=ref-id-h0575>Google Scholar</a></div><dt class=label><a href=#bb0580 id=ref-id-b0580 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Yu et al., 2006</a><dd class=reference id=h0580><div class=contribution>Q. Yu, P. Gong, N. Clinton, G. Biging, M. Kelly, D. Schirokauer<div id=ref-id-h0580><strong class=title>Object-based detailed vegetation classification with airborne high spatial resolution remote sensing imagery</strong></div></div><div class=host>Photogramm. Eng. Remote Sens., 72 (2006), pp. 799-811</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-33745615125></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-33745615125&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0580>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Object-based%20detailed%20vegetation%20classification%20with%20airborne%20high%20spatial%20resolution%20remote%20sensing%20imagery&amp;publication_year=2006&amp;author=Q.%20Yu&amp;author=P.%20Gong&amp;author=N.%20Clinton&amp;author=G.%20Biging&amp;author=M.%20Kelly&amp;author=D.%20Schirokauer" aria-describedby=ref-id-h0580>Google Scholar</a></div><dt class=label><a href=#bb0585 id=ref-id-b0585 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhan et al., 2005</a><dd class=reference id=h0585><div class=contribution>Q. Zhan, M. Molenaar, K. Tempfli, W. Shi<div id=ref-id-h0585><strong class=title>Quality assessment for geo-spatial objects derived from remotely sensed data</strong></div></div><div class=host>Int. J. Remote Sens., 26 (2005), pp. 2953-2974</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160500057764></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160500057764 aria-describedby=ref-id-h0585>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-23844440634&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0585>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Quality%20assessment%20for%20geo-spatial%20objects%20derived%20from%20remotely%20sensed%20data&amp;publication_year=2005&amp;author=Q.%20Zhan&amp;author=M.%20Molenaar&amp;author=K.%20Tempfli&amp;author=W.%20Shi" aria-describedby=ref-id-h0585>Google Scholar</a></div><dt class=label><a href=#bb0590 id=ref-id-b0590 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhang, 2015</a><dd class=reference id=h0590><div class=contribution>C. Zhang<div id=ref-id-h0590><strong class=title>Applying data fusion techniques for benthic habitat mapping and monitoring in a coral reef ecosystem</strong></div></div><div class=host>ISPRS J. Photogramm. Remote Sens., 104 (2015), pp. 213-223</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271614001531 aria-describedby=ref-id-h0590>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271614001531/pdfft?md5=6236a018361ff909819237e6142c5174&amp;pid=1-s2.0-S0924271614001531-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84939415665&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0590>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Applying%20data%20fusion%20techniques%20for%20benthic%20habitat%20mapping%20and%20monitoring%20in%20a%20coral%20reef%20ecosystem&amp;publication_year=2015&amp;author=C.%20Zhang" aria-describedby=ref-id-h0590>Google Scholar</a></div><dt class=label><a href=#bb0595 id=ref-id-b0595 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhang et al., 2008</a><dd class=reference id=h0595><div class=contribution>H. Zhang, J.E. Fritts, S.A. Goldman<div id=ref-id-h0595><strong class=title>Image segmentation evaluation: a survey of unsupervised methods</strong></div></div><div class=host>Comput. Vision Image Understanding, 110 (2008), pp. 260-280</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S1077314207001294 aria-describedby=ref-id-h0595>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S1077314207001294/pdfft?md5=f0d8b20c4a7c9def389b3d0188d348dd&amp;pid=1-s2.0-S1077314207001294-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-41949100170&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0595>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Image%20segmentation%20evaluation%3A%20a%20survey%20of%20unsupervised%20methods&amp;publication_year=2008&amp;author=H.%20Zhang&amp;author=J.E.%20Fritts&amp;author=S.A.%20Goldman" aria-describedby=ref-id-h0595>Google Scholar</a></div><dt class=label><a href=#bb0600 id=ref-id-b0600 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhen et al., 2013</a><dd class=reference id=h0600><div class=contribution>Z. Zhen, L.J. Quackenbush, S.V. Stehman, L. Zhang<div id=ref-id-h0600><strong class=title>Impact of training and validation sample selection on classification accuracy and accuracy assessment when using reference polygons in object-based classification</strong></div></div><div class=host>Int. J. Remote Sens., 34 (19) (2013), pp. 6914-6930</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431161.2013.810822></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431161.2013.810822 aria-describedby=ref-id-h0600>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84880164554&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0600>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Impact%20of%20training%20and%20validation%20sample%20selection%20on%20classification%20accuracy%20and%20accuracy%20assessment%20when%20using%20reference%20polygons%20in%20object-based%20classification&amp;publication_year=2013&amp;author=Z.%20Zhen&amp;author=L.J.%20Quackenbush&amp;author=S.V.%20Stehman&amp;author=L.%20Zhang" aria-describedby=ref-id-h0600>Google Scholar</a></div><dt class=label><a href=#bb0605 id=ref-id-b0605 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhao et al., 2017</a><dd class=reference id=h0605><span>Zhao, W., Du, S., Emery, W.J., 2017. Object-based convolutional neural network for high-resolution imagery classification. IEEE J. Sel. Top. Appl. Earth Obs. Rem. Sens. PP(99), 1–11.</span><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Zhao,%20W.,%20Du,%20S.,%20Emery,%20W.J.,%202017.%20Object-based%20convolutional%20neural%20network%20for%20high-resolution%20imagery%20classification.%20IEEE%20J.%20Sel.%20Top.%20Appl.%20Earth%20Obs.%20Rem.%20Sens.%20PP,%20111.">Google Scholar</a></div><dt class=label><a href=#bb0610 id=ref-id-b0610 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zolkos et al., 2013</a><dd class=reference id=h0610><div class=contribution>S.G. Zolkos, S.J. Goetz, R. Dubayah<div id=ref-id-h0610><strong class=title>A meta-analysis of terrestrial aboveground biomass estimation using lidar remote sensing</strong></div></div><div class=host>Remote Sens. Environ., 128 (2013), pp. 289-298</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712004051 aria-describedby=ref-id-h0610>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425712004051/pdfft?md5=45bee09490b11b25f5fbd82376cfaddd&amp;pid=1-s2.0-S0034425712004051-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-84868663200&amp;partnerID=10&amp;rel=R3.0.0" aria-describedby=ref-id-h0610>View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20meta-analysis%20of%20terrestrial%20aboveground%20biomass%20estimation%20using%20lidar%20remote%20sensing&amp;publication_year=2013&amp;author=S.G.%20Zolkos&amp;author=S.J.%20Goetz&amp;author=R.%20Dubayah" aria-describedby=ref-id-h0610>Google Scholar</a></div></dl></section></section><div id=section-cited-by><section class="ListArticles preview"><div class=PageDivider></div><header id=citing-articles-header><h2 class="u-h3 u-margin-l-ver u-font-serif">Cited by (512)</h2></header><div aria-describedby=citing-articles-header><div class="citing-articles u-margin-l-bottom"><ul><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S1574954122002187 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article0-title>UAV-based classification of maritime Antarctic vegetation types using GEOBIA and random forest</h3></a><div class="article-source ellipsis">2022, Ecological Informatics</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article0-title aria-controls=citing-articles-article0 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271622002337 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article1-title>A hybrid image segmentation method for building extraction from high-resolution RGB images</h3></a><div class="article-source ellipsis">2022, ISPRS Journal of Photogrammetry and Remote Sensing</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article1-title aria-controls=citing-articles-article1 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0341816222004416 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article2-title>An approach to evaluate the dominant river biogeomorphic succession phase at the reach-scale</h3></a><div class="article-source ellipsis">2022, Catena</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article2-title aria-controls=citing-articles-article2 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0017931022004896 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article3-title>Machine learning enabled condensation heat transfer measurement</h3></a><div class="article-source ellipsis">2022, International Journal of Heat and Mass Transfer</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article3-title aria-controls=citing-articles-article3 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0924271622001848 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article4-title>Cross-spatiotemporal land-cover classification from VHR remote sensing images with deep learning based domain adaptation</h3></a><div class="article-source ellipsis">2022, ISPRS Journal of Photogrammetry and Remote Sensing</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article4-title aria-controls=citing-articles-article4 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S235293852200074X target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article5-title>Impact assessment of humanitarian demining using object-based peri-urban land cover classification and morphological building detection from VHR Worldview imagery</h3></a><div class="article-source ellipsis">2022, Remote Sensing Applications: Society and Environment</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article5-title aria-controls=citing-articles-article5 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div></ul><a class="button-alternative button-alternative-secondary button-cited-by-more" href="http://www-scopus-com.simsrad.net.ocs.mq.edu.au/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-85021219961&amp;md5=acf7f64ba971b20acaff6fc8fec331a" id=citing-articles-view-all-btn target=_blank><svg focusable=false viewBox="0 0 78 128" width=32 height=32 class="icon icon-arrow-up-right"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg><span class=button-alternative-text>View all citing articles on Scopus</span></a></div></div></section></div><div class=Copyright><span class=copyright-line>© 2017 The Authors. Published by Elsevier B.V. on behalf of International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS).</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class=RelatedContent aria-label="Related content"><section class="SidePanel u-margin-s-bottom"><header id=recommended-articles-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type=button><span class=button-link-text><h2 class="section-title u-h4">Recommended articles</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=recommended-articles-header><div id=recommended-articles><ul><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0030401812008760><h3 class="article-title ellipsis text-s" id=recommended-articles-article0-title><span>Building region derivation from LiDAR data using a reversed iterative mathematic morphological algorithm</span></h3></a><div class="article-source ellipsis"><div class=source>Optics Communications, Volume 286, 2013, pp. 244-250</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0030401812008760/pdfft?md5=095d187db8e6a146e85f6b95218a19f6&amp;pid=1-s2.0-S0030401812008760-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article0-title aria-controls=recommended-articles-article0 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article0 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716303820><h3 class="article-title ellipsis text-s" id=recommended-articles-article1-title><span>Assessing the robustness of Random Forests to map land cover with high resolution satellite image time series over large areas</span></h3></a><div class="article-source ellipsis"><div class=source>Remote Sensing of Environment, Volume 187, 2016, pp. 156-168</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425716303820/pdfft?md5=d1da9bd3ca2eee57221271a4096ae84e&amp;pid=1-s2.0-S0034425716303820-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article1-title aria-controls=recommended-articles-article1 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article1 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715301565><h3 class="article-title ellipsis text-s" id=recommended-articles-article2-title><span>Landsat 8 OLI image based terrestrial water extraction from heterogeneous backgrounds using a reflectance homogenization approach</span></h3></a><div class="article-source ellipsis"><div class=source>Remote Sensing of Environment, Volume 171, 2015, pp. 14-32</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715301565/pdfft?md5=07971bcc4186fe70b353d5225cac2d52&amp;pid=1-s2.0-S0034425715301565-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article2-title aria-controls=recommended-articles-article2 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article2 aria-hidden=true></div></ul></div><div class="pagination u-position-relative u-padding-s-bottom"><span class=u-position-absolute></span><span class=pagination-pages-label><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class=u-position-absolute><button class="button-link button-link-secondary next-button" data-aa-button="sd:product:journal:article:location=recommended-articles:type=Next" type=button><span class=button-link-text>Next</span><svg focusable=false viewBox="0 0 54 128" width=10.125 height=24 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel u-margin-s-bottom"><header id=metrics-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true type=button><span class=button-link-text><h2 class="section-title u-h4">Article Metrics</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=metrics-header><div class=plum-sciencedirect-theme><div class=PlumX-Summary><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top sf-hidden"></div><div class=pps-cols><div class="pps-col plx-citation"><div class=plx-citation><div class=pps-title>Citations</div><ul><li class=plx-citation><span class=pps-label>Citation Indexes: </span><span class=pps-count>495</span><li class=plx-citation><span class=pps-label>Policy Citations: </span><span class=pps-count>2</span></ul></div></div><div class="pps-col plx-capture"><div class=plx-capture><div class=pps-title>Captures</div><ul><li class=plx-capture><span class=pps-label>Exports-Saves: </span><span class=pps-count>16</span><li class=plx-capture><span class=pps-label>Readers: </span><span class=pps-count>1056</span></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src=data:image/png;base64,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 class=plx-logo></div><a target=_blank href="https://plu.mx/plum/a/?doi=10.1016/j.isprsjprs.2017.06.001&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class=pps-seemore title="PlumX Metrics Detail Page">View details<svg fill=currentColor tabindex=-1 focusable=false width=16 height=16 viewBox="0 0 16 16" class=svg-arrow><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div><div></div></div></div><footer role=contentinfo class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a aria-label="Elsevier home page (opens in a new tab)" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/ target=_blank rel=nofollow><img class=footer-logo src=data:null;base64, alt="Elsevier logo with wordmark" height=64 width=58 loading=lazy></a></div><div class=els-footer-content><div class=u-remove-if-print><ul class="els-footer-links u-margin-xs-bottom" style=list-style:none><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/solutions/sciencedirect id=els-footer-about-science-direct target=_blank rel=nofollow><span class=anchor-text>About ScienceDirect</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=" id=els-footer-remote-access target=_blank rel=nofollow><span class=anchor-text>Remote access</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://sd-cart-elsevier-com.simsrad.net.ocs.mq.edu.au/? id=els-footer-shopping-cart target=_blank rel=nofollow><span class=anchor-text>Shopping cart</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=http://elsmediakits.com/ id=els-footer-advertise target=_blank rel=nofollow><span class=anchor-text>Advertise</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/contact/supporthub/sciencedirect/ id=els-footer-contact-support target=_blank rel=nofollow><span class=anchor-text>Contact and support</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/elsevier-website-terms-and-conditions id=els-footer-terms-condition target=_blank rel=nofollow><span class=anchor-text>Terms and conditions</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/privacy-policy id=els-footer-privacy-policy target=_blank rel=nofollow><span class=anchor-text>Privacy policy</span></a></ul></div><p id=els-footer-cookie-message class=u-remove-if-print>We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the <a class="anchor u-clr-grey8 u-margin-0-right" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/legal/use-of-cookies target=_blank rel=nofollow><span class=anchor-text><strong>use of cookies</strong></span></a>.<p id=els-footer-copyright>Copyright © 2022 Elsevier B.V. or its licensors or contributors. <span class=u-remove-if-print>ScienceDirect® is a registered trademark of Elsevier B.V.</span><p class="u-remove-if-not-print sf-hidden">ScienceDirect® is a registered trademark of Elsevier B.V.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a aria-label="RELX home page (opens in a new tab)" id=els-footer-relx href=https://www.relx.com/ target=_blank rel=nofollow><img loading=lazy src=data:null;base64, width=93 height=20 alt="RELX group home page"></a></div></footer></div></section></div></div></div>
<div id=UMS_TOOLTIP style=position:absolute;cursor:pointer;z-index:2147483647;background:transparent;top:-100000px;left:-100000px></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><button aria-label=Feedback type=button id=_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E data-layout=badgeBlank class="_pendo-badge _pendo-badge_" style="z-index:19000;margin:0px;line-height:1;font-size:0px;background:rgba(255,255,255,0);padding:0px;height:32px;width:128px;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;border:0px;float:none;vertical-align:baseline;cursor:pointer;position:absolute;top:55655px;left:1423px"><img id=pendo-image-badge-c2b2bcc0 src=data:image/svg+xml;base64,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 data-_pendo-image-1 class="_pendo-image _pendo-badge-image" style="display:block;height:32px;width:128px;padding:0px;margin:0px;line-height:1;border:none;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;float:none;vertical-align:baseline"></button><umsdataelement id=UMSSendDataEventElement data="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS092427161630661X&amp;from=globalheader" docguid=null></umsdataelement><div id=tmtoolbar_manual_rating_injected style=display:none>init</div>