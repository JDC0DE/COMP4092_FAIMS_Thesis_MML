<!DOCTYPE html> <html lang=en style><!--
 Page saved with SingleFile 
 url: https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350 
 saved date: Sun Sep 18 2022 17:34:02 GMT+1000 (Australian Eastern Standard Time)
--><meta charset=utf-8>
<meta name=citation_pii content=S0034425706001350>
<meta name=citation_issn content=0034-4257>
<meta name=citation_volume content=103>
<meta name=citation_lastpage content=189>
<meta name=citation_issue content=2>
<meta name=citation_publisher content=Elsevier>
<meta name=citation_firstpage content=179>
<meta name=citation_journal_title content="Remote Sensing of Environment">
<meta name=citation_type content=JOUR>
<meta name=citation_doi content=10.1016/j.rse.2006.04.001>
<meta name=dc.identifier content=10.1016/j.rse.2006.04.001>
<meta name=citation_article_type content="Full-length article">
<meta property=og:description content="The accuracy of a supervised image classification is a function of the training data used in its generation. It is, therefore, critical that the trainâ€¦">
<meta property=og:image content=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0034425706X02985-cov150h.gif>
<meta name=citation_title content="The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM">
<meta property=og:title content="The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM">
<meta name=citation_publication_date content=2006/07/30>
<meta name=citation_online_date content=2006/06/22>
<meta name=robots content=INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR>
<title>The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM - ScienceDirect</title>
<link rel=canonical href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350>
<meta property=og:type content=article>
<meta name=viewport content="initial-scale=1">
<meta name=SDTech content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
<meta http-equiv=origin-trial content="A+cA2PUOfIOKAdSDJOW5CP9ZlxONy1yu+hqAq72zUtKw4rLdihqRp6Nui/jUyCyegr+BUtH+C+Elv0ufn05yBQEAAACFeyJvcmlnaW4iOiJodHRwczovL2RvdWJsZWNsaWNrLm5ldDo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="A+zsdH3aNZT/bkjT8U/o5ACzyaeNYzTvtoVmwf/KOilfv39pxY2AIsOwhQJv+YnXp98i3TqrQibIVtMWs5UHjgoAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXN5bmRpY2F0aW9uLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv=origin-trial content="AxceVEhIegcDEHqLXFQ2+vPKqzCppoJYsRCZ/BdfVnbM/sUUF2BXV8lwNosyYjvoxnTh2FC8cOlAnA5uULr/zAUAAACLeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZXRhZ3NlcnZpY2VzLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjY5NzY2Mzk5LCJpc1N1YmRvbWFpbiI6dHJ1ZSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><style>.plx-logo{height:16px;width:65.62712px;margin:0 0 0 4px!important;vertical-align:top;border:none!important;-ms-interpolation-mode:bicubic}.PlumX-Summary{font-weight:400;overflow:hidden}.PlumX-Summary *{text-align:left}.PlumX-Summary ul,.PlumX-Summary li{list-style-type:none!important;margin:0!important;padding:0!important;line-height:normal!important;background-image:none!important}.PlumX-Summary .pps-branding{height:14px;line-height:14px;color:#6e1a62}.PlumX-Summary .pps-cols{overflow:hidden}.PlumX-Summary .pps-container{border-top:none}.PlumX-Summary .pps-seemore{color:#007dbb!important}.PlumX-Summary .pps-container-vertical .pps-seemore{text-align:center!important;position:relative;top:-0.7em}.PlumX-Summary .pps-container{position:relative}.plum-sciencedirect-theme{font-family:Arial,Helvetica,"Lucida Sans Unicode","Microsoft Sans Serif","Segoe UI Symbol",STIXGeneral,"Cambria Math","Arial Unicode MS",sans-serif;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:auto}.plum-sciencedirect-theme .PlumX-Summary{font-family:Arial,Helvetica,"Lucida Sans Unicode","Microsoft Sans Serif","Segoe UI Symbol",STIXGeneral,"Cambria Math","Arial Unicode MS",sans-serif;font-size:16px}.plum-sciencedirect-theme .PlumX-Summary .pps-container{float:none;border:0}.plum-sciencedirect-theme .PlumX-Summary .pps-title{margin-bottom:12px;line-height:1.5;font-weight:normal;padding-bottom:6px;color:#505050!important}.plum-sciencedirect-theme .PlumX-Summary .pps-cols{margin-bottom:16px}.plum-sciencedirect-theme .PlumX-Summary .pps-col{float:none;width:auto;padding:4px 0;border-top:none}.plum-sciencedirect-theme .PlumX-Summary .pps-col div.plx-citation>.pps-title{border-bottom:2px solid #fd5704}.plum-sciencedirect-theme .PlumX-Summary .pps-col div.plx-capture>.pps-title{border-bottom:2px solid #c43bf3}.plum-sciencedirect-theme .PlumX-Summary .pps-col li:after,.plum-bigben-theme .PlumX-Summary .pps-col li:after{content:"";display:table;clear:both}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore{color:#007dbb;text-decoration:none;font-size:13px}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore .svg-arrow{width:8px;height:8px;margin-left:8px;transform:rotate(270deg)}.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:active,.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:focus,.plum-sciencedirect-theme .PlumX-Summary .pps-seemore:hover,.plum-bigben-theme .PlumX-Summary .pps-seemore:active,.plum-bigben-theme .PlumX-Summary .pps-seemore:focus,.plum-bigben-theme .PlumX-Summary .pps-seemore:hover{color:#e9711c;text-decoration:underline;outline:0}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-seemore{display:inline;float:right;text-align:left;margin-top:12px}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-branding{width:auto}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-col{margin-bottom:8px}.plum-sciencedirect-theme .PlumX-Summary .pps-container-vertical .pps-branding-bottom{display:inline}.plum-sciencedirect-theme .PlumX-Summary .pps-count{color:#505050;font-weight:normal;display:block;float:left;vertical-align:top;width:30%;font-size:14px;text-align:right}.plum-sciencedirect-theme .PlumX-Summary .pps-label{color:#505050;display:block;float:left;vertical-align:top;width:66%;padding-right:4%;padding-bottom:8px;font-size:14px}.MJX_Assistive_MathML{position:absolute!important;top:0;left:0;clip:rect(1px,1px,1px,1px);padding:1px 0 0 0!important;border:0!important;height:1px!important;width:1px!important;overflow:hidden!important;display:block!important;-webkit-touch-callout:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1em;bottom:1.5em;background-color:#E6E6E6;border:1px solid #959595;margin:0px;padding:2px 8px;z-index:102;color:black;font-size:80%;width:auto;white-space:nowrap}#\_pendo-badge\_9BcFvkCLLiElWp6hocDK3ZG6Z4E{top:auto!important;left:auto!important;bottom:0px!important;right:20px!important;position:fixed!important}.MathJax_SVG{font-style:normal;font-weight:normal;line-height:normal;text-indent:0;text-align:left;text-transform:none;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;direction:ltr;max-width:none;max-height:none;min-width:0;min-height:0;border:0;padding:0;margin:0}.MathJax_SVG *{transition:none;-webkit-transition:none;-moz-transition:none;-ms-transition:none;-o-transition:none}.sf-hidden{display:none!important}</style><meta http-equiv=content-security-policy content="default-src 'none'; font-src 'self' data:; img-src 'self' data:; style-src 'unsafe-inline'; media-src 'self' data:; script-src 'unsafe-inline' data:;"></head>
<body><div style=visibility:hidden;overflow:hidden;position:absolute;top:0px;height:1px;width:auto;padding:0px;border:0px;margin:0px;text-align:left;text-indent:0px;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal><div id=MathJax_SVG_Hidden></div><svg><defs id=MathJax_SVG_glyphs><path stroke-width=1 id=MJMATHI-6E d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMAIN-3D d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path><path stroke-width=1 id=MJMATHI-3C3 d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path><path stroke-width=1 id=MJMAIN-32 d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path stroke-width=1 id=MJMATHI-7A d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path><path stroke-width=1 id=MJMATHI-68 d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path><path stroke-width=1 id=MJMAIN-2B d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width=1 id=MJMATHI-4E d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path><path stroke-width=1 id=MJMATHI-79 d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMATHI-69 d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMAIN-28 d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width=1 id=MJMAINB-77 d="M624 444Q636 441 722 441Q797 441 800 444H805V382H741L593 11Q592 10 590 8T586 4T584 2T581 0T579 -2T575 -3T571 -3T567 -4T561 -4T553 -4H542Q525 -4 518 6T490 70Q474 110 463 137L415 257L367 137Q357 111 341 72Q320 17 313 7T289 -4H277Q259 -4 253 -2T238 11L90 382H25V444H32Q47 441 140 441Q243 441 261 444H270V382H222L310 164L382 342L366 382H303V444H310Q322 441 407 441Q508 441 523 444H531V382H506Q481 382 481 380Q482 376 529 259T577 142L674 382H617V444H624Z"></path><path stroke-width=1 id=MJMAIN-22C5 d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path><path stroke-width=1 id=MJMAINB-78 d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path><path stroke-width=1 id=MJMATHI-62 d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path><path stroke-width=1 id=MJMAIN-29 d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path><path stroke-width=1 id=MJMAIN-2212 d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path stroke-width=1 id=MJMAIN-31 d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path stroke-width=1 id=MJMAIN-2265 d="M83 616Q83 624 89 630T99 636Q107 636 253 568T543 431T687 361Q694 356 694 346T687 331Q685 329 395 192L107 56H101Q83 58 83 76Q83 77 83 79Q82 86 98 95Q117 105 248 167Q326 204 378 228L626 346L360 472Q291 505 200 548Q112 589 98 597T83 616ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path><path stroke-width=1 id=MJMAIN-30 d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path stroke-width=1 id=MJMAIN-7C d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width=1 id=MJMAIN-6D d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path stroke-width=1 id=MJMAIN-69 d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z"></path><path stroke-width=1 id=MJMAIN-6E d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path stroke-width=1 id=MJMAIN-7B d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path><path stroke-width=1 id=MJMAIN-7D d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path><path stroke-width=1 id=MJSZ2-7B d="M547 -643L541 -649H528Q515 -649 503 -645Q324 -582 293 -466Q289 -449 289 -428T287 -200L286 42L284 53Q274 98 248 135T196 190T146 222L121 235Q119 239 119 250Q119 262 121 266T133 273Q262 336 284 449L286 460L287 701Q287 737 287 794Q288 949 292 963Q293 966 293 967Q325 1080 508 1148Q516 1150 527 1150H541L547 1144V1130Q547 1117 546 1115T536 1109Q480 1086 437 1046T381 950L379 940L378 699Q378 657 378 594Q377 452 374 438Q373 437 373 436Q350 348 243 282Q192 257 186 254L176 251L188 245Q211 236 234 223T287 189T340 135T373 65Q373 64 374 63Q377 49 378 -93Q378 -156 378 -198L379 -438L381 -449Q393 -504 436 -544T536 -608Q544 -611 545 -613T547 -629V-643Z"></path><path stroke-width=1 id=MJSZ2-7D d="M119 1130Q119 1144 121 1147T135 1150H139Q151 1150 182 1138T252 1105T326 1046T373 964Q378 942 378 702Q378 469 379 462Q386 394 439 339Q482 296 535 272Q544 268 545 266T547 251Q547 241 547 238T542 231T531 227T510 217T477 194Q390 129 379 39Q378 32 378 -201Q378 -441 373 -463Q342 -580 165 -644Q152 -649 139 -649Q125 -649 122 -646T119 -629Q119 -622 119 -619T121 -614T124 -610T132 -607T143 -602Q195 -579 235 -539T285 -447Q286 -435 287 -199T289 51Q294 74 300 91T329 138T390 197Q412 213 436 226T475 244L489 250L472 258Q455 265 430 279T377 313T327 366T293 434Q289 451 289 472T287 699Q286 941 285 948Q279 978 262 1005T227 1048T184 1080T151 1100T129 1109L127 1110Q119 1113 119 1130Z"></path><path stroke-width=1 id=MJMAIN-3E d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path><path stroke-width=1 id=MJMATHI-3BE d="M268 632Q268 704 296 704Q314 704 314 687Q314 682 311 664T308 635T309 620V616H315Q342 619 360 619Q443 619 443 586Q439 548 358 546H344Q326 546 317 549T290 566Q257 550 226 505T195 405Q195 381 201 364T211 342T218 337Q266 347 298 347Q375 347 375 314Q374 297 359 288T327 277T280 275Q234 275 208 283L195 286Q149 260 119 214T88 130Q88 116 90 108Q101 79 129 63T229 20Q238 17 243 15Q337 -21 354 -33Q383 -53 383 -94Q383 -137 351 -171T273 -205Q240 -205 202 -190T158 -167Q156 -163 156 -159Q156 -151 161 -146T176 -140Q182 -140 189 -143Q232 -168 274 -168Q286 -168 292 -165Q313 -151 313 -129Q313 -112 301 -104T232 -75Q214 -68 204 -64Q198 -62 171 -52T136 -38T107 -24T78 -8T56 12T36 37T26 66T21 103Q21 149 55 206T145 301L154 307L148 313Q141 319 136 323T124 338T111 358T103 382T99 413Q99 471 143 524T259 602L271 607Q268 618 268 632Z"></path><path stroke-width=1 id=MJMATHI-43 d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path><path stroke-width=1 id=MJSZ1-2211 d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path><path stroke-width=1 id=MJMATHI-72 d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMAIN-5B d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path><path stroke-width=1 id=MJMAIN-2223 d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path><path stroke-width=1 id=MJMAIN-5D d="M22 710V750H159V-250H22V-210H119V710H22Z"></path><path stroke-width=1 id=MJSZ3-5B d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"></path><path stroke-width=1 id=MJSZ3-5D d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"></path><path stroke-width=1 id=MJMATHI-66 d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path><path stroke-width=1 id=MJMATHI-78 d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path><path stroke-width=1 id=MJMAIN-73 d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path stroke-width=1 id=MJMAIN-67 d="M329 409Q373 453 429 453Q459 453 472 434T485 396Q485 382 476 371T449 360Q416 360 412 390Q410 404 415 411Q415 412 416 414V415Q388 412 363 393Q355 388 355 386Q355 385 359 381T368 369T379 351T388 325T392 292Q392 230 343 187T222 143Q172 143 123 171Q112 153 112 133Q112 98 138 81Q147 75 155 75T227 73Q311 72 335 67Q396 58 431 26Q470 -13 470 -72Q470 -139 392 -175Q332 -206 250 -206Q167 -206 107 -175Q29 -140 29 -75Q29 -39 50 -15T92 18L103 24Q67 55 67 108Q67 155 96 193Q52 237 52 292Q52 355 102 398T223 442Q274 442 318 416L329 409ZM299 343Q294 371 273 387T221 404Q192 404 171 388T145 343Q142 326 142 292Q142 248 149 227T179 192Q196 182 222 182Q244 182 260 189T283 207T294 227T299 242Q302 258 302 292T299 343ZM403 -75Q403 -50 389 -34T348 -11T299 -2T245 0H218Q151 0 138 -6Q118 -15 107 -34T95 -74Q95 -84 101 -97T122 -127T170 -155T250 -167Q319 -167 361 -139T403 -75Z"></path><path stroke-width=1 id=MJMATHI-3B1 d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path><path stroke-width=1 id=MJMATHI-6B d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path><path stroke-width=1 id=MJMAIN-2C d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width=1 id=MJMATHI-65 d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path><path stroke-width=1 id=MJMATHI-3B3 d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path><path stroke-width=1 id=MJSZ2-28 d="M180 96T180 250T205 541T266 770T353 944T444 1069T527 1150H555Q561 1144 561 1141Q561 1137 545 1120T504 1072T447 995T386 878T330 721T288 513T272 251Q272 133 280 56Q293 -87 326 -209T399 -405T475 -531T536 -609T561 -640Q561 -643 555 -649H527Q483 -612 443 -568T353 -443T266 -270T205 -41Z"></path><path stroke-width=1 id=MJSZ2-29 d="M35 1138Q35 1150 51 1150H56H69Q113 1113 153 1069T243 944T330 771T391 541T416 250T391 -40T330 -270T243 -443T152 -568T69 -649H56Q43 -649 39 -647T35 -637Q65 -607 110 -548Q283 -316 316 56Q324 133 324 251Q324 368 316 445Q278 877 48 1123Q36 1137 35 1138Z"></path><path stroke-width=1 id=MJSZ3-28 d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"></path><path stroke-width=1 id=MJSZ3-29 d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"></path><path stroke-width=1 id=MJMATHI-77 d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path><path stroke-width=1 id=MJMATHI-63 d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path><path stroke-width=1 id=MJMATHI-6D d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width=1 id=MJMATHI-54 d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path><path stroke-width=1 id=MJMATHI-6C d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path><path stroke-width=1 id=MJMAIN-2260 d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path><path stroke-width=1 id=MJMATHBI-3D5 d="M274 -7Q232 -4 195 7T125 38T71 94T51 176V190Q51 213 60 242T95 307T156 373T255 425T393 451L397 452L427 568Q434 597 443 636Q452 677 456 685T472 694H486H495Q517 694 517 680L514 665Q510 650 503 621T489 564L460 451H469Q527 447 574 430T657 370T693 266Q693 163 599 82T350 -7H346L322 -100Q301 -190 295 -197Q291 -202 283 -202H269H258Q238 -202 238 -188Q238 -186 260 -96L283 -7H274ZM449 400Q448 400 404 225T359 47T366 45Q464 55 516 119Q542 149 558 199T575 295Q575 387 462 398L449 400ZM384 398Q384 399 381 399Q350 399 298 378T214 308Q168 236 168 149Q168 68 259 49Q282 44 294 44H295L384 398Z"></path><path stroke-width=1 id=MJSZ1-28 d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path><path stroke-width=1 id=MJSZ1-29 d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path><path stroke-width=1 id=MJMAIN-2026 d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path><path stroke-width=1 id=MJMAIN-2E d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path stroke-width=1 id=MJMAIN-2208 d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path><path stroke-width=1 id=MJMAIN-61 d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z"></path><path stroke-width=1 id=MJMAIN-72 d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z"></path><path stroke-width=1 id=MJMAIN-78 d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z"></path></defs></svg></div><div class=zotero-notificaton style='position:relative;z-index:2147483647;background:linear-gradient(rgb(255,225,62),rgb(255,199,3));color:rgba(0,0,0,0.95);border-bottom:1px solid rgb(191,138,1);padding:3px 10px 4px;display:flex;flex-direction:row;align-items:center;box-sizing:border-box;cursor:default;transition:margin-top 300ms ease 0s,opacity 300ms ease 0s;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;margin-top:0px;margin-right:0px;margin-left:0px'><span style=flex-grow:1>Zotero detected that you are accessing www.sciencedirect.com through a proxy. Would you like to automatically redirect future requests to www.sciencedirect.com through simsrad.net.ocs.mq.edu.au?</span><span style=min-width:30px></span><a data-id=2 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350 style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>Accept</a><a data-id=1 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350 style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>Proxy Settings</a><a data-id=0 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350 style='padding:3px;text-decoration:none;margin:0px 0px 0px 30px;white-space:nowrap;font-family:"Lucida Grande",Tahoma,sans;font-size:8.5pt;line-height:1.4em;font-weight:bold;color:rgba(0,0,0,0.95)'>âœ•</a></div><div id=MathJax_Message style=display:none></div>
<noscript>
      JavaScript is disabled on your browser.
      Please enable JavaScript to use all the features on this page.
      <img src=https://smetrics-elsevier-com.simsrad.net.ocs.mq.edu.au/b/ss/elsevier-sd-prod/1/G.4--NS/1663486445956?pageName=sd%3Aproduct%3Ajournal%3Aarticle&c16=els%3Arp%3Ast&c2=sd&v185=img&v33=ae%3AANON_IP&c1=ae%3A2378&c12=ae%3A21981 />
    </noscript>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-content>Skip to main content</a>
<a class="sr-only sr-only-focusable" href=#screen-reader-main-title>Skip to article</a>
<!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/answers/detail/a_id/9831">this support page</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
<div data-iso-key=_0><div class=App id=app data-aa-name=root data-reactroot><div class=page><section><div class=sd-flex-container><div class=sd-flex-content><header id=gh-cnt><div id=gh-main-cnt class="u-flex-center-ver u-position-relative u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-lg"><a id=gh-branding class=u-flex-center-ver href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/ aria-label="ScienceDirect home page" data-aa-region=header data-aa-name=ScienceDirect><img class=gh-logo src=data:null;base64, alt="Elsevier logo" height=48 width=54><svg xmlns=http://www.w3.org/2000/svg role=img version=1.1 height=30 width=138 viewBox="0 0 138 30" class="gh-wordmark u-margin-s-left" fill=#f36d21 aria-labelledby=gh-wm-science-direct focusable=false aria-hidden=true alt="ScienceDirect Wordmark"><title id=gh-wm-science-direct>ScienceDirect</title><g><path class=a d=M4.23,21a9.79,9.79,0,0,1-4.06-.83l.29-2.08a7.17,7.17,0,0,0,3.72,1.09c2.13,0,3-1.22,3-2.39C7.22,13.85.3,13.43.3,9c0-2.37,1.56-4.29,5.2-4.29a9.12,9.12,0,0,1,3.77.75l-.1,2.08a7.58,7.58,0,0,0-3.67-1c-2.24,0-2.91,1.22-2.91,2.39,0,3,6.92,3.61,6.92,7.8C9.5,19.1,7.58,21,4.23,21Z></path><path class=a d=M20.66,20A6.83,6.83,0,0,1,16.76,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H18.81c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M23.75,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,23.75,6.9ZM22.76,9h2V20.74h-2Z></path><path class=a d=M29.55,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,32.77,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM35.63,13c-.08-2.29-1.09-2.7-3-2.7A3.78,3.78,0,0,0,31,10.7,3.7,3.7,0,0,0,29.76,13Z></path><path class=a d=M49.7,20.74h-2s.1-2.73.08-5.1c0,0,0-1.56,0-2.5-.05-1.79-.21-2.7-2-2.7a4.87,4.87,0,0,0-1.64.31,12.11,12.11,0,0,0-1.95,2.08v7.9h-2v-8.5a19.47,19.47,0,0,0-.1-2.34L39.95,9h1.85l.31,1.74a4.71,4.71,0,0,1,3.82-2.05c2.11,0,3.54.68,3.74,3.09.1,1.17.08,2.34.08,3.51C49.75,17.2,49.7,20.74,49.7,20.74Z></path><path class=a d=M61.5,20A6.83,6.83,0,0,1,57.6,21c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H59.66c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M64.75,14.6V15c0,2.81,1.38,4.34,3.85,4.34a6.37,6.37,0,0,0,3.69-1.22l.16,1.82A7.94,7.94,0,0,1,68,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM70.84,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,65,13Z></path><path class=a d=M81.21,20.74H75.83V5h5.62c5.54,0,7.46,4.21,7.46,7.8C88.91,16.26,86.93,20.74,81.21,20.74Zm-.1-14H77.88V19.07h3c4,0,5.75-2.31,5.75-6.24C86.59,10.15,85.34,6.7,81.11,6.7Z></path><path class=a d=M92.86,6.9a1.45,1.45,0,0,1-1.3-1.46,1.32,1.32,0,1,1,2.63,0A1.5,1.5,0,0,1,92.86,6.9ZM91.87,9h2V20.74h-2Z></path><path class=a d=M104.48,10.83l-1.64.47c0-.18-.08-1-.83-1-1.14,0-2.08,1.9-2.5,2.91v7.49h-2V12.18a18.78,18.78,0,0,0-.1-2.29L97.3,9h1.85l.34,1.87a3.22,3.22,0,0,1,2.68-2.16,2,2,0,0,1,2.26,1.72c0,.18.05.29.05.31Z></path><path class=a d=M107.44,14.6V15c0,2.81,1.38,4.34,3.85,4.34A6.37,6.37,0,0,0,115,18.11l.16,1.82A7.94,7.94,0,0,1,110.67,21c-3,0-5.3-2.29-5.3-6.16,0-4.06,2.21-6.24,5.25-6.24,3.61,0,4.73,1.87,4.73,6ZM113.53,13c-.08-2.29-1.09-2.7-3-2.7a3.78,3.78,0,0,0-1.56.36A3.7,3.7,0,0,0,107.65,13Z></path><path class=a d=M126.24,20a6.83,6.83,0,0,1-3.9,1.09c-3,0-5.23-2.18-5.23-6.29,0-4.29,2.91-6.11,5.28-6.11,2.16,0,3.67.55,3.85,2.11,0,.23,0,.57,0,.86H124.4c0-1-.55-1.25-1.9-1.25a2.87,2.87,0,0,0-1.35.21c-.21.13-1.85.94-1.85,4.11s1.9,4.65,3.59,4.65a5.91,5.91,0,0,0,3.2-1.2Z></path><path class=a d=M134.51,20.45a7.36,7.36,0,0,1-2.7.62c-1.53,0-2.63-.86-2.63-2.94V10.52H127V9h2.13V5.81h2V9h3.09v1.56h-3.09v7c0,1.33.34,1.85,1.25,1.85a5.66,5.66,0,0,0,2-.55Z></path></g></svg></a><div class="gh-nav-cnt u-hide-from-print"><div class="gh-nav-links-container gh-nav-links-container-h u-hide-from-print gh-nav-content-container"><nav aria-label=links class="gh-nav gh-nav-links gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-nav-item gh-move-to-spine"><a class="anchor gh-nav-action" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/browse/journals-and-books data-aa-region=header data-aa-name="Journals &amp; Books"><span class=anchor-text>Journals &amp; Books</span></a></ul></nav><nav aria-label=utilities class="gh-nav gh-nav-utilities gh-nav-h"><ul class="gh-nav-list u-list-reset"><li class="gh-move-to-spine gh-help-button gh-help-icon"><div class=popover id=gh-help-icon-popover><div id=popover-trigger-gh-help-icon-popover><button class="button-link gh-nav-help-icon gh-icon-btn button-link-primary" aria-expanded=false aria-label="ScienceDirect Support Center links" type=button><svg focusable=false viewBox="0 0 114 128" aria-hidden=true alt="ScienceDirect help page" width=21.375 height=24 class="icon icon-help gh-icon"><path d="m57 8c-14.7 0-28.5 5.72-38.9 16.1-10.38 10.4-16.1 24.22-16.1 38.9 0 30.32 24.68 55 55 55 14.68 0 28.5-5.72 38.88-16.1 10.4-10.4 16.12-24.2 16.12-38.9 0-30.32-24.68-55-55-55zm0 1e1c24.82 0 45 20.18 45 45 0 12.02-4.68 23.32-13.18 31.82s-19.8 13.18-31.82 13.18c-24.82 0-45-20.18-45-45 0-12.02 4.68-23.32 13.18-31.82s19.8-13.18 31.82-13.18zm-0.14 14c-11.55 0.26-16.86 8.43-16.86 18v2h1e1v-2c0-4.22 2.22-9.66 8-9.24 5.5 0.4 6.32 5.14 5.78 8.14-1.1 6.16-11.78 9.5-11.78 20.5v6.6h1e1v-5.56c0-8.16 11.22-11.52 12-21.7 0.74-9.86-5.56-16.52-16-16.74-0.39-0.01-0.76-0.01-1.14 0zm-4.86 5e1v1e1h1e1v-1e1h-1e1z"></path></svg><span class=button-link-text></span></button></div></div><li class="gh-search-toggle gh-nav-item search-button-link"><a class="anchor button-link-primary gh-nav-action gh-icon-btn" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/search data-aa-button=search-in-header-opened-from-article aria-label="Opens ScienceDirect Search"><span class=anchor-text></span><svg focusable=false viewBox="0 0 100 128" aria-hidden=true alt=Search width=18.75 height=24 class="icon icon-search gh-icon"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></a></ul></nav></div></div><div class="gh-move-to-spine u-hide-from-print gh-institution-item"><div class="popover text-s" id=institution-popover><div id=popover-trigger-institution-popover><button id=gh-inst-icon-btn class="gh-icon-btn gh-has-institution u-margin-m-left" aria-expanded=false aria-label="Institutional Access"><svg focusable=false viewBox="0 0 106 128" aria-hidden=true alt="Institutional Access" width=19.875 height=24 class="icon icon-institution gh-inst-icon"><path d="m84 98h1e1v1e1h-82v-1e1h1e1v-46h14v46h1e1v-46h14v46h1e1v-46h14v46zm-72-61.14l41-20.84 41 20.84v5.14h-82v-5.14zm92 15.14v-21.26l-51-25.94-51 25.94v21.26h1e1v36h-1e1v3e1h102v-3e1h-1e1v-36h1e1z"></path></svg></button></div></div></div><div class="gh-profile-container gh-move-to-spine u-hide-from-print"><a class="link-button u-margin-m-left link-button-primary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0034425706001350" id=gh-corpsignin-btn data-aa-region=header data-aa-name="Corporate sign in"><span class=link-button-text>Corporate sign in</span></a><a class="link-button u-margin-m-left link-button-secondary link-button-small" role=button href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/login?targetURL=%2Fscience%2Farticle%2Fpii%2FS0034425706001350&amp;from=globalheader" id=gh-signin-btn data-aa-region=header data-aa-name="Sign in"><span class=link-button-text>Sign in / register</span></a></div><div class="gh-lib-banner u-hide-from-print gh-lb-legacy"><a href=http://www.mq.edu.au/on_campus/library/ target=_blank rel="noopener noreferrer" class=gh-lib-banner-link><img class="u-img-responsive u-max-lib-height" src=data:null;base64, alt="You have institutional access"></a></div><div id=gh-mobile-menu class="mobile-menu u-hide-from-print sf-hidden"></div></div></header><div class=Article id=mathjax-container><div class=sticky-outer-wrapper><div class=sticky-inner-wrapper style=position:relative;z-index:2;transform:translate3d(0px,0px,0px)><div id=screen-reader-main-content></div><div class=accessbar role=region aria-label="Download options and search"><div class=accessbar-label></div><ul aria-label="PDF Options"><li><a class="link-button link-button-primary accessbar-primary-link" role=button aria-expanded=true aria-label="Download single PDF. Opens in a new window." aria-live=polite target=_blank href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350/pdfft?md5=036642101a3458e2b3ad72823e84dcb8&amp;pid=1-s2.0-S0034425706001350-main.pdf" rel=nofollow><svg focusable=false viewBox="0 0 32 32" height=24 width=24 class="icon icon-pdf-multicolor pdf-icon"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=link-button-text>View&nbsp;<strong>PDF</strong></span></a><li><button class="button button-anchor accessbar-anchor-link" aria-label="Download Full Issue" type=button><span class=button-text>Download Full Issue</span></button></ul><form class=QuickSearch action=/search#submit aria-label=form><input type=search class=query aria-label="Search ScienceDirect" name=qs placeholder="Search ScienceDirect" value><button class="button button-primary" type=submit aria-label="Submit search"><span class=button-text><svg focusable=false viewBox="0 0 100 128" height=20 width=18.75 class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button></form></div></div></div><div class="article-wrapper u-padding-s-top grid row"><div class="u-show-from-lg col-lg-6"><div class="TableOfContents u-margin-l-bottom" lang=en><div class=Outline id=toc-outline><h2 class=u-h4>Outline</h2><ol class=u-padding-xs-bottom><li><a href=#aep-abstract-id7 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Abstract>Abstract</a><li><a href=#aep-keywords-id9 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Keywords>Keywords</a><li><a href=#aep-section-id10 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="1. Introduction">1. Introduction</a><li><a href=#aep-section-id11 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="2. SVM classification">2. SVM classification</a><li><a href=#aep-section-id12 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="3. Data and methods">3. Data and methods</a><li><a href=#aep-section-id13 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="4. Results and discussion">4. Results and discussion</a><li><a href=#aep-section-id14 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title="5. Summary and conclusion">5. Summary and conclusion</a><li><a href=#aep-acknowledgment-id15 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=Acknowledgments>Acknowledgments</a><li><a href=#aep-bibliography-id16 data-aa-button="sd:product:journal:article:type=anchor:name=outlinelink" title=References>References</a></ol><div class=PageDivider></div></div><div class=CitedBy id=toc-cited-by><h2 class=u-h4><a href=#section-cited-by>Cited By (343)</a></h2><div class=PageDivider></div></div><div class=Figures id=toc-figures><h2 class=u-h4>Figures (4)</h2><ol><li><a href=#fig1 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 1. A simplified view of the location of pure and mixed class spectral responses inâ€¦" src=data:null;base64, style=max-width:85px;max-height:93px></div></a><li><a href=#fig2 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 2. Basic issues in classification SVM for (A) linearly separable and (B) partiallyâ€¦" src=data:null;base64, style=max-width:125px;max-height:39px></div></a><li><a href=#fig3 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 3. SPOT HRV near-infrared image of the study area with the location of the threeâ€¦" src=data:null;base64, style=max-width:75px;max-height:93px></div></a><li><a href=#fig4 data-aa-button="sd:product:journal:article:type=anchor:name=figure"><div><img alt="Fig. 4. Summary of the method based on mixed spectral responses" src=data:null;base64, style=max-width:79px;max-height:93px></div></a></ol><div class=PageDivider></div></div><div class=Tables id=toc-tables><h2 class=u-h4>Tables (5)</h2><ol class=u-padding-s-bottom><li><a href=#tbl1 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="The SVM parameter settings used"><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 1</a><li><a href=#tbl2 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Confusion matrix from the SVM classification derived with the use of the conventional, 30p, training set"><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 2</a><li><a href=#tbl3 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Confusion matrix from the SVM classification derived with the use of the mixed spectral response training set"><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 3</a><li><a href=#tbl4 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Confusion matrix from the SVM classification derived with the use of the conventional, same size, training set"><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 4</a><li><a href=#tbl5 data-aa-button="sd:product:journal:article:type=anchor:name=table" title="Confusion matrix from the SVM classification derived with the use of the conventional, same fields, training set"><svg focusable=false viewBox="0 0 98 128" width=18.375 height=24 class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg>Table 5</a></ol><div class=PageDivider></div></div></div></div><article class="col-lg-12 col-md-16 pad-left pad-right" role=main lang=en><div class=Publication id=publication><div class="publication-brand u-show-from-sm"><a title="Go to Remote Sensing of Environment on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/remote-sensing-of-environment><img class=publication-brand-image src=data:null;base64, alt=Elsevier></a></div><div class="publication-volume u-text-center"><h2 class="publication-title u-h3" id=publication-title><a class=publication-title-link title="Go to Remote Sensing of Environment on ScienceDirect" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/remote-sensing-of-environment>Remote Sensing of Environment</a></h2><div class=text-xs><a title="Go to table of contents for this volume/issue" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/remote-sensing-of-environment/vol/103/issue/2>Volume 103, Issue 2</a>, 30 July 2006, Pages 179-189</div></div><div class="publication-cover u-show-from-sm"><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/journal/remote-sensing-of-environment/vol/103/issue/2><img class=publication-cover-image src=data:null;base64, alt="Remote Sensing of Environment"></a></div></div><h1 id=screen-reader-main-title class="Head u-font-serif u-h2 u-margin-s-ver"><span class=title-text>The use of small training sets containing mixed pixels for accurate hard image classification: Training on mixed spectral responses for classification by a SVM</span></h1><div class=Banner id=banner><div class="wrapper truncated"><div class="AuthorGroups text-xs"><div class=author-group id=author-group><span class=sr-only>Author links open overlay panel</span><a class="author size-m workspace-trigger" name=baep-author-id5 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350#!><span class=content><span class="text given-name">Giles M.</span><span class="text surname">Foody</span><span class=author-ref id=baff1><sup>a</sup></span><svg focusable=false viewBox="0 0 106 128" width=19.875 height=24 class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable=false viewBox="0 0 102 128" width=19.125 height=24 class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name=baep-author-id6 href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001350#!><span class=content><span class="text given-name">Ajay</span><span class="text surname">Mathur</span><span class=author-ref id=baff2><sup>b</sup></span></span></a></div></div></div><button id=show-more-btn class="button show-hide-details button-primary" type=button data-aa-button=icon-expand><span class=button-text>Show more</span><svg focusable=false viewBox="0 0 92 128" height=20 width=17.25 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="banner-options u-padding-xs-bottom"><button class="button toc-button button-anchor u-hide-from-lg u-margin-s-right sf-hidden" type=button><svg focusable=false viewBox="0 0 104 128" width=19.5 height=24 class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg></button><button class="button-link AddToMendeley button show-on-desktop button-link-primary" type=button><svg focusable=false viewBox="0 0 86 128" height=16 width=16 class="icon icon-plus"><path d="m48 58v-38h-1e1v38h-38v1e1h38v38h1e1v-38h38v-1e1z"></path></svg><span class=button-link-text>Add to Mendeley</span></button><div class="Social u-display-inline-block" id=social><div class="popover social-popover" id=social-popover><div id=popover-trigger-social-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 128 128" height=16 width=16 class="icon icon-share"><path d="m9e1 112c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm-66-36c-6.62 0-12-5.38-12-12s5.38-12 12-12 12 5.38 12 12-5.38 12-12 12zm66-6e1c6.62 0 12 5.38 12 12s-5.38 12-12 12-12-5.38-12-12 5.38-12 12-12zm0 62c-6.56 0-12.44 2.9-16.48 7.48l-28.42-15.28c0.58-1.98 0.9-4.04 0.9-6.2s-0.32-4.22-0.9-6.2l28.42-15.28c4.04 4.58 9.92 7.48 16.48 7.48 12.14 0 22-9.86 22-22s-9.86-22-22-22-22 9.86-22 22c0 1.98 0.28 3.9 0.78 5.72l-28.64 15.38c-4.02-4.34-9.76-7.1-16.14-7.1-12.14 0-22 9.86-22 22s9.86 22 22 22c6.38 0 12.12-2.76 16.14-7.12l28.64 15.38c-0.5 1.84-0.78 3.76-0.78 5.74 0 12.14 9.86 22 22 22s22-9.86 22-22-9.86-22-22-22z"></path></svg><span class=button-text>Share</span></button></div></div></div><div class="ExportCitation u-display-inline-block" id=export-citation><div class="popover export-citation-popover" id=export-citation-popover><div id=popover-trigger-export-citation-popover><button class="button button-anchor" aria-expanded=false aria-haspopup=true type=button><svg focusable=false viewBox="0 0 106 128" height=16 width=16 class="icon icon-cited-by-66"><path xmlns=http://www.w3.org/2000/svg d="m2 58.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78v-1e1c-25.9 0-44 15.12-44 36.78zm1e2 -26.78v-1e1c-25.9 0-44 15.12-44 36.78v47.22h44v-42h-34v-5.22c0-18.5 17.08-26.78 34-26.78z"></path></svg><span class=button-text>Cite</span></button></div></div></div></div></div><div class=ArticleIdentifierLinks id=article-identifier-links><a class=doi href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.rse.2006.04.001 target=_blank rel="noreferrer noopener" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi-org.simsrad.net.ocs.mq.edu.au/10.1016/j.rse.2006.04.001</a><a class=rights-and-content target=_blank rel="noreferrer noopener" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0034425706001350&amp;orderBeanReset=true">Get rights and content</a></div><section class=ReferencedArticles></section><section class=ReferencedArticles></section><div class=PageDivider></div><div class="Abstracts u-font-serif" id=abstracts><div class="abstract author" id=aep-abstract-id7><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Abstract</h2><div id=aep-abstract-sec-id8><p>The accuracy of a supervised image classification is a function of the training data used in its generation. It is, therefore, critical that the training stage of a supervised classification is designed to provide the necessary information. Guidance on the design of the training stage of a classification typically calls for the use of a large sample of randomly selected pure pixels in order to characterise the classes. Such guidance is generally made without regard to the specific nature of the application in-hand, including the classifier to be used. The design of the training stage should really be based on the classifier to be used since individual training cases can vary in value as can any one training set to a range of classifiers. It is argued here that the training stage can be designed on the basis of the way the classifier operates and with emphasis on the desire to separate the classes rather than describe them. An approach to the training of a support vector machine (SVM) classifier that is the opposite of that generally promoted for training set design is suggested. This approach uses a small sample of mixed spectral responses drawn from purposefully selected locations (geographical boundaries) in training. The approach is based on mixed pixels which are normally masked-out of analyses as undesirable and problematic. A sample of such data should, however, be easier and cheaper to acquire than that suggested by conventional approaches. This new approach to training set design was evaluated against conventional approaches with a set of classifications of agricultural crops from satellite sensor data. The main result was that classifications derived from the use of the mixed spectral responses and the conventional approach did not differ significantly, with the overall accuracy of classifications generally âˆ¼&nbsp;92%.</p></div></div></div><ul id=issue-navigation class="issue-navigation u-margin-s-bottom u-bg-grey1"><li class="previous move-left u-padding-s-ver u-padding-s-left"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001209><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-left"><path d="m1 61l45-45 7 7-38 38 38 38-7 7z"></path></svg><span class=button-alternative-text><strong>Previous </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span></a><li class="next move-right u-padding-s-ver u-padding-s-right"><a class="button-alternative button-alternative-tertiary" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425706001520><span class=button-alternative-text><strong>Next </strong><span class=extra-detail-1>article</span><span class=extra-detail-2> in issue</span></span><svg focusable=false viewBox="0 0 54 128" width=32 height=32 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></a></ul><div class="Keywords u-font-serif"><div id=aep-keywords-id9 class=keywords-section><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">Keywords</h2><div class=keyword><span>Training set</span></div><div class=keyword><span>Mixed pixel</span></div><div class=keyword><span>Support vector machine</span></div><div class=keyword><span>Classification</span></div></div></div><div class="Body u-font-serif" id=body><div><section id=aep-section-id10><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">1. Introduction</h2><p>Supervised classification is one of the most commonly undertaken analyses in remote sensing. The training stage of a supervised image classification provides the class descriptors upon which all allocations are based. Clearly, the quality of the training data set used is of fundamental importance to a classification and a major determinant of classification accuracy. Many studies have shown that the accuracy of a classification varies as a function of a range of training set properties (<a name=bbib9 href=#bib9 class=workspace-trigger>Fardanesh and Ersoy, 1998</a>, <a name=bbib10 href=#bib10 class=workspace-trigger>Foody, 1999</a>, <a name=bbib12 href=#bib12 class=workspace-trigger>Foody and Arora, 1997</a>, <a name=bbib15 href=#bib15 class=workspace-trigger>Foody et al., 1995</a>, <a name=bbib25 href=#bib25 class=workspace-trigger>Staufer and Fischer, 1997</a>, <a name=bbib29 href=#bib29 class=workspace-trigger>Tsai and Philpot, 2002</a>, <a name=bbib33 href=#bib33 class=workspace-trigger>Zhuang et al., 1994</a>). The nature of an ideal training set is, however, unclear, possibly because of uncertainty about the aim of the training stage.<p>Fundamentally, the training stage seeks to provide descriptive statistics for each class in the image that may be used to direct the accurate determination of class membership by the selected classifier. The typical approach to training adopted involves the acquisition of a sample of pixels of known class membership from the image to characterise the classes. These selected pixels represent the training set upon which the remainder of the classification analysis is based. Typically, the descriptive statistics derived from the training pixels are used to characterise the classes and ultimately convey the information needed to partition feature space so that class membership may be determined for all image pixels. Obtaining an accurate description of each class is seen as fundamental to the derivation of an accurate classification (<a name=bbib18 href=#bib18 class=workspace-trigger>Kuo and Landgrebe, 2002</a>, <a name=bbib19 href=#bib19 class=workspace-trigger>Mather, 2004</a>).<p>The way in which the training data are used varies between classifiers. For example, the maximum likelihood classifier uses parameters such as the mean and covariance matrix that summarise the spectral response of each class while a multi-layer perceptron neural network uses each training case directly. Classifiers can, therefore, differ greatly in terms of the training information they require for an accurate classification. A training set that could be used to derive a highly accurate classification from one classifier may yield a considerably lower accuracy if used with another classifier (<a name=bbib10 href=#bib10 class=workspace-trigger>Foody, 1999</a>). Consequently, the nature of the classifier selected for a particular application should inform the design of the training data collection programme. The literature, however, generally promotes a relatively uniform or classifier-independent approach to training set design.<p>The ruling paradigm in the design of the training stage appears to be strongly based on conventional statistics. In this, the training stage is viewed as aiming to derive an accurate description of each class. The descriptive statistics derived may then be used to allocate each pixel of unknown membership to the class with which it has the greatest similarity. Thus, for example, with the maximum likelihood classifier, descriptive statistics that summarize the spectral response of each class are used to determine the most likely class of membership for all pixels in the image. Critically, therefore, key issues in the design of the training stage relate to basic sampling theory and the literature urges the use of rigorous statistical approaches to derive the class descriptive statistics. Major concerns with the use of such an approach are that the training sample acquired should provide a representative and unbiased description of the classes. Thus, some variant of random sampling is typically promoted to help ensure the sample acquired satisfies the assumed conditions. In addition, the sample size required to characterise the spectral response of a class may, assuming it follows a normal distribution, be calculated from<span class=display><span id=fd1 class=formula><span class=label>(1)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-1-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">n</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msup is="true"><mi is="true">&amp;#x3C3;</mi><mn is="true">2</mn></msup><msup is="true"><mi is="true">z</mi><mn is="true">2</mn></msup></mrow><mrow is="true"><msup is="true"><mi is="true">h</mi><mn is="true">2</mn></msup><mo is="true">+</mo><mfrac is="true"><mrow is="true"><msup is="true"><mi is="true">&amp;#x3C3;</mi><mn is="true">2</mn></msup><msup is="true"><mi is="true">z</mi><mn is="true">2</mn></msup></mrow><mi is="true">N</mi></mfrac></mrow></mfrac></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=11.741ex height=5.086ex viewBox="0 -1094.9 5055.3 2189.9" role=img focusable=false style=vertical-align:-2.543ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6E></use></g><g is=true transform=translate(878,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1656,0)><g transform=translate(397,0)><rect stroke=none width=2880 height=60 x=0 y=220></rect><g is=true transform=translate(750,411)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-3C3></use></g><g is=true transform=translate(405,256)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g><g is=true transform=translate(726,0)><g is=true><use transform=scale(0.707) href=#MJMATHI-7A></use></g><g is=true transform=translate(331,256)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(60,-718)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,298)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g><g is=true transform=translate(728,0)><use transform=scale(0.707) href=#MJMAIN-2B></use></g><g is=true transform=translate(1279,0)><g transform=translate(120,0)><rect stroke=none width=1241 height=60 x=0 y=146></rect><g is=true transform=translate(60,335)><g is=true><g is=true><use transform=scale(0.5) href=#MJMATHI-3C3></use></g><g is=true transform=translate(286,144)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g><g is=true transform=translate(586,0)><g is=true><use transform=scale(0.5) href=#MJMATHI-7A></use></g><g is=true transform=translate(234,144)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(398,-319)><use transform=scale(0.5) href=#MJMATHI-4E></use></g></g></g></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>n</mi><mo is=true>=</mo><mfrac is=true><mrow is=true><msup is=true><mi is=true>Ïƒ</mi><mn is=true>2</mn></msup><msup is=true><mi is=true>z</mi><mn is=true>2</mn></msup></mrow><mrow is=true><msup is=true><mi is=true>h</mi><mn is=true>2</mn></msup><mo is=true>+</mo><mfrac is=true><mrow is=true><msup is=true><mi is=true>Ïƒ</mi><mn is=true>2</mn></msup><msup is=true><mi is=true>z</mi><mn is=true>2</mn></msup></mrow><mi is=true>N</mi></mfrac></mrow></mfrac></math></span></span></span></span></span>where <em>h</em> is a specified half-width of the confidence interval, <em>Ïƒ</em> is the planning or estimated value for the population standard deviation, <em>z</em> is the value of the <em>z</em> score at a specified level of confidence and <em>N</em> the size of the population. This equation indicates the sample size required to estimate the mean value of a distribution with a specified degree of precision. For large populations, which given the number of pixels in a typical image data set will generally be the case in remote sensing applications, the equation tends toward that for an infinite population which is<span class=display><span id=fd2 class=formula><span class=label>(2)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-2-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">n</mi><mo is="true">=</mo><mfrac is="true"><mrow is="true"><msup is="true"><mi is="true">&amp;#x3C3;</mi><mn is="true">2</mn></msup><msup is="true"><mi is="true">z</mi><mn is="true">2</mn></msup></mrow><mrow is="true"><msup is="true"><mi is="true">h</mi><mn is="true">2</mn></msup></mrow></mfrac></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=8.532ex height=3.932ex viewBox="0 -1094.9 3673.5 1693.1" role=img focusable=false style=vertical-align:-1.389ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6E></use></g><g is=true transform=translate(878,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(1656,0)><g transform=translate(397,0)><rect stroke=none width=1498 height=60 x=0 y=220></rect><g is=true transform=translate(60,411)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-3C3></use></g><g is=true transform=translate(405,256)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g><g is=true transform=translate(726,0)><g is=true><use transform=scale(0.707) href=#MJMATHI-7A></use></g><g is=true transform=translate(331,256)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(385,-536)><g is=true><g is=true><use transform=scale(0.707) href=#MJMATHI-68></use></g><g is=true transform=translate(407,298)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>n</mi><mo is=true>=</mo><mfrac is=true><mrow is=true><msup is=true><mi is=true>Ïƒ</mi><mn is=true>2</mn></msup><msup is=true><mi is=true>z</mi><mn is=true>2</mn></msup></mrow><mrow is=true><msup is=true><mi is=true>h</mi><mn is=true>2</mn></msup></mrow></mfrac></math></span></span></span></span></span><p>The theory underlying this relationship may also be used to estimate the sample size required for the estimation of a proportion and has been used commonly as the basis for determining sample size requirements for classification applications in remote sensing (e.g. <a name=bbib21 href=#bib21 class=workspace-trigger>Nelson et al., 1984</a>, <a name=bbib28 href=#bib28 class=workspace-trigger>Todd et al., 1980</a>). Alternatively, simple heuristics are often used to determine training sample size (<a name=bbib19 href=#bib19 class=workspace-trigger>Mather, 2004</a>, <a name=bbib30 href=#bib30 class=workspace-trigger>Van Niel et al., 2005</a>). For example, it is often stated that the number of training samples for each class should comprise at least 10â€“30 times the number of wavebands, or other discriminating variables, used in the analysis (<a name=bbib19 href=#bib19 class=workspace-trigger>Mather, 2004</a>, <a name=bbib23 href=#bib23 class=workspace-trigger>Piper, 1992</a>) with a â€˜the larger the betterâ€™ attitude often held.<p>Deviations from the widely used approaches outlined above tend to promote a focus on pure cases of the classes. For example, it is, of course, desirable that only pixels that actually represent an area of the class being described are used in its description. Thus, the pixels selected for training purposes should be pure members of the relevant classes. To achieve this researchers often deliberately mask out or exclude boundary regions where the mixing of class spectral responses may occur (e.g. <a name=bbib2 href=#bib2 class=workspace-trigger>Airkan, 2004</a>). Furthermore, some analysts apply post-acquisition refinement operations to the training data that remove outliers or down-weight the contribution of cases perceived to be atypical of the class being characterised (<a name=bbib1 href=#bib1 class=workspace-trigger>Aria, 1992</a>, <a name=bbib5 href=#bib5 class=workspace-trigger>Buttner et al., 1989</a>, <a name=bbib8 href=#bib8 class=workspace-trigger>Ediriwickrema and Khorram, 1997</a>, <a name=bbib19 href=#bib19 class=workspace-trigger>Mather, 2004</a>). Alternatively, in acquiring training data, some researchers use seed functions (e.g. <a name=bbib26 href=#bib26 class=workspace-trigger>Sun et al., 2003</a>), which do not allow the inclusion of pixels with values greatly different to the seed, to acquire the training set. The use of these post-acquisition refinement operations and seed functions will generally act to shift focus towards the purest exemplars of the classes, what may often be considered to be end members.<p>As a crude summary, the target in training has conventionally been to acquire a large sample of pure pixels to describe the classes. Moreover, it is typically argued that the bigger the training set the better, as this will increase the precision of the estimates made. This latter issue is evident from Eqs. <a name=bfd1 href=#fd1 class=workspace-trigger>(1)</a>, <a name=bfd2 href=#fd2 class=workspace-trigger>(2)</a> where the half-width of the confidence interval, which indicates the precision of the estimate, is inversely related to the sample size. While obtaining an accurate and representative description is typically seen as desirable some approaches for training set definition or refinement adopted may, however, act to bias the description by placing emphasis on cases typical of the class centroid. More fundamentally, however, the basis of the conventional approach to training set design may not be focused on the provision of the critical information needed for an accurate classification.<p>In this article, it is suggested that the conventional paradigm to the design of training data collection programmes may not always be the most appropriate to adopt. Critically, it is argued that the conventional paradigm appears to be focused on describing the classes and in particular the purest cases of the classes. Not only is this approach biased to the purest cases (and to the class centroid) but it also is not meeting the fundamental aim of training. The aim of training is not to accurately describe the classes but to provide information on the classes that will aid the fitting of classification decision boundaries or hyperplanes to separate them. From this perspective, alternative approaches to training may be promoted and are classifier-dependent. Here, we argue that an approach which is essentially the direct opposite to that conventionally used may be employed to yield accurate image classifications. This is argued with reference to classification by a support vector machine (SVM), which has been shown to be at least as accurate as other widely used classifiers in remote sensing (<a name=bbib13 href=#bib13 class=workspace-trigger>Foody and Mathur, 2004a</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>Huang et al., 2002</a>).<div><p>With a SVM the most useful training cases are those that lie close to where the hyperplane is to be fitted (<a name=bbib14 href=#bib14 class=workspace-trigger>Foody and Mathur, 2004b</a>, <a name=bbib31 href=#bib31 class=workspace-trigger>Vapnik, 1995</a>). The hyperplane, of course, lies between the classes and so in the region in which mixtures of the class spectral responses occur (<a name=bfig1 href=#fig1 class=workspace-trigger>Fig. 1</a>). On this basis, mixed spectral responses and mixed pixels may represent the useful and informative training cases for a classification. Moreover, as these are the informative training cases and others, located closer to the class centroids, may have no impact at all on the fitting of the hyperplane only a very small training sample may be required for accurate classification (<a name=bbib14 href=#bib14 class=workspace-trigger>Foody and Mathur, 2004b</a>, <a name=bbib22 href=#bib22 class=workspace-trigger>Pal and Mather, 2005</a>). Thus, for classification by techniques such as SVM we propose that a small training sample, based around mixed pixels, can be used to derive an accurate classification. This proposal will be evaluated by a series of SVM classifications of a remotely sensed data set.<figure class="figure text-xs" id=fig1><span><img src=data:null;base64, height=316 alt><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0034425706001350-gr1.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span><p><span class=label>Fig. 1</span>. A simplified view of the location of pure and mixed class spectral responses in a feature space. Note that between the distributions of the two pure classes, A and B, are distributions associated with simple spectral mixtures of the classes and the hyperplane that optimally separates the classes. Class composition is indicated as the percentage cover of class A:Class B.</p></span></span></figure></div></section><section id=aep-section-id11><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">2. SVM classification</h2><p>SVM based approaches have considerable potential for the supervised classification of remotely sensed data. Comparative studies have shown that classification by a SVM can be more accurate than popular contemporary techniques such as neural networks and decision trees as well as conventional probabilistic classifiers such as the maximum likelihood classification (<a name=bbib13 href=#bib13 class=workspace-trigger>Foody and Mathur, 2004a</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>Huang et al., 2002</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>Melgani and Bruzzone, 2004</a>). Although SVMs were designed for binary classification various methods exist to extend the binary approach to multi-class classification (<a name=bbib16 href=#bib16 class=workspace-trigger>Hsu and Lin, 2002</a>, <a name=bbib17 href=#bib17 class=workspace-trigger>Huang et al., 2002</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>Melgani and Bruzzone, 2004</a>). In all situations, however, the fundamental nature of the SVM is the same.<div><p>Classification by a SVM is based on the fitting of an optimal separating hyperplane between classes by focusing on the training samples that lie at the edge of the class distributions and between the class centroids, the support vectors (<a name=bfig2 href=#fig2 class=workspace-trigger>Fig. 2</a>). All of the other training samples are effectively discarded as they do not contribute to the estimation of hyperplane location (<a name=bbib3 href=#bib3 class=workspace-trigger>Belousov et al., 2002</a>, <a name=bbib4 href=#bib4 class=workspace-trigger>Brown et al., 2000</a>, <a name=bbib32 href=#bib32 class=workspace-trigger>Wang et al., 2005</a>). Thus, only the training samples that lie close to the location of the optimal separating hyperplane are used in its establishment. With SVM based classification, therefore, not only is an optimal hyperplane fitted, in the sense that it is expected to have a large degree of generalizability, but also a high accuracy may be obtained with the use of a small training set (<a name=bbib14 href=#bib14 class=workspace-trigger>Foody and Mathur, 2004b</a>, <a name=bbib32 href=#bib32 class=workspace-trigger>Wang et al., 2005</a>). Given the costs of training data acquisition is often noted as a concern (e.g. <a name=bbib27 href=#bib27 class=workspace-trigger>Tadjudin and Landgrebe, 2000</a>, <a name=bbib6 href=#bib6 class=workspace-trigger>Chi and Bruzzone, 2005</a>), the potential to limit training set size may be an advantageous feature, especially if it is achieved without any significant loss of discriminatory power in the final classification.<figure class="figure text-xs" id=fig2><span><img src=data:null;base64, height=195 alt><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0034425706001350-gr2.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span><p><span class=label>Fig. 2</span>. Basic issues in classification SVM for (A) linearly separable and (B) partially inseparable situations. Note that the support vectors have been highlighted in grey and lie on two hyperplanes that are parallel to the optimal separating hyperplane (OSH).</p></span></span></figure></div><p>In order to appreciate the potential value of mixed pixels as a resource in training it may be helpful to outline the basic features of classification by a SVM. Extensive discussion of classification by a SVM is available in the literature (e.g. <a name=bbib17 href=#bib17 class=workspace-trigger>Huang et al., 2002</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>Melgani and Bruzzone, 2004</a>, <a name=bbib31 href=#bib31 class=workspace-trigger>Vapnik, 1995</a>) but here we draw on an earlier discussion (<a name=bbib13 href=#bib13 class=workspace-trigger>Foody &amp; Mathur, 2004a</a>) to highlight the key issues. The simplest starting point for outlining the nature of SVM based classification is to consider the situation in which there are two classes that are linearly separable in <em>q</em> dimensional space. For each of the <em>r</em> training cases there is a vector, <strong>x</strong><sub><em>i</em></sub>, that represents the spectral response of the case and its location in feature space together with a definition of class membership, <em>y</em><sub><em>i</em></sub>. Using the training data represented by {<strong>x</strong><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>}, <em>i</em>&nbsp;=&nbsp;1,â€¦,<em>r</em>, <em>y</em><sub><em>i</em></sub>&nbsp;âˆˆ&nbsp;{1, âˆ’&nbsp;1}, the aim is to develop a classifier that generalizes accurately. For this, a decision boundary or hyperplane that separates the classes in feature space is required. A large number of candidate hyperplanes could be fitted to separate the classes but there is only one optimal separating hyperplane, which is expected to generalize well in comparison to other possible hyperplanes. This optimal separating hyperplane should lie between the two classes in feature space and be positioned such that all of the samples of a class are located to one side of it and it is located such that the distance to the closest training data samples in both of the classes is as large as possible. In that way the margin between the two classes is maximised.<p>A hyperplane in feature space is defined by the equation <strong>w</strong>Â·<strong>x</strong>&nbsp;+&nbsp;<em>b</em>&nbsp;=&nbsp;0, where <strong>x</strong> is a point lying on the hyperplane, <strong>w</strong> is normal to the hyperplane and <em>b</em> is the bias (<a name=bfig2 href=#fig2 class=workspace-trigger>Fig. 2</a>). A separating hyperplane can be defined for the two classes as: <strong>w</strong>Â·<strong>x</strong><sub><em>i</em></sub>&nbsp;+&nbsp;<em>b</em>&nbsp;â‰¥&nbsp;1 (for the class <em>y</em><sub><em>i</em></sub>&nbsp;=&nbsp;+&nbsp;1) and <strong>w</strong>Â·<strong>x</strong><sub><em>i</em></sub>&nbsp;+&nbsp;<em>b</em>&nbsp;â‰¤&nbsp;âˆ’&nbsp;1 (for the class <em>y</em><sub><em>i</em></sub>&nbsp;=&nbsp;âˆ’&nbsp;1). These two equations may be combined to give<span class=display><span id=fd3 class=formula><span class=label>(3)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-3-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub><mo is="true">(</mo><mi mathvariant="bold" is="true">w</mi><mo is="true">&amp;#xB7;</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">+</mo><mi is="true">b</mi><mo is="true">)</mo><mo is="true">&amp;#x2212;</mo><mn is="true">1</mn><mo is="true">&amp;#x2265;</mo><mn is="true">0</mn></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=21.671ex height=2.779ex viewBox="0 -846.5 9330.5 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><use href=#MJMATHI-79></use></g><g is=true transform=translate(490,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><g is=true transform=translate(834,0)><use href=#MJMAIN-28></use></g><g is=true transform=translate(1224,0)><use href=#MJMAINB-77></use></g><g is=true transform=translate(2278,0)><use href=#MJMAIN-22C5></use></g><g is=true transform=translate(2778,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><g is=true transform=translate(3952,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(4953,0)><use href=#MJMATHI-62></use></g><g is=true transform=translate(5382,0)><use href=#MJMAIN-29></use></g><g is=true transform=translate(5994,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(6995,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(7773,0)><use href=#MJMAIN-2265></use></g><g is=true transform=translate(8829,0)><use href=#MJMAIN-30></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub><mo is=true>(</mo><mi mathvariant=bold is=true>w</mi><mo is=true>Â·</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>+</mo><mi is=true>b</mi><mo is=true>)</mo><mo is=true>âˆ’</mo><mn is=true>1</mn><mo is=true>â‰¥</mo><mn is=true>0</mn></math></span></span></span></span></span><p>The training samples on these two hyperplanes are termed the support vectors and are central to the establishment of the optimal separating hyperplane on which SVM classification is based.<p>The support vectors of the two classes lie on two hyperplanes, P1 and P2, which themselves are parallel to the optimal hyperplane and are defined by <strong>w</strong>Â·<strong>x</strong><sub><em>i</em></sub>&nbsp;+&nbsp;<em>b</em>&nbsp;=&nbsp;Â±&nbsp;1 (<a name=bfig2 href=#fig2 class=workspace-trigger>Fig. 2</a>). The margin between these planes is <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-4-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mfrac is="true"><mn is="true">2</mn><mrow is="true"><mo is="true">|</mo><mi mathvariant="bold" is="true">w</mi><mo is="true">|</mo></mrow></mfrac></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=3.116ex height=3.817ex viewBox="0 -945.9 1341.8 1643.4" role=img focusable=false style=vertical-align:-1.62ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g transform=translate(120,0)><rect stroke=none width=1101 height=60 x=0 y=220></rect><g is=true transform=translate(373,403)><use transform=scale(0.707) href=#MJMAIN-32></use></g><g is=true transform=translate(60,-435)><g is=true><use transform=scale(0.707) href=#MJMAIN-7C></use></g><g is=true transform=translate(196,0)><use transform=scale(0.707) href=#MJMAINB-77></use></g><g is=true transform=translate(784,0)><use transform=scale(0.707) href=#MJMAIN-7C></use></g></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mfrac is=true><mn is=true>2</mn><mrow is=true><mo is=true>|</mo><mi mathvariant=bold is=true>w</mi><mo is=true>|</mo></mrow></mfrac></math></span></span></span> and the analysis aims to maximise this margin through the constrained optimization problem,<span class=display><span class=formula><span class=label>(4)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-5-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">min</mi><mo is="true">{</mo><mfrac is="true"><mn is="true">1</mn><mn is="true">2</mn></mfrac><msup is="true"><mrow is="true"><mo stretchy="true" is="true">|</mo><mi mathvariant="bold" is="true">w</mi><mo stretchy="true" is="true">|</mo></mrow><mn is="true">2</mn></msup><mo is="true">}</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=12.913ex height=4.625ex viewBox="0 -1244 5559.8 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMAIN-6D></use><use href=#MJMAIN-69 x=833 y=0></use><use href=#MJMAIN-6E x=1112 y=0></use></g><use href=#MJSZ2-7B is=true x=1668 y=-1></use><g is=true transform=translate(2336,0)><g transform=translate(120,0)><rect stroke=none width=473 height=60 x=0 y=220></rect><g is=true transform=translate(60,403)><use transform=scale(0.707) href=#MJMAIN-31></use></g><g is=true transform=translate(60,-375)><use transform=scale(0.707) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(3049,0)><g is=true><g is=true><use href=#MJMAIN-7C></use></g><g is=true transform=translate(278,0)><use href=#MJMAINB-77></use></g><g is=true transform=translate(1110,0)><use href=#MJMAIN-7C></use></g></g><g is=true transform=translate(1388,477)><use transform=scale(0.707) href=#MJMAIN-32></use></g></g><use href=#MJSZ2-7D is=true x=4892 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>min</mi><mo is=true>{</mo><mfrac is=true><mn is=true>1</mn><mn is=true>2</mn></mfrac><msup is=true><mrow is=true><mo stretchy=true is=true>|</mo><mi mathvariant=bold is=true>w</mi><mo stretchy=true is=true>|</mo></mrow><mn is=true>2</mn></msup><mo is=true>}</mo></math></span></span></span></span></span>under the inequality constraints of Eq. <a name=bfd3 href=#fd3 class=workspace-trigger>(3)</a>.<p>If the classes are not linearly separable, slack variables, {<em>Î¾</em><sub><em>i</em></sub>}<sub><em>i</em>=1</sub><sup><em>r</em></sup>, that indicate the distance the sample is from the hyperplane P1 or P2 passing through the support vectors of the class to which the sample belongs (<a name=bfig2 href=#fig2 class=workspace-trigger>Fig. 2</a>), and so the amount of violation of the constraints allowed, are introduced (<a name=bbib7 href=#bib7 class=workspace-trigger>Cortes &amp; Vapnik, 1995</a>). With this addition, Eq. <a name=bfd3 href=#fd3 class=workspace-trigger>(3)</a> may be rewritten as,<span class=display><span id=fd4 class=formula><span class=label>(5)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-6-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">y</mi><mo is="true">(</mo><mi mathvariant="bold" is="true">w</mi><mo is="true">&amp;#xB7;</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">+</mo><mi is="true">b</mi><mo is="true">)</mo><mo is="true">&amp;gt;</mo><mn is="true">1</mn><mo is="true">&amp;#x2212;</mo><msub is="true"><mi is="true">&amp;#x3BE;</mi><mi is="true">i</mi></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=21.543ex height=2.779ex viewBox="0 -846.5 9275.5 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-79></use></g><g is=true transform=translate(497,0)><use href=#MJMAIN-28></use></g><g is=true transform=translate(887,0)><use href=#MJMAINB-77></use></g><g is=true transform=translate(1940,0)><use href=#MJMAIN-22C5></use></g><g is=true transform=translate(2441,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><g is=true transform=translate(3615,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(4616,0)><use href=#MJMATHI-62></use></g><g is=true transform=translate(5045,0)><use href=#MJMAIN-29></use></g><g is=true transform=translate(5712,0)><use href=#MJMAIN-3E></use></g><g is=true transform=translate(6769,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(7491,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(8492,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>y</mi><mo is=true>(</mo><mi mathvariant=bold is=true>w</mi><mo is=true>Â·</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>+</mo><mi is=true>b</mi><mo is=true>)</mo><mo is=true>&gt;</mo><mn is=true>1</mn><mo is=true>âˆ’</mo><msub is=true><mi is=true>Î¾</mi><mi is=true>i</mi></msub></math></span></span></span></span></span><p>If outliers exist in the data set, Eq. <a name=bfd4 href=#fd4 class=workspace-trigger>(5)</a> can always be satisfied by making <em>Î¾</em><sub><em>i</em></sub> very large and so, a penalty term, <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-7-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">C</mi><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">r</mi></munderover><msub is="true"><mi is="true">&amp;#x3BE;</mi><mi is="true">i</mi></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=9.713ex height=2.779ex viewBox="0 -846.5 4181.8 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-43></use></g><g is=true transform=translate(927,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-72></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(3399,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>C</mi><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>r</mi></munderover><msub is=true><mi is=true>Î¾</mi><mi is=true>i</mi></msub></math></span></span></span> is added to penalize solutions for which <em>Î¾</em><sub><em>i</em></sub> are very large. The constant <em>C</em> controls the magnitude of the penalty associated with training samples that lie on the wrong side of the hyperplane. The value of <em>C</em> must be set by the analyst and requires careful selection to avoid problems such as over-fitting that may limit the classifier's generalization capacity. Although the parameter <em>C</em> must, therefore, be selected with care SVM based classification has been shown to display a large degree of robustness to variation in parameter values (<a name=bbib3 href=#bib3 class=workspace-trigger>Belousov et al., 2002</a>). With the addition of the penalty term to the analysis, the optimization problem becomes,<span class=display><span id=fd5 class=formula><span class=label>(6)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-8-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">min</mi><mo is="true">[</mo><mfrac is="true"><mrow is="true"><mo is="true">|</mo><mi mathvariant="bold" is="true">w</mi><msup is="true"><mo is="true">|</mo><mn is="true">2</mn></msup></mrow><mn is="true">2</mn></mfrac><mo is="true">+</mo><mi is="true">C</mi><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">r</mi></munderover><msub is="true"><mi is="true">&amp;#x3BE;</mi><mi is="true">i</mi></msub><mo is="true">]</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=22.745ex height=6.009ex viewBox="0 -1542.1 9793.1 2587.3" role=img focusable=false style=vertical-align:-2.428ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMAIN-6D></use><use href=#MJMAIN-69 x=833 y=0></use><use href=#MJMAIN-6E x=1112 y=0></use></g><use href=#MJSZ3-5B is=true x=1668 y=-1></use><g is=true transform=translate(2197,0)><g transform=translate(120,0)><rect stroke=none width=1422 height=60 x=0 y=220></rect><g is=true transform=translate(60,721)><g is=true transform=translate(0,671)><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-752></use><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-1150></use></g><g is=true transform=translate(196,0)><use transform=scale(0.707) href=#MJMAINB-77></use></g><g is=true transform=translate(784,0)><g is=true><use transform=scale(0.707) href=#MJMAIN-7C></use></g><g is=true transform=translate(196,337)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(534,-375)><use transform=scale(0.707) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(4082,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(5082,0)><use href=#MJMATHI-43></use></g><g is=true transform=translate(6009,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-72></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(8481,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJSZ3-5D is=true x=9264 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>min</mi><mo is=true>[</mo><mfrac is=true><mrow is=true><mo is=true>|</mo><mi mathvariant=bold is=true>w</mi><msup is=true><mo is=true>|</mo><mn is=true>2</mn></msup></mrow><mn is=true>2</mn></mfrac><mo is=true>+</mo><mi is=true>C</mi><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>r</mi></munderover><msub is=true><mi is=true>Î¾</mi><mi is=true>i</mi></msub><mo is=true>]</mo></math></span></span></span></span></span>under the constraints of Eq. <a name=bfd4 href=#fd4 class=workspace-trigger>(5)</a>. Note that the first part of Eq. <a name=bfd5 href=#fd5 class=workspace-trigger>(6)</a> seeks to maximize the margin between the classes while the second part aims to penalize the samples located on the incorrect side of the hyperplane with <em>C</em> controlling the relative balance of these two competing objectives. If the classes overlap considerably in feature space, then <span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-9-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">C</mi><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">r</mi></munderover><msub is="true"><mi is="true">&amp;#x3BE;</mi><mi is="true">i</mi></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=9.713ex height=2.779ex viewBox="0 -846.5 4181.8 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-43></use></g><g is=true transform=translate(927,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-72></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(3399,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>C</mi><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>r</mi></munderover><msub is=true><mi is=true>Î¾</mi><mi is=true>i</mi></msub></math></span></span></span> can be very large and the optimal hyperplane may be expected to have limited generalization ability.<p>The basic approach to SVM classification may be extended to allow for non-linear decision surfaces. For this situation, the input data are mapped into a high dimensional space through some non-linear mapping which has the effect of spreading the distribution of the data points in a way that facilitates the fitting of a linear hyperplane. The classification decision function is then<span class=display><span class=formula><span class=label>(7)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-10-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">f</mi><mo is="true">(</mo><mi is="true">x</mi><mo is="true">)</mo><mo is="true">=</mo><mi is="true">sgn</mi><mo stretchy="true" is="true">(</mo><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">r</mi></munderover><msub is="true"><mi is="true">&amp;#x3B1;</mi><mi is="true">i</mi></msub><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub><mi is="true">k</mi><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><mo is="true">+</mo><mi is="true">b</mi><mo stretchy="true" is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=34.178ex height=2.779ex viewBox="0 -846.5 14715.4 1196.3" role=img focusable=false style=vertical-align:-0.812ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-66></use></g><use href=#MJMAIN-28 is=true x=550 y=0></use><g is=true transform=translate(940,0)><use href=#MJMATHI-78></use></g><use href=#MJMAIN-29 is=true x=1512 y=0></use><g is=true transform=translate(2179,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(3236,0)><use href=#MJMAIN-73></use><use href=#MJMAIN-67 x=394 y=0></use><use href=#MJMAIN-6E x=895 y=0></use></g><use href=#MJMAIN-28 is=true x=4687 y=0></use><g is=true transform=translate(5077,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-72></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(7548,0)><g is=true><use href=#MJMATHI-3B1></use></g><g is=true transform=translate(640,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><g is=true transform=translate(8533,0)><g is=true><use href=#MJMATHI-79></use></g><g is=true transform=translate(490,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><g is=true transform=translate(9368,0)><use href=#MJMATHI-6B></use></g><use href=#MJMAIN-28 is=true x=9890 y=0></use><g is=true transform=translate(10279,0)><use href=#MJMAINB-78></use></g><g is=true transform=translate(10887,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(11332,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJMAIN-29 is=true x=12284 y=0></use><g is=true transform=translate(12895,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(13896,0)><use href=#MJMATHI-62></use></g><use href=#MJMAIN-29 is=true x=14325 y=0></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>f</mi><mo is=true>(</mo><mi is=true>x</mi><mo is=true>)</mo><mo is=true>=</mo><mi is=true>sgn</mi><mo stretchy=true is=true>(</mo><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>r</mi></munderover><msub is=true><mi is=true>Î±</mi><mi is=true>i</mi></msub><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub><mi is=true>k</mi><mo is=true>(</mo><mi mathvariant=bold is=true>x</mi><mo is=true>,</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><mo is=true>+</mo><mi is=true>b</mi><mo stretchy=true is=true>)</mo></math></span></span></span></span></span>where <em>Î±</em><sub><em>i</em></sub>, <em>i</em>&nbsp;=&nbsp;1,â€¦,<em>r</em> are lagrange multipliers and <em>k</em>(<strong>x</strong>,<strong>x</strong><sub><em>i</em></sub>) is a kernel function (<a name=bbib32 href=#bib32 class=workspace-trigger>Wang et al., 2005</a>). The magnitude of <em>Î±</em><sub><em>i</em></sub> is determined by the parameter <em>C</em> and indicates the contribution made by the training samples to the fitting of the optimal separating hyperplane with values and lying within the range of 0â€“<em>C</em> (<a name=bbib3 href=#bib3 class=workspace-trigger>Belousov et al., 2002</a>). The kernel used must meet Mercer's condition (<a name=bbib31 href=#bib31 class=workspace-trigger>Vapnik, 1995</a>) and one such kernel which is used in this research is the radial basis function,<span class=display><span class=formula><span class=label>(8)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-11-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">k</mi><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">,</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><mo is="true">=</mo><msup is="true"><mi is="true">e</mi><mrow is="true"><mo is="true">&amp;#x2212;</mo><mi is="true">&amp;#x3B3;</mi><mo is="true">|</mo><mo is="true">(</mo><mi mathvariant="bold" is="true">x</mi><mo is="true">&amp;#x2212;</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><msup is="true"><mo is="true">|</mo><mn is="true">2</mn></msup></mrow></msup></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=23.336ex height=6.009ex viewBox="0 -1542.1 10047.2 2587.3" role=img focusable=false style=vertical-align:-2.428ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMATHI-6B></use></g><use href=#MJSZ3-28 is=true x=521 y=-1></use><g is=true transform=translate(1258,0)><use href=#MJMAINB-78></use></g><g is=true transform=translate(1865,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(2310,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJSZ3-29 is=true x=3262 y=-1></use><g is=true transform=translate(4276,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5333,0)><g is=true><use href=#MJMATHI-65></use></g><g is=true transform=translate(466,567)><g is=true><use transform=scale(0.707) href=#MJMAIN-2212></use></g><g is=true transform=translate(550,0)><use transform=scale(0.707) href=#MJMATHI-3B3></use></g><g is=true transform=translate(934,671)><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-752></use><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-1150></use></g><use transform=scale(0.707) href=#MJSZ2-28 is=true x=1600 y=-1></use><g is=true transform=translate(1554,0)><use transform=scale(0.707) href=#MJMAINB-78></use></g><g is=true transform=translate(1983,0)><use transform=scale(0.707) href=#MJMAIN-2212></use></g><g is=true transform=translate(2534,0)><g is=true><use transform=scale(0.707) href=#MJMAINB-78></use></g><g is=true transform=translate(429,-107)><use transform=scale(0.5) href=#MJMATHI-69></use></g></g><use transform=scale(0.707) href=#MJSZ2-29 is=true x=4535 y=-1></use><g is=true transform=translate(3629,0)><g is=true transform=translate(0,671)><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-752></use><use transform=scale(0.707) href=#MJMAIN-2223 x=0 y=-1150></use></g><g is=true transform=translate(196,478)><use transform=scale(0.5) href=#MJMAIN-32></use></g></g></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>k</mi><mo is=true>(</mo><mi mathvariant=bold is=true>x</mi><mo is=true>,</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><mo is=true>=</mo><msup is=true><mi is=true>e</mi><mrow is=true><mo is=true>âˆ’</mo><mi is=true>Î³</mi><mo is=true>|</mo><mo is=true>(</mo><mi mathvariant=bold is=true>x</mi><mo is=true>âˆ’</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><msup is=true><mo is=true>|</mo><mn is=true>2</mn></msup></mrow></msup></math></span></span></span></span></span>the width of which is controlled by the parameter <em>Î³</em>.<p>The basic SVM approach for binary classification may be extended for multi-class classifications. Commonly, this has been achieved using either the one-against-one or one-against-all strategies in which the multi-class classification problem is reduced to a set of binary problems (<a name=bbib17 href=#bib17 class=workspace-trigger>Huang et al., 2002</a>, <a name=bbib20 href=#bib20 class=workspace-trigger>Melgani and Bruzzone, 2004</a>). Alternatively, however, a multi-class SVM may be used (<a name=bbib16 href=#bib16 class=workspace-trigger>Hsu &amp; Lin, 2002</a>). One multi-class SVM which is similar to the one-against-all approach involves the solution of a single optimisation problem and was used in the research reported below. With this approach, for an <em>c</em> class problem, <em>c</em> two class rules where the <em>m</em>th function <strong>w</strong><sub><em>m</em></sub><sup><em>T</em></sup>(<strong>x</strong>)&nbsp;+&nbsp;<em>b</em> separates the training data vectors of class <em>m</em> from that of others are constructed. Hence, there are <em>c</em> decision functions or hyperplanes but all are obtained by solving one problem,<span class=display><span class=formula><span class=label>(9)</span><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-12-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><munder is="true"><mi is="true">min</mi><mrow is="true"><mi is="true">w</mi><mo is="true">,</mo><mi is="true">b</mi><mo is="true">,</mo><mi is="true">&amp;#x3BE;</mi></mrow></munder><mfrac is="true"><mn is="true">1</mn><mn is="true">2</mn></mfrac><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">m</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">c</mi></munderover><msubsup is="true"><mi mathvariant="bold" is="true">w</mi><mi is="true">m</mi><mi is="true">T</mi></msubsup><msub is="true"><mi mathvariant="bold" is="true">w</mi><mi is="true">m</mi></msub><mo is="true">+</mo><mi is="true">C</mi><munderover is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mi is="true">l</mi></munderover><munder is="true"><mo is="true">&amp;#x2211;</mo><mrow is="true"><mi is="true">m</mi><mo is="true">&amp;#x2260;</mo><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub></mrow></munder><msub is="true"><mi is="true">&amp;#x3BE;</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">m</mi></mrow></msub><mtext is="true">,</mtext></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=42.402ex height=4.74ex viewBox="0 -1045.3 18256.4 2040.9" role=img focusable=false style=vertical-align:-2.312ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><use href=#MJMAIN-6D></use><use href=#MJMAIN-69 x=833 y=0></use><use href=#MJMAIN-6E x=1112 y=0></use></g><g is=true transform=translate(75,-667)><g is=true><use transform=scale(0.707) href=#MJMATHI-77></use></g><g is=true transform=translate(506,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(703,0)><use transform=scale(0.707) href=#MJMATHI-62></use></g><g is=true transform=translate(1007,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1204,0)><use transform=scale(0.707) href=#MJMATHI-3BE></use></g></g></g><g is=true transform=translate(1668,0)><g transform=translate(286,0)><rect stroke=none width=473 height=60 x=0 y=220></rect><g is=true transform=translate(60,403)><use transform=scale(0.707) href=#MJMAIN-31></use></g><g is=true transform=translate(60,-375)><use transform=scale(0.707) href=#MJMAIN-32></use></g></g></g><g is=true transform=translate(2715,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-63></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-6D></use></g><g is=true transform=translate(621,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(1171,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(5564,0)><g is=true><use href=#MJMAINB-77></use></g><g is=true transform=translate(831,345)><use transform=scale(0.707) href=#MJMATHI-54></use></g><g is=true transform=translate(831,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><g is=true transform=translate(7117,0)><g is=true><use href=#MJMAINB-77></use></g><g is=true transform=translate(831,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><g is=true transform=translate(8892,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(9892,0)><use href=#MJMATHI-43></use></g><g is=true transform=translate(10819,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,477)><use transform=scale(0.707) href=#MJMATHI-6C></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(794,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g></g></g><g is=true transform=translate(13291,0)><g is=true><use href=#MJSZ1-2211></use></g><g is=true transform=translate(1056,-287)><g is=true><use transform=scale(0.707) href=#MJMATHI-6D></use></g><g is=true transform=translate(621,0)><use transform=scale(0.707) href=#MJMAIN-2260></use></g><g is=true transform=translate(1171,0)><g is=true><use transform=scale(0.707) href=#MJMATHI-79></use></g><g is=true transform=translate(346,-171)><use transform=scale(0.5) href=#MJMATHI-69></use></g></g></g></g><g is=true transform=translate(16376,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(441,0)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g></g><g is=true transform=translate(17977,0)><use href=#MJMAIN-2C></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><munder is=true><mi is=true>min</mi><mrow is=true><mi is=true>w</mi><mo is=true>,</mo><mi is=true>b</mi><mo is=true>,</mo><mi is=true>Î¾</mi></mrow></munder><mfrac is=true><mn is=true>1</mn><mn is=true>2</mn></mfrac><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>m</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>c</mi></munderover><msubsup is=true><mi mathvariant=bold is=true>w</mi><mi is=true>m</mi><mi is=true>T</mi></msubsup><msub is=true><mi mathvariant=bold is=true>w</mi><mi is=true>m</mi></msub><mo is=true>+</mo><mi is=true>C</mi><munderover is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn></mrow><mi is=true>l</mi></munderover><munder is=true><mo is=true>âˆ‘</mo><mrow is=true><mi is=true>m</mi><mo is=true>â‰ </mo><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub></mrow></munder><msub is=true><mi is=true>Î¾</mi><mrow is=true><mi is=true>i</mi><mo is=true>,</mo><mi is=true>m</mi></mrow></msub><mtext is=true>,</mtext></math></span></span></span></span></span>under the constraints,<span class=display><span class=formula><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-13-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup is="true"><mi mathvariant="bold" is="true">w</mi><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub><mi is="true">T</mi></msubsup><mi mathvariant="bold" is="true">&amp;#x3D5;</mi><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><mo is="true">+</mo><msub is="true"><mi is="true">b</mi><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub></msub><mo is="true">&amp;#x2265;</mo><msubsup is="true"><mi mathvariant="bold" is="true">w</mi><mi is="true">m</mi><mi is="true">T</mi></msubsup><mi mathvariant="bold" is="true">&amp;#x3D5;</mi><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><mo is="true">+</mo><msub is="true"><mi is="true">b</mi><mi is="true">m</mi></msub><mo is="true">+</mo><mn is="true">2</mn><mo is="true">&amp;#x2212;</mo><msub is="true"><mi is="true">&amp;#x3BE;</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">m</mi></mrow></msub><mtext is="true">,</mtext></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=44.392ex height=3.24ex viewBox="0 -945.9 19113.4 1395" role=img focusable=false style=vertical-align:-1.043ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><use href=#MJMAINB-77></use></g><g is=true transform=translate(831,345)><use transform=scale(0.707) href=#MJMATHI-54></use></g><g is=true transform=translate(831,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-79></use></g><g is=true transform=translate(346,-171)><use transform=scale(0.5) href=#MJMATHI-69></use></g></g></g><g is=true transform=translate(1521,0)><use href=#MJMATHBI-3D5></use></g><use href=#MJSZ1-28 is=true x=2234 y=-1></use><g is=true transform=translate(2692,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJSZ1-29 is=true x=3644 y=-1></use><g is=true transform=translate(4325,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(5326,0)><g is=true><use href=#MJMATHI-62></use></g><g is=true transform=translate(429,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-79></use></g><g is=true transform=translate(346,-171)><use transform=scale(0.5) href=#MJMATHI-69></use></g></g></g><g is=true transform=translate(6723,0)><use href=#MJMAIN-2265></use></g><g is=true transform=translate(7779,0)><g is=true><use href=#MJMAINB-77></use></g><g is=true transform=translate(831,345)><use transform=scale(0.707) href=#MJMATHI-54></use></g><g is=true transform=translate(831,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><g is=true transform=translate(9332,0)><use href=#MJMATHBI-3D5></use></g><use href=#MJSZ1-28 is=true x=10045 y=-1></use><g is=true transform=translate(10503,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJSZ1-29 is=true x=11455 y=-1></use><g is=true transform=translate(12136,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(13136,0)><g is=true><use href=#MJMATHI-62></use></g><g is=true transform=translate(429,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><g is=true transform=translate(14509,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(15510,0)><use href=#MJMAIN-32></use></g><g is=true transform=translate(16233,0)><use href=#MJMAIN-2212></use></g><g is=true transform=translate(17233,0)><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(441,0)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g></g><g is=true transform=translate(18834,0)><use href=#MJMAIN-2C></use></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msubsup is=true><mi mathvariant=bold is=true>w</mi><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub><mi is=true>T</mi></msubsup><mi mathvariant=bold is=true>Ï•</mi><mo is=true>(</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><mo is=true>+</mo><msub is=true><mi is=true>b</mi><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub></msub><mo is=true>â‰¥</mo><msubsup is=true><mi mathvariant=bold is=true>w</mi><mi is=true>m</mi><mi is=true>T</mi></msubsup><mi mathvariant=bold is=true>Ï•</mi><mo is=true>(</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><mo is=true>+</mo><msub is=true><mi is=true>b</mi><mi is=true>m</mi></msub><mo is=true>+</mo><mn is=true>2</mn><mo is=true>âˆ’</mo><msub is=true><mi is=true>Î¾</mi><mrow is=true><mi is=true>i</mi><mo is=true>,</mo><mi is=true>m</mi></mrow></msub><mtext is=true>,</mtext></math></span></span></span></span></span><span class=display><span class=formula><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-14-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><msub is="true"><mi is="true">&amp;#x3BE;</mi><mrow is="true"><mi is="true">i</mi><mo is="true">,</mo><mi is="true">m</mi></mrow></msub><mo is="true">&amp;#x2265;</mo><mn is="true">0</mn><mo is="true">,</mo><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">.</mo><mo is="true">.</mo><mo is="true">,</mo><mi is="true">l</mi><mo is="true">,</mo><mi is="true">m</mi><mo is="true">&amp;#x2208;</mo><mo is="true">{</mo><mn is="true">1</mn><mo is="true">,</mo><mo is="true">&amp;#x2026;</mo><mo is="true">.</mo><mi is="true">c</mi><mo is="true">}</mo><mi is="true">&amp;#x29F9;</mi><msub is="true"><mi is="true">y</mi><mi is="true">i</mi></msub></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=41.989ex height=4.625ex viewBox="0 -1244 18078.4 1991.2" role=img focusable=false style=vertical-align:-1.735ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><g is=true><use href=#MJMATHI-3BE></use></g><g is=true transform=translate(438,-150)><g is=true><use transform=scale(0.707) href=#MJMATHI-69></use></g><g is=true transform=translate(244,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(441,0)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g></g><g is=true transform=translate(1878,0)><use href=#MJMAIN-2265></use></g><g is=true transform=translate(2934,0)><use href=#MJMAIN-30></use></g><g is=true transform=translate(3435,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(3880,0)><use href=#MJMATHI-69></use></g><g is=true transform=translate(4503,0)><use href=#MJMAIN-3D></use></g><g is=true transform=translate(5560,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(6060,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(6505,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(7845,0)><use href=#MJMAIN-2E></use></g><g is=true transform=translate(8290,0)><use href=#MJMAIN-2E></use></g><g is=true transform=translate(8735,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9180,0)><use href=#MJMATHI-6C></use></g><g is=true transform=translate(9479,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(9924,0)><use href=#MJMATHI-6D></use></g><g is=true transform=translate(11080,0)><use href=#MJMAIN-2208></use></g><use href=#MJSZ2-7B is=true x=12025 y=-1></use><g is=true transform=translate(12693,0)><use href=#MJMAIN-31></use></g><g is=true transform=translate(13193,0)><use href=#MJMAIN-2C></use></g><g is=true transform=translate(13638,0)><use href=#MJMAIN-2026></use></g><g is=true transform=translate(14978,0)><use href=#MJMAIN-2E></use></g><g is=true transform=translate(15423,0)><use href=#MJMATHI-63></use></g><use href=#MJSZ2-7D is=true x=15856 y=-1></use><g is=true transform=translate(16524,0)><text font-family="STIXGeneral,'Arial Unicode MS',serif" font-style=italic stroke=none transform="scale(55.199) matrix(1 0 0 -1 0 0)">â§¹</text></g><g is=true transform=translate(17243,0)><g is=true><use href=#MJMATHI-79></use></g><g is=true transform=translate(490,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><msub is=true><mi is=true>Î¾</mi><mrow is=true><mi is=true>i</mi><mo is=true>,</mo><mi is=true>m</mi></mrow></msub><mo is=true>â‰¥</mo><mn is=true>0</mn><mo is=true>,</mo><mi is=true>i</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>â€¦</mo><mo is=true>.</mo><mo is=true>.</mo><mo is=true>,</mo><mi is=true>l</mi><mo is=true>,</mo><mi is=true>m</mi><mo is=true>âˆˆ</mo><mo is=true>{</mo><mn is=true>1</mn><mo is=true>,</mo><mo is=true>â€¦</mo><mo is=true>.</mo><mi is=true>c</mi><mo is=true>}</mo><mi is=true>â§¹</mi><msub is=true><mi is=true>y</mi><mi is=true>i</mi></msub></math></span></span></span></span></span>where <em>i</em>&nbsp;=&nbsp;1,â€¦,<em>l</em> are the training data vectors. The decision function is then,<span class=display><span class=formula><span class=math><span class=MathJax_Preview></span><span class=MathJax_SVG id=MathJax-Element-15-Frame tabindex=0 data-mathml='<math xmlns="http://www.w3.org/1998/Math/MathML"><mi is="true">arg</mi><munder is="true"><mi is="true">max</mi><mrow is="true"><mi is="true">m</mi><mo is="true">=</mo><mn is="true">1</mn><mo is="true">,</mo><mi is="true">c</mi></mrow></munder><mo is="true">(</mo><msubsup is="true"><mi mathvariant="bold" is="true">w</mi><mi is="true">m</mi><mi is="true">T</mi></msubsup><mi mathvariant="bold" is="true">&amp;#x3D5;</mi><mo is="true">(</mo><msub is="true"><mi mathvariant="bold" is="true">x</mi><mi is="true">i</mi></msub><mo is="true">)</mo><mo is="true">+</mo><msub is="true"><mi is="true">b</mi><mi is="true">m</mi></msub><mo is="true">)</mo></math>' role=presentation style=font-size:90%;display:inline-block;position:relative><svg xmlns:xlink=http://www.w3.org/1999/xlink width=28.163ex height=6.009ex viewBox="0 -1542.1 12125.8 2587.3" role=img focusable=false style=vertical-align:-2.428ex aria-hidden=true><g stroke=currentColor fill=currentColor stroke-width=0 transform="matrix(1 0 0 -1 0 0)"><g is=true><use href=#MJMAIN-61></use><use href=#MJMAIN-72 x=500 y=0></use><use href=#MJMAIN-67 x=893 y=0></use></g><g is=true transform=translate(1560,0)><g is=true transform=translate(83,0)><use href=#MJMAIN-6D></use><use href=#MJMAIN-61 x=833 y=0></use><use href=#MJMAIN-78 x=1334 y=0></use></g><g is=true transform=translate(0,-651)><g is=true><use transform=scale(0.707) href=#MJMATHI-6D></use></g><g is=true transform=translate(621,0)><use transform=scale(0.707) href=#MJMAIN-3D></use></g><g is=true transform=translate(1171,0)><use transform=scale(0.707) href=#MJMAIN-31></use></g><g is=true transform=translate(1525,0)><use transform=scale(0.707) href=#MJMAIN-2C></use></g><g is=true transform=translate(1722,0)><use transform=scale(0.707) href=#MJMATHI-63></use></g></g></g><use href=#MJSZ3-28 is=true x=3589 y=-1></use><g is=true transform=translate(4325,0)><g is=true><use href=#MJMAINB-77></use></g><g is=true transform=translate(831,345)><use transform=scale(0.707) href=#MJMATHI-54></use></g><g is=true transform=translate(831,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><g is=true transform=translate(5878,0)><use href=#MJMATHBI-3D5></use></g><use href=#MJSZ3-28 is=true x=6590 y=-1></use><g is=true transform=translate(7327,0)><g is=true><use href=#MJMAINB-78></use></g><g is=true transform=translate(607,-150)><use transform=scale(0.707) href=#MJMATHI-69></use></g></g><use href=#MJSZ3-29 is=true x=8279 y=-1></use><g is=true transform=translate(9237,0)><use href=#MJMAIN-2B></use></g><g is=true transform=translate(10238,0)><g is=true><use href=#MJMATHI-62></use></g><g is=true transform=translate(429,-150)><use transform=scale(0.707) href=#MJMATHI-6D></use></g></g><use href=#MJSZ3-29 is=true x=11389 y=-1></use></g></svg><span class=MJX_Assistive_MathML role=presentation><math xmlns=http://www.w3.org/1998/Math/MathML><mi is=true>arg</mi><munder is=true><mi is=true>max</mi><mrow is=true><mi is=true>m</mi><mo is=true>=</mo><mn is=true>1</mn><mo is=true>,</mo><mi is=true>c</mi></mrow></munder><mo is=true>(</mo><msubsup is=true><mi mathvariant=bold is=true>w</mi><mi is=true>m</mi><mi is=true>T</mi></msubsup><mi mathvariant=bold is=true>Ï•</mi><mo is=true>(</mo><msub is=true><mi mathvariant=bold is=true>x</mi><mi is=true>i</mi></msub><mo is=true>)</mo><mo is=true>+</mo><msub is=true><mi is=true>b</mi><mi is=true>m</mi></msub><mo is=true>)</mo></math></span></span></span></span></span><p>By reducing the classification to a single optimization problem this approach may also require fewer support vectors than a multi-class classification based on the combined use of many binary SVMs (<a name=bbib16 href=#bib16 class=workspace-trigger>Hsu &amp; Lin, 2002</a>), an advantage when wishing to constrain training set size. Additionally, with the multi-class SVM approach to classification the values for the parameters <em>C</em> and <em>Î³</em> need only to be defined once.<p>The accuracy with which a SVM may classify a data set is dependent on the magnitude of the parameters <em>C</em> and <em>Î³</em>. With a large value of <em>Î³</em> and/or <em>C</em>, there is a tendency for the SVM to over-fit to the training data, yielding a classifier that may generalize poorly. In this situation it may be possible to classify the training data set accurately but the accuracy with which an independent testing set is classified may be low. Since the interest in remote sensing is typically to derive a classifier from a limited training set which may be applied usefully to other samples, the generalization ability of the classifier is of fundamental importance and hence over-fitting is undesirable. Consequently, the magnitude of <em>C</em> and <em>Î³</em> must be determined carefully for the task in-hand. To help reduce the subjectivity in setting the SVM parameter values, a cross-validation approach is commonly adopted for their determination (<a name=bbib3 href=#bib3 class=workspace-trigger>Belousov et al., 2002</a>).<p>The critical feature to note about SVM classification, however, is that only the support vectors contribute to the fitting of the optimal separating hyperplane. It is, therefore, possible to derive an accurate classification from a small and unrepresentative sample of training cases. The strict demands imposed by random sampling are no longer necessary as samples can be deliberately selected by the analyst. To fully exploit this feature, a means to identify sites likely to furnish support vectors in advance of the classification is required (<a name=bbib14 href=#bib14 class=workspace-trigger>Foody &amp; Mathur, 2004b</a>). Such judgemental sampling can provide the training information needed for a classification and should be easier to acquire than a random sample. As a further refinement or as an alternative, the analyst could try and focus attention onto the general region around the expected location of a suitable hyperplane. In feature space, the support vectors lie on the edges of the class distributions and in between the class centroids. One would also expect to find mixed pixels, the spectral response of which being a composite of the pure class responses involved, lying in this region between the class centroids. Broadly one would expect that a mixed pixel comprising equal proportions of the classes would lie roughly mid-way between the class centroids (<a name=bfig1 href=#fig1 class=workspace-trigger>Fig. 1</a>). Indeed this is the basis of using a SVM for spectral unmixing (<a name=bbib4 href=#bib4 class=workspace-trigger>Brown et al., 2000</a>). The spectral response of mixed pixels of imbalanced class composition would generally be located between this mid-way position and the centroid of the class which made the dominant contribution to the pixel's spectral response, the precise position in feature space being a function of the proportional cover of the classes and getting closer to the class centroid the purer the pixel's composition. Critically, mixed spectral responses would generally be expected to lie between the class centroids and be closer than the class centroids to the location of a hyperplane that can accurately separate the classes. Moreover, the spectral response derived from an imbalanced mixture may be expected to lie close to locations of hyperplanes such as P1 and P2 used in SVM classification. One might expect, therefore, that mixed spectral responses are actually more useful than those from pure pixels in fitting a hyperplane. Furthermore, as only support vectors contribute to the fitting of the hyperplane it should be possible to use a small training set of cases comprised of mixed spectral responses to classify a data set as accurately as a large, conventionally defined, training set of pure cases. It should be possible, therefore, to use a small training set comprising of data on mixed spectral responses to derive an accurate classification. This paper seeks to test this hypothesised scenario and illustrate the potential for using mixed spectral responses in training a supervised image classifier.</p></section><section id=aep-section-id12><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">3. Data and methods</h2><div><p>The potential to focus the training stage of a SVM classification on mixed spectral responses was evaluated from a series of classification analyses of imagery acquired of an agricultural test site. The study area selected was located around the village of Feltwell, UK (<a name=bfig3 href=#fig3 class=workspace-trigger>Fig. 3</a>). This region is topographically flat and most of the land has been divided up into large agricultural fields. For the purposes of this research, the effect of marked variations in soil type in the region on class separability was reduced by focusing on only land to the east of the drainage dyke that runs through the region. In this study area the fields had been planted mainly to sugar beet, winter wheat or spring barley on a brown renzina soil. All analyses focused on the classification of these three classes.<figure class="figure text-xs" id=fig3><span><img src=data:null;base64, height=607 alt><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0034425706001350-gr3.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span><p><span class=label>Fig. 3</span>. SPOT HRV near-infrared image of the study area with the location of the three field boundaries used in the derivation of the mixed spectral response training set. Class labels are winter wheat (ww), spring barley (SpB) and sugar beet (sb).</p></span></span></figure></div><p>A three waveband multispectral SPOT HRV image of the test site acquired on 16 June 1986 was used for the analyses. This image provided data in three spectral wavebands: red, green and near-infrared (<a name=bfig3 href=#fig3 class=workspace-trigger>Fig. 3</a>). No pre-processing of the image was undertaken with actions such as atmospheric correction viewed as unnecessary for the classification of a single image (<a name=bbib24 href=#bib24 class=workspace-trigger>Song et al., 2001</a>) and the agricultural fields were readily identifiable in the imagery removing the need for a geometric correction. The classification analyses were, therefore, undertaken using image DN values. At the time of image acquisition, the winter wheat and spring barley classes were at a mature stage of growth, with typically &gt;&nbsp;95% ground cover. However, the fields that had been planted to sugar beet were essentially bare at the time of image acquisition. A map showing the crop type that had been planted in each field that had been produced by conventional methods was available as ground data. For simplicity, the class membership of each field will be described in terms of the crop which had been planted within its area.<p>A variety of training sets were formed. To provide a basic benchmark to aid evaluation of the results, one training set contained 90 randomly selected pixels of each crop type. The pixels selected were all drawn from large areas of homogeneous class coverage. This training set corresponded to that which would be acquired through adoption of the standard heuristic that 30 training samples per-class per-waveband should be acquired for a classification. This is referred to later as the training set formed from the use of the 30<em>p</em> heuristic, where <em>p</em> indicates the number of wavebands used.<p>A training set based on mixed spectral responses was acquired. Here, the aim was to use a small number of mixed spectral responses to provide the training information for the classification. For this, 3 field boundaries were selected (<a name=bfig3 href=#fig3 class=workspace-trigger>Fig. 3</a>). These boundaries were selected relatively arbitrarily, they were simply a set of easy to identify and reasonably long boundaries that, critically, lay between every possible combination of the classes. That is, the data set contained a boundary that separated each possible pair of crops. Thus, in this simple three class scenario, there was the potential to obtain information on the way the spectral response of each class mixed with each other class.<div><p>The boundaries between fields were clearly visible in the imagery and could be more objectively located by inspection of the DN values if required. For each selected boundary, a transect of pixels that ran along the length of the boundary was identified. The pixels in this transect would all be mixed pixels. The spectral response of these mixed pixels may convey all of the information that is needed to locate the hyperplane between the classes. However, this might require detailed and difficult to acquire information on the exact nature of the mixing (e.g. the proportion of the area represented by the pixel covered by the component classes). Here, an approach which requires no such detailed information was adopted. Specifically, the approach simply required mixed spectral responses. Critically, however, these mixed responses needed to be biased towards a class. There also needed to be cases for each mixing scenario in which a class was the major component of the mixture and cases when it was the minor component of the mixture. In this way two sets of mixed responses for the class pair would be derived. These mixed spectral responses were derived since it would be expected that the hyperplane in feature space to separate the pair of classes would lie between the locations of the two mixed spectral responses derived for that pair. To remove the need to know the class composition of the mixed pixels, the DN value of the pixel on the transect was averaged with the DN value of its neighbour to one side of the boundary in one of the fields. This yielded a mixed spectral response that was dominated by the class associated with the field from which the neighbouring pixel was drawn. This process was then repeated but by averaging the mixed pixel response with the corresponding pixel on the opposite side of the boundary. In this way, for each mixed pixel straddling a boundary, two mixed spectral responses were derived, one dominated by each of the classes separated by that boundary. It would be expected that the hyperplane to separate the classes would lie between these two sets of mixed spectral responses in feature space. Moreover, these mixed responses would be closer to the hyperplane than the responses of the pure classes and this may help in define an appropriate separating hyperplane. For training the classification itself, each mixed spectral response was associated fully with its dominant class (i.e. labelled as a member of the dominant class) in order to establish the location of the separating hyperplanes. A simple generalization of the process is given in <a name=bfig4 href=#fig4 class=workspace-trigger>Fig. 4</a>.<figure class="figure text-xs" id=fig4><span><img src=data:null;base64, height=578 alt><ol class=links-for-figure><li><a class="anchor download-link u-font-sans" href=https://ars-els-cdn-com.simsrad.net.ocs.mq.edu.au/content/image/1-s2.0-S0034425706001350-gr4.jpg target=_blank download title="Download full-size image"><span class=anchor-text>Download : <span class=download-link-title>Download full-size image</span></span></a></ol></span><span class=captions><span><p><span class=label>Fig. 4</span>. Summary of the method based on mixed spectral responses. (A) An appropriate field boundary separating two classes, A and B, is identified in the image. A transect of mixed pixels that runs along the boundary is located. (B) For each pixel along this transect, its DN is averaged with its neighbouring pixel in the field containing class A. (C) For each pixel along the transect, its DN is averaged with its neighbouring pixel in the field containing class B. (D) From (B) and (C) two sets of mixed spectral response have been derived for the pair of classes. In the example shown there are four cases for each mixture. One set of mixed responses is dominated by the response of class A while the other is dominated by that of class B and these lie closer to the expected location of the hyperplane to separate the classes than the pure class distributions. The cases in are labelled by their dominant class and used to train the SVM.</p></span></span></figure></div><p>The mixed spectral response data for training were generated from 3 boundaries. Thus only ground data on the land cover of a maximum of 6 fields was required. The actual number of fields required to provide the required number of boundary types is a function of the crop mosaic on the ground. For the simple 3 class scenario considered here it would be possible to need information on just 3 fields if appropriately located relative to each other. In this research, however, 5 fields were used to provide the required boundary information (<a name=bfig3 href=#fig3 class=workspace-trigger>Fig. 3</a>). The boundaries selected varied in length from 12 to 28 pixels. The total boundary length involving winter wheat was 41 pixels while that for spring barley and sugar beet was 40 and 25 pixels, respectively. Given that the mixed spectral responses were derived by averaging the DN of the transect pixels with those from neigbouring pixels drawn from inside the fields, the total number of pixels associated with each class was 82, 80 and 50 for the winter wheat, spring barley and sugar beet classes, respectively (note that the mixed pixels along the transect were associated with two classes simultaneously). The data set derived from this process was referred to as the mixed spectral response training set.<p>To facilitate comparison between the classification trained with the mixed spectral responses and those trained with more conventional approaches, two further training sets were generated. These training sets were generated to provide classifications which could be compared directly, albeit from different perspectives, against that based on the mixed spectral responses. The key feature of these additional training sets was that their size, in terms of ground data needed, could be considered the same as that used in generating the mixed spectral response training set. First, a training set containing the same number of cases per-class as in that used in the derivation of the training set based on mixed spectral responses was generated. This contained 82, 80 and 50 randomly selected pure cases of winter wheat, spring barley and sugar beet. In keeping with conventional practice, all of the sampled pixels were drawn from large areas of homogeneous cover. The training set was, therefore, similar to that used to generate the benchmark classification but with a smaller sample size, which matched that associated with the training set based on the mixed spectral responses. In this way it would be possible to compare directly a conventional approach to training with that based on mixed responses in which the sample sizes used were the same. The data set derived is referred to as the conventional (same size) training set.<p>The final training set derived was based on the spectral response of the 5 fields used to generate the mixed spectral response data. The aim was to use the ground data on the 5 selected fields in a conventional manner. For this, the boundary region was masked out and only the pure pixels that were separated from the boundary by at least 2 pixels were acquired for training purposes. In this way, a standard set of pure pixels extracted from the same fields used to generate the mixed spectral response data set were acquired for training. Thus, the same total amount of ground data, in terms of fields of known class membership, was available to the analysis as for the classification based on the mixed spectral responses. This training set comprised 225, 203 and 100 pixels of winter wheat, spring barley and sugar beet, respectively. This is referred to as the conventional (same fields) training set.<div><p>The <em>C</em> and <em>Î³</em> parameters for each SVM classification were selected with the aid of a five-fold cross-validation analysis undertaken. The values used in each classification are presented in <a name=btbl1 href=#tbl1 class=workspace-trigger>Table 1</a>.<div class="tables frame-topbot colsep-0 rowsep-0" id=tbl1><span class=captions><span><p><span class=label>Table 1</span>. The SVM parameter settings used</p></span></span><div class=groups><table><thead><tr class=rowsep-1><th scope=col class=valign-top>Training set<th scope=col class=valign-top><em>C</em><th scope=col class=valign-top><em>Î³</em><tbody><tr><td class=valign-top>Conventional, 30<em>p</em><td class="align-char valign-top">8.0<td class="align-char valign-top">2<sup>âˆ’&nbsp;5</sup><tr><td class=valign-top>Mixed spectral response<td class="align-char valign-top">8.0<td class="align-char valign-top">2<sup>âˆ’&nbsp;8</sup><tr><td class=valign-top>Conventional, same size<td class="align-char valign-top">32.0<td class="align-char valign-top">2<sup>âˆ’&nbsp;4</sup><tr><td class=valign-top>Conventional, same fields<td class="align-char valign-top">8.0<td class="align-char valign-top">2<sup>âˆ’&nbsp;3</sup></table></div></div></div><p>The accuracy of each classification was assessed using a testing set comprising 90 randomly selected pure pixels of each class. Classification accuracy was expressed as the percentage of correctly allocated cases but the confusion matrices are also presented below to allow other measures of accuracy to be derived if desired. The classification accuracy statements were compared to determine if the classifications differed significantly in terms of accuracy. Since the same testing set was used to evaluate all of the classifications, this comparison was based on a McNemar test (<a name=bbib11 href=#bib11 class=workspace-trigger>Foody, 2004</a>). With this approach, the evaluation of the significance of a difference in estimated accuracy is based on the magnitude of a computed <em>z</em> score. The difference between two accuracy statements is viewed as being statistically significant at the 95% level of confidence if <em>z</em>&nbsp;&gt;&nbsp;âˆ£1.96âˆ£.</p></section><section id=aep-section-id13><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">4. Results and discussion</h2><div><p>The classification trained with 90 cases of each class in accordance with the widely used 30<em>p</em> heuristic yielded a very accurate classification. The overall accuracy of this classification was 92.59% with, as expected from the crop canopy covers evident at the time of image acquisition, most confusion between the winter wheat and spring barley classes (<a name=btbl2 href=#tbl2 class=workspace-trigger>Table 2</a>). This benchmark classification derived using a conventional approach to training showed that the classes may be classified to a high accuracy.<div class="tables frame-topbot colsep-0 rowsep-0" id=tbl2><span class=captions><span><p><span class=label>Table 2</span>. Confusion matrix from the SVM classification derived with the use of the conventional, 30<em>p</em>, training set</p></span></span><div class=groups><table><thead><tr><th scope=col class="rowsep-1 valign-top" rowspan=2>Actual class<th scope=col class="rowsep-1 valign-top" colspan=4>Predicted class<tr><th scope=col class="rowsep-1 valign-top">ww<th scope=col class="rowsep-1 valign-top">SpB<th scope=col class="rowsep-1 valign-top">sb<th scope=col class="rowsep-1 valign-top">Î£<tbody><tr><td class=valign-top>Winter wheat (ww)<td class="align-char valign-top">84<td class="align-char valign-top">6<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Spring barley (SpB)<td class="align-char valign-top">14<td class="align-char valign-top">76<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Sugar beet (sb)<td class="align-char valign-top">0<td class="align-char valign-top">0<td class="align-char valign-top">90<td class="align-char valign-top">90<tr><td class=valign-top>Î£<td class="align-char valign-top">98<td class="align-char valign-top">82<td class="align-char valign-top">90<td class="align-char valign-top">270</table></div><p class=legend><p>The overall accuracy was 92.59%.<p></p></div></div><div><p>The classification trained with the mixed spectral responses also yielded a highly accurate classification (<a name=btbl3 href=#tbl3 class=workspace-trigger>Table 3</a>). The accuracy of this classification was 91.11%, marginally but insignificantly less than that derived from the benchmark analysis (<em>z</em>&nbsp;=&nbsp;1.15). The pattern of class allocations in the two classifications was also very similar (<a name=btbl2 href=#tbl2 class=workspace-trigger>Table 2</a>, <a name=btbl3 href=#tbl3 class=workspace-trigger>Table 3</a>). Thus, a classification trained on mixed spectral responses yielded a similar classification that was of comparable accuracy to that derived from the adoption of the conventional approach. Moreover, the classification trained on mixed spectral responses required substantially less ground data, just class membership information on 5 fields whereas the conventional approach needed information on 90 pixels drawn from numerous fields across the test site.<div class="tables frame-topbot colsep-0 rowsep-0" id=tbl3><span class=captions><span><p><span class=label>Table 3</span>. Confusion matrix from the SVM classification derived with the use of the mixed spectral response training set</p></span></span><div class=groups><table><thead><tr><th scope=col class="rowsep-1 valign-top" rowspan=2>Actual class<th scope=col class="rowsep-1 valign-top" colspan=4>Predicted class<tr><th scope=col class="rowsep-1 valign-top">ww<th scope=col class="rowsep-1 valign-top">SpB<th scope=col class="rowsep-1 valign-top">sb<th scope=col class="rowsep-1 valign-top">Î£<tbody><tr><td class=valign-top>Winter wheat (ww)<td class="align-char valign-top">83<td class="align-char valign-top">7<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Spring barley (SpB)<td class="align-char valign-top">17<td class="align-char valign-top">73<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Sugar beet (sb)<td class="align-char valign-top">0<td class="align-char valign-top">0<td class="align-char valign-top">90<td class="align-char valign-top">90<tr><td class=valign-top>Î£<td class="align-char valign-top">100<td class="align-char valign-top">80<td class="align-char valign-top">90<td class="align-char valign-top">270</table></div><p class=legend><p>The overall accuracy was 91.11%.<p></p></div></div><div><p>Direct comparison between the two approaches to training is difficult as the size of the training sets used differed. However, a classification trained using the same number of training cases for each class as associated with the analysis based on mixed spectral responses yielded a similar classification (<a name=btbl4 href=#tbl4 class=workspace-trigger>Table 4</a>) to the other analyses (<a name=btbl2 href=#tbl2 class=workspace-trigger>Table 2</a>, <a name=btbl3 href=#tbl3 class=workspace-trigger>Table 3</a>). The differences in accuracy between these classifications was insignificant (<em>z</em>&nbsp;=&nbsp;0.23 and <em>z</em>&nbsp;=&nbsp;1.13 for the comparison of the classifications derived with the conventional (same size) training set against that from the use of the mixed spectral responses and the 30<em>p</em> training sets respectively). However, the use of just the pure pixels from the 5 fields that provided the mixed spectral response data resulted in a classification of markedly, and significantly at 95% level of confidence, lower accuracy than the others (<a name=btbl5 href=#tbl5 class=workspace-trigger>Table 5</a>). This was evident in comparison against the classification trained with mixed spectral responses (<em>z</em>&nbsp;=&nbsp;3.40) and the training set formed through the use of the 30<em>p</em> heuristic (<em>z</em>&nbsp;=&nbsp;3.53). Thus, although the approach based on mixed spectral responses required only class membership for a few fields and resulted in an accurate classification the use of the pure pixels from these fields yielded a classification of substantially lower accuracy.<div class="tables frame-topbot colsep-0 rowsep-0" id=tbl4><span class=captions><span><p><span class=label>Table 4</span>. Confusion matrix from the SVM classification derived with the use of the conventional, same size, training set</p></span></span><div class=groups><table><thead><tr><th scope=col class="rowsep-1 valign-top" rowspan=2>Actual class<th scope=col class="rowsep-1 valign-top" colspan=4>Predicted class<tr><th scope=col class="rowsep-1 valign-top">ww<th scope=col class="rowsep-1 valign-top">SpB<th scope=col class="rowsep-1 valign-top">sb<th scope=col class="rowsep-1 valign-top">Î£<tbody><tr><td class=valign-top>Winter wheat (ww)<td class="align-char valign-top">79<td class="align-char valign-top">11<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Spring barley (SpB)<td class="align-char valign-top">12<td class="align-char valign-top">78<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Sugar beet (sb)<td class="align-char valign-top">0<td class="align-char valign-top">0<td class="align-char valign-top">90<td class="align-char valign-top">90<tr><td class=valign-top>Î£<td class="align-char valign-top">91<td class="align-char valign-top">89<td class="align-char valign-top">90<td class="align-char valign-top">270</table></div><p class=legend><p>The overall accuracy was 91.48%.<p></p></div><div class="tables frame-topbot colsep-0 rowsep-0" id=tbl5><span class=captions><span><p><span class=label>Table 5</span>. Confusion matrix from the SVM classification derived with the use of the conventional, same fields, training set</p></span></span><div class=groups><table><thead><tr><th scope=col class="rowsep-1 valign-top" rowspan=2>Actual class<th scope=col class="rowsep-1 valign-top" colspan=4>Predicted class<tr><th scope=col class="rowsep-1 valign-top">ww<th scope=col class="rowsep-1 valign-top">SpB<th scope=col class="rowsep-1 valign-top">sb<th scope=col class="rowsep-1 valign-top">Î£<tbody><tr><td class=valign-top>Winter wheat (ww)<td class="align-char valign-top">83<td class="align-char valign-top">1<td class="align-char valign-top">6<td class="align-char valign-top">90<tr><td class=valign-top>Spring barley (SpB)<td class="align-char valign-top">33<td class="align-char valign-top">57<td class="align-char valign-top">0<td class="align-char valign-top">90<tr><td class=valign-top>Sugar beet (sb)<td class="align-char valign-top">0<td class="align-char valign-top">0<td class="align-char valign-top">90<td class="align-char valign-top">90<tr><td class=valign-top>Î£<td class="align-char valign-top">116<td class="align-char valign-top">58<td class="align-char valign-top">96<td class="align-char valign-top">270</table></div><p class=legend><p>The overall accuracy was 85.18%.<p></p></div></div><p>The results demonstrate that it is possible to use a small training set based on mixed spectral responses to derive a classification of similar accuracy to one trained using a larger number of pure pixels acquired in a conventional manner. These results indicate considerable promise for adopting a very different approach to the training of supervised classifications than that widely promoted. The approach also offers the potential for substantial savings in training data acquisition as the number of training cases and the precision with which they must be defined spatially is less than with the conventional approach. This is most apparent perhaps in relation to the effort required to acquire the training sample. With the conventional approach adopted it is necessary to strictly adhere to the constraints imposed by a random sampling design while the approach based on mixed spectral responses is compatible with an easier to implement judgemental sampling approach. The conventional approach may, for example, require ground data to be collected for sites distributed widely across the entire study area in locations that might be difficult to access and which may need to be located with high precision. With the alternative approach attention is focused on boundary regions that typically are easy to observe, in both the field and imagery, as well as selected on the basis of analyst's judgement which may include issues such as convenience and practicality. Thus judgemental sampling, which is often viewed unfavorably and avoided but which is easy to implement, may have a role to play in training image classifications.<p>Clearly, the work reported here represents only a first step and there are many issues to be addressed. The example used was, for instance, of a very simple classification problem to illustrate the potential of the approach. More complex situations exist, particularly with greater spectral overlap in the class distributions. Since the classes here were relatively highly separable the precise location of the hyperplane to classify the data was not critical. It would be interesting to evaluate in detail the location of the hyperplane used to derive the classification relative to the optimal separating hyperplane derived from a large sample in the normal way. Ideally, the hyperplane fitted using the mixed spectral response data would lie close to or correspond exactly with the real optimal separating hyperplane to discriminate the classes. Other issues also require further evaluation. For example, the orientation of the boundary with respect to the image's raster grid will impact on the class composition of the mixed pixels. If, for example, the boundary is parallel with the rows or columns of the grid the mixed pixels will have the same class composition. If, however, the boundary was to cut diagonally through the pixel grid then the composition of the mixed pixels would vary. The implications of this in terms of the class composition contributing to the mixed responses and its impact on the location of the separating hyperplane in particular should be evaluated. Additionally, for example, the approach presented will be impacted by variations in boundary properties and pixel size. If, for example, the boundary between the fields was a wide hedge, perhaps wider than the dimensions of a pixel, the boundary region focused on in this article would not convey much information about the mixing of the crop spectral responses. In such circumstances the approach presented here would result in the boundary itself being characterised and so not necessarily help in locating an appropriate hyperplane. Finally, as the training stage of the classification should be tailored to the nature of the classifier to be used, the applicability of the general approach to other classifiers should be evaluated. Further work is, therefore, required to understand the potentials of the approach and raise awareness of its limitations. Current work is, for example, focused on the potential of using just mixed pixels to convey the information required for a classification.</p></section><section id=aep-section-id14><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">5. Summary and conclusion</h2><p>The accuracy of a supervised image classification is a function of the training data set used. Many studies have shown that variables such as the size, composition and nature of the training cases used can have a substantial impact on image classification accuracy. However, most guidance on training set design urges the use of a large number of pure pixels. Emphasis is also placed on describing the classes rather than on the provision of information to separate them. It has been argued here that the basis of the conventional approach to the design of the training stage of a supervised classification is not always appropriate and an alternative suggested.<p>In remote sensing, the aim of the training stage has typically been seen as being the production of descriptive statistics for each class which may then be used in the determination of class membership by the selected classifier. The typical approach used involves the acquisition of a sample of pixels of known class membership from the image. The ruling paradigm for training a classifier seems based implicitly on a desire to provide an accurate characterization of the classes. Emphasis in such an approach is on the use of a large number of pure pixels selected at random. Such an approach may not always be appropriate and the training stage should be designed with regard to the classifier selected for the analysis. Here, we have proposed an approach for the design of the training stage that is essentially the opposite, the use of a small number of purposefully selected mixed spectral responses to provide information to discriminate the classes with a SVM classifier. The approach, in effect, has focused on the boundary between classes in geographical space for the provision of the information needed to separate the classes in spectral feature space. These geographical locations are, in the conventional practice, often regarded as the most undesirable samples and often deliberately masked-out of analyses.<p>The new approach to the design of the training stage was grounded on the basis of the proposed classifier, an SVM. It or a derivative of it may be applicable to other classifiers as the aim is simply to use the training stage to provide information that helps separate the classes rather than describe them. The approach proposed may, however, be inappropriate for use with some classifiers. For example, the approach proposed would not be expected to yield training data that gave a representative description of the classes and so be inappropriate for many basic statistical classifiers. It is important, therefore, that the training data acquisition programme is informed of the type of classifier to be used. The way that a SVM operates provides considerable potential to refine the design of training data collection activities. In the example shown above, a small amount of ground data (the class membership for 5 fields) was required to accurately classify three classes with a SVM. For example, the use of a training set comprising mixed spectral responses yielded a classification with an accuracy of 91.11%, nearly identical to that that derived from the use of a conventionally defined training set containing the same number of cases which had an accuracy of 91.48%. The new approach is, therefore, an attractive alternative, but not necessarily a replacement, of the conventional approach, especially in circumstances when training data collection is costly.<p>Although the classification was very simplistic the work reported represents the first steps in trying to design a new approach to classifier training that is tailored to the way in which the specific classifier operates which also has operational advantages (principally the potential to use a small and inexpensive sample to form the training set). A central focus is to acquire training data to separate the classes and not to describe them as well as to free the training data collection programme from the constraints often imposed by the need to adhere to the demands of random sampling by allowing judgemental sampling.</p></section></div><section id=aep-acknowledgment-id15><h2 class="u-h3 u-margin-l-top u-margin-xs-bottom">Acknowledgments</h2><p>The authors are grateful to the NERC Earth Observation Data Centre for the provision of the SPOT HRV data and the European commission for the crop map made available through the AgriSAR campaign. We are also grateful to the British Council for providing a Commonwealth Scholarship to AM to undertake postgraduate research at the University of Southampton while on leave from the Punjab Remote Sensing Centre. The support vector machines were constructed with BSVM (version 2.01) software developed by Chih-Wei Hsu and Chih-Jen Lin, National Taiwan University, Taipei. We are also grateful to Dr. Jadunandan Dash for assistance with the research. Finally, we are grateful to the three referees for their helpful comments on the original submission.</p></section></div><div class="related-content-links u-hide-from-md sf-hidden"></div><div class=Tail></div><section class="bibliography u-font-serif text-s" id=aep-bibliography-id16><h2 class="section-title u-h3 u-margin-l-top u-margin-xs-bottom">References</h2><section class=bibliography-sec id=aep-bibliography-sec-id17><dl class=references id=reference-links-aep-bibliography-sec-id17><dt class=label><a href=#bbib2 id=ref-id-bib2 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Airkan, 2004</a><dd class=reference><div class=contribution>M. Airkan<div><strong class=title>Parcel-based crop mapping through multi-temporal masking classification of Landsat 7 images in Karacabey, Turkey</strong></div></div><div class=host>Proceedings of the ISPRS symposium, Istanbul, International Archives of Photogrammetry, Remote Sensing and Spatial Information Science, Vol. 34 (2004)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Parcel-based%20crop%20mapping%20through%20multi-temporal%20masking%20classification%20of%20Landsat%207%20images%20in%20Karacabey%2C%20Turkey&amp;publication_year=2004&amp;author=M.%20Airkan">Google Scholar</a></div><dt class=label><a href=#bbib1 id=ref-id-bib1 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Aria, 1992</a><dd class=reference><div class=contribution>K. Aria<div><strong class=title>A supervised Thematic Mapper classification with a purification of training samples</strong></div></div><div class=host>International Journal of Remote Sensing, 13 (1992), pp. 2039-2049</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20supervised%20Thematic%20Mapper%20classification%20with%20a%20purification%20of%20training%20samples&amp;publication_year=1992&amp;author=K.%20Aria">Google Scholar</a></div><dt class=label><a href=#bbib3 id=ref-id-bib3 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Belousov et al., 2002</a><dd class=reference><div class=contribution>A.I. Belousov, S.A. Verzakov, J. von Frese<div><strong class=title>A flexible classification approach with optimal generalisation performance: support vector machines</strong></div></div><div class=host>Chemometrics and Intelligent Laboratory Systems, 64 (2002), pp. 15-25</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0169743902000461>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0169743902000461/pdfft?md5=5ce9849d257fb3ed350691e4116ad161&amp;pid=1-s2.0-S0169743902000461-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0037191113&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20flexible%20classification%20approach%20with%20optimal%20generalisation%20performance%3A%20support%20vector%20machines&amp;publication_year=2002&amp;author=A.I.%20Belousov&amp;author=S.A.%20Verzakov&amp;author=J.%20von%20Frese">Google Scholar</a></div><dt class=label><a href=#bbib4 id=ref-id-bib4 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Brown et al., 2000</a><dd class=reference><div class=contribution>M. Brown, H.G. Lewis, S.R. Gunn<div><strong class=title>Linear spectral mixture models and support vector machines for remote sensing</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 38 (2000), pp. 2346-2360</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0034270699></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0034270699&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Linear%20spectral%20mixture%20models%20and%20support%20vector%20machines%20for%20remote%20sensing&amp;publication_year=2000&amp;author=M.%20Brown&amp;author=H.G.%20Lewis&amp;author=S.R.%20Gunn">Google Scholar</a></div><dt class=label><a href=#bbib5 id=ref-id-bib5 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Buttner et al., 1989</a><dd class=reference><div class=contribution>G. Buttner, T. Hajos, M. Korandi<div><strong class=title>Improvements to the effectiveness of supervised training procedures</strong></div></div><div class=host>International Journal of Remote Sensing, 10 (1989), pp. 1005-1013</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431168908903940></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431168908903940>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0024855536&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Improvements%20to%20the%20effectiveness%20of%20supervised%20training%20procedures&amp;publication_year=1989&amp;author=G.%20Buttner&amp;author=T.%20Hajos&amp;author=M.%20Korandi">Google Scholar</a></div><dt class=label><a href=#bbib6 id=ref-id-bib6 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Chi and Bruzzone, 2005</a><dd class=reference><div class=contribution>M. Chi, L. Bruzzone<div><strong class=title>A semilabeled-sample-driven bagging technique for ill-posed classification problems</strong></div></div><div class=host>IEEE Geoscience and Remote Sensing Letters, 2 (2005), pp. 69-73</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-12844275025></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-12844275025&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20semilabeled-sample-driven%20bagging%20technique%20for%20ill-posed%20classification%20problems&amp;publication_year=2005&amp;author=M.%20Chi&amp;author=L.%20Bruzzone">Google Scholar</a></div><dt class=label><a href=#bbib7 id=ref-id-bib7 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Cortes and Vapnik, 1995</a><dd class=reference><div class=contribution>C. Cortes, V. Vapnik<div><strong class=title>Support vector networks</strong></div></div><div class=host>Machine Learning, 20 (1995), pp. 273-297</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Support%20vector%20networks&amp;publication_year=1995&amp;author=C.%20Cortes&amp;author=V.%20Vapnik">Google Scholar</a></div><dt class=label><a href=#bbib8 id=ref-id-bib8 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Ediriwickrema and Khorram, 1997</a><dd class=reference><div class=contribution>J. Ediriwickrema, S. Khorram<div><strong class=title>Hierarchical maximum-likelihood classification for improved accuracies</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 35 (1997), pp. 810-816</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0031190719></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0031190719&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Hierarchical%20maximum-likelihood%20classification%20for%20improved%20accuracies&amp;publication_year=1997&amp;author=J.%20Ediriwickrema&amp;author=S.%20Khorram">Google Scholar</a></div><dt class=label><a href=#bbib9 id=ref-id-bib9 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Fardanesh and Ersoy, 1998</a><dd class=reference><div class=contribution>M.T. Fardanesh, O.K. Ersoy<div><strong class=title>Classification accuracy improvement of neural network classifiers by using unlabeled data</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 36 (1998), pp. 1020-1025</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0000853761></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0000853761&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Classification%20accuracy%20improvement%20of%20neural%20network%20classifiers%20by%20using%20unlabeled%20data&amp;publication_year=1998&amp;author=M.T.%20Fardanesh&amp;author=O.K.%20Ersoy">Google Scholar</a></div><dt class=label><a href=#bbib10 id=ref-id-bib10 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody, 1999</a><dd class=reference><div class=contribution>G.M. Foody<div><strong class=title>The significance of border training patterns in classification by a feedforward neural network using backpropagation learning</strong></div></div><div class=host>International Journal of Remote Sensing, 20 (1999), pp. 3549-3562</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0033372875></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0033372875&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20significance%20of%20border%20training%20patterns%20in%20classification%20by%20a%20feedforward%20neural%20network%20using%20backpropagation%20learning&amp;publication_year=1999&amp;author=G.M.%20Foody">Google Scholar</a></div><dt class=label><a href=#bbib11 id=ref-id-bib11 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody, 2004</a><dd class=reference><div class=contribution>G.M. Foody<div><strong class=title>Thematic map comparison: evaluating the statistical significance of differences in classification accuracy</strong></div></div><div class=host>Photogrammetric Engineering and Remote Sensing, 70 (2004), pp. 627-633</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-3042661357></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-3042661357&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Thematic%20map%20comparison%3A%20evaluating%20the%20statistical%20significance%20of%20differences%20in%20classification%20accuracy&amp;publication_year=2004&amp;author=G.M.%20Foody">Google Scholar</a></div><dt class=label><a href=#bbib12 id=ref-id-bib12 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody and Arora, 1997</a><dd class=reference><div class=contribution>G.M. Foody, M.K. Arora<div><strong class=title>An evaluation of some factors affecting the accuracy of classification by an artificial neural network</strong></div></div><div class=host>International Journal of Remote Sensing, 18 (1997), pp. 799-810</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0031105722></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0031105722&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=An%20evaluation%20of%20some%20factors%20affecting%20the%20accuracy%20of%20classification%20by%20an%20artificial%20neural%20network&amp;publication_year=1997&amp;author=G.M.%20Foody&amp;author=M.K.%20Arora">Google Scholar</a></div><dt class=label><a href=#bbib13 id=ref-id-bib13 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody and Mathur, 2004a</a><dd class=reference><div class=contribution>G.M. Foody, A. Mathur<div><strong class=title>A relative evaluation of multiclass image classification by support vector machines</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 42 (2004), pp. 1335-1343</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20relative%20evaluation%20of%20multiclass%20image%20classification%20by%20support%20vector%20machines&amp;publication_year=2004&amp;author=G.M.%20Foody&amp;author=A.%20Mathur">Google Scholar</a></div><dt class=label><a href=#bbib14 id=ref-id-bib14 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody and Mathur, 2004b</a><dd class=reference><div class=contribution>G.M. Foody, A. Mathur<div><strong class=title>Toward intelligent training of supervised image classifications: directing training data acquisition for SVM classification</strong></div></div><div class=host>Remote Sensing of Environment, 93 (2004), pp. 107-117</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003442570400207X>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S003442570400207X/pdfft?md5=3fc0fe66c6551e8f1189442dbf6bed7d&amp;pid=1-s2.0-S003442570400207X-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-4544272407&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Toward%20intelligent%20training%20of%20supervised%20image%20classifications%3A%20directing%20training%20data%20acquisition%20for%20SVM%20classification&amp;publication_year=2004&amp;author=G.M.%20Foody&amp;author=A.%20Mathur">Google Scholar</a></div><dt class=label><a href=#bbib15 id=ref-id-bib15 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Foody et al., 1995</a><dd class=reference><div class=contribution>G.M. Foody, M.B. McCulloch, W.B. Yates<div><strong class=title>The effect of training set size and composition on artificial neural network classification</strong></div></div><div class=host>International Journal of Remote Sensing, 16 (1995), pp. 1707-1723</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431169508954507></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431169508954507>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0029473455&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20effect%20of%20training%20set%20size%20and%20composition%20on%20artificial%20neural%20network%20classification&amp;publication_year=1995&amp;author=G.M.%20Foody&amp;author=M.B.%20McCulloch&amp;author=W.B.%20Yates">Google Scholar</a></div><dt class=label><a href=#bbib16 id=ref-id-bib16 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Hsu and Lin, 2002</a><dd class=reference><div class=contribution>C.-W. Hsu, C.-J. Lin<div><strong class=title>A comparison of methods for multiclass support vector machines</strong></div></div><div class=host>IEEE Transactions on Neural Networks, 13 (2002), pp. 415-425</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0036505670></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0036505670&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20comparison%20of%20methods%20for%20multiclass%20support%20vector%20machines&amp;publication_year=2002&amp;author=C.-W.%20Hsu&amp;author=C.-J.%20Lin">Google Scholar</a></div><dt class=label><a href=#bbib17 id=ref-id-bib17 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Huang et al., 2002</a><dd class=reference><div class=contribution>C. Huang, L.S. Davis, J.R.G. Townshend<div><strong class=title>An assessment of support vector machines for land cover classification</strong></div></div><div class=host>International Journal of Remote Sensing, 23 (2002), pp. 725-749</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0037138473></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0037138473&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=An%20assessment%20of%20support%20vector%20machines%20for%20land%20cover%20classification&amp;publication_year=2002&amp;author=C.%20Huang&amp;author=L.S.%20Davis&amp;author=J.R.G.%20Townshend">Google Scholar</a></div><dt class=label><a href=#bbib18 id=ref-id-bib18 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Kuo and Landgrebe, 2002</a><dd class=reference><div class=contribution>B.C. Kuo, D.A. Landgrebe<div><strong class=title>A covariance estimator for small sample size classification problems and its application to feature extraction</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 40 (2002), pp. 814-819</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0036544159></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0036544159&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20covariance%20estimator%20for%20small%20sample%20size%20classification%20problems%20and%20its%20application%20to%20feature%20extraction&amp;publication_year=2002&amp;author=B.C.%20Kuo&amp;author=D.A.%20Landgrebe">Google Scholar</a></div><dt class=label><a href=#bbib19 id=ref-id-bib19 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Mather, 2004</a><dd class=reference><div class=contribution>P.M. Mather<div><strong class=title>Computer Processing of Remotely-Sensed Images</strong></div></div><div class=host> (3rd edition), Wiley, Chichester (2004)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Computer%20Processing%20of%20Remotely-Sensed%20Images&amp;publication_year=2004&amp;author=P.M.%20Mather">Google Scholar</a></div><dt class=label><a href=#bbib20 id=ref-id-bib20 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Melgani and Bruzzone, 2004</a><dd class=reference><div class=contribution>F. Melgani, L. Bruzzone<div><strong class=title>Classification of hyperspectral remote sensing images with support vector machines</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 42 (2004), pp. 1778-1790</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-4344614511></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-4344614511&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Classification%20of%20hyperspectral%20remote%20sensing%20images%20with%20support%20vector%20machines&amp;publication_year=2004&amp;author=F.%20Melgani&amp;author=L.%20Bruzzone">Google Scholar</a></div><dt class=label><a href=#bbib21 id=ref-id-bib21 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Nelson et al., 1984</a><dd class=reference><div class=contribution>R.F. Nelson, R.S. Latty, G. Mott<div><strong class=title>Classifying northern forests using Thematic Mapper simulator data</strong></div></div><div class=host>Photogrammetric Engineering and Remote Sensing, 50 (1984), pp. 607-617</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0021575243></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0021575243&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Classifying%20northern%20forests%20using%20Thematic%20Mapper%20simulator%20data&amp;publication_year=1984&amp;author=R.F.%20Nelson&amp;author=R.S.%20Latty&amp;author=G.%20Mott">Google Scholar</a></div><dt class=label><a href=#bbib22 id=ref-id-bib22 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Pal and Mather, 2005</a><dd class=reference><div class=contribution>M. Pal, P.M. Mather<div><strong class=title>Support vector machines for classification in remote sensing</strong></div></div><div class=host>International Journal of Remote Sensing, 26 (2005), pp. 1007-1011</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431160512331314083></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431160512331314083>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-13644256120&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Support%20vector%20machines%20for%20classification%20in%20remote%20sensing&amp;publication_year=2005&amp;author=M.%20Pal&amp;author=P.M.%20Mather">Google Scholar</a></div><dt class=label><a href=#bbib23 id=ref-id-bib23 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Piper, 1992</a><dd class=reference><div class=contribution>J. Piper<div><strong class=title>Variability and bias in experimentally measured classifier error rates</strong></div></div><div class=host>Pattern Recognition Letters, 13 (1992), pp. 685-692</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/016786559290097J>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/016786559290097J/pdf?md5=6d15dc0481630cda2214b0493b34c9a2&amp;pid=1-s2.0-016786559290097J-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0027067290&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Variability%20and%20bias%20in%20experimentally%20measured%20classifier%20error%20rates&amp;publication_year=1992&amp;author=J.%20Piper">Google Scholar</a></div><dt class=label><a href=#bbib24 id=ref-id-bib24 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Song et al., 2001</a><dd class=reference><div class=contribution>C. Song, C.E. Woodcock, K.C. Seto, M. Pax Lenney, S.A. Macomber<div><strong class=title>Classification and Change Detection Using Landsat TM Data. When and how to correct for atmospheric effects?</strong></div></div><div class=host>Remote Sensing of Environment, 75 (2001), pp. 230-244</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425700001693>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425700001693/pdfft?md5=6c529ad7312b3afa4c3d2897d2cc0944&amp;pid=1-s2.0-S0034425700001693-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0035141621&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar?q=Classification%20and%20Change%20Detection%20Using%20Landsat%20TM%20Data.%20When%20and%20how%20to%20correct%20for%20atmospheric%20effects">Google Scholar</a></div><dt class=label><a href=#bbib25 id=ref-id-bib25 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Staufer and Fischer, 1997</a><dd class=reference><div class=contribution>P. Staufer, M.M. Fischer<div><strong class=title>Spectral pattern recognition by a two-layer perceptron: effects of training set size</strong></div></div><div class=host>I. Kanellopoulos, G.G. Wilkinson, F. Roli, J. Austin (Eds.), Neurocomoputation in Remote Sensing Data Analysis, Springer, Berlin (1997), pp. 105-116</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/978-3-642-59041-2_12></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/978-3-642-59041-2_12>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Spectral%20pattern%20recognition%20by%20a%20two-layer%20perceptron%3A%20effects%20of%20training%20set%20size&amp;publication_year=1997&amp;author=P.%20Staufer&amp;author=M.M.%20Fischer">Google Scholar</a></div><dt class=label><a href=#bbib26 id=ref-id-bib26 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Sun et al., 2003</a><dd class=reference><div class=contribution>W.X. Sun, V. Heidt, P. Gong, G. Xu<div><strong class=title>Information fusion for rural land-use classification with high-resolution satellite imagery</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 41 (2003), pp. 883-890</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0037934720></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0037934720&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Information%20fusion%20for%20rural%20land-use%20classification%20with%20high-resolution%20satellite%20imagery&amp;publication_year=2003&amp;author=W.X.%20Sun&amp;author=V.%20Heidt&amp;author=P.%20Gong&amp;author=G.%20Xu">Google Scholar</a></div><dt class=label><a href=#bbib27 id=ref-id-bib27 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tadjudin and Landgrebe, 2000</a><dd class=reference><div class=contribution>S. Tadjudin, D.A. Landgrebe<div><strong class=title>Robust parameter estimation for mixture model</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 38 (2000), pp. 439-445</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0033886917></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0033886917&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Robust%20parameter%20estimation%20for%20mixture%20model&amp;publication_year=2000&amp;author=S.%20Tadjudin&amp;author=D.A.%20Landgrebe">Google Scholar</a></div><dt class=label><a href=#bbib28 id=ref-id-bib28 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Todd et al., 1980</a><dd class=reference><div class=contribution>W.J. Todd, S. Ustin, J.F. Haman<div><strong class=title>Landsat wildland mapping accuracy</strong></div></div><div class=host>Photogrammetric Engineering and Remote Sensing, 46 (1980), pp. 509-520</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0018911539></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0018911539&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Landsat%20wildland%20mapping%20accuracy&amp;publication_year=1980&amp;author=W.J.%20Todd&amp;author=S.%20Ustin&amp;author=J.F.%20Haman">Google Scholar</a></div><dt class=label><a href=#bbib29 id=ref-id-bib29 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Tsai and Philpot, 2002</a><dd class=reference><div class=contribution>F. Tsai, W.D. Philpot<div><strong class=title>A derivative-aided hyperspectral image analysis system for land cover classification</strong></div></div><div class=host>IEEE Transactions on Geoscience and Remote Sensing, 40 (2002), pp. 416-425</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=scopus doc-id=2-s2.0-0036477182></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0036477182&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=A%20derivative-aided%20hyperspectral%20image%20analysis%20system%20for%20land%20cover%20classification&amp;publication_year=2002&amp;author=F.%20Tsai&amp;author=W.D.%20Philpot">Google Scholar</a></div><dt class=label><a href=#bbib30 id=ref-id-bib30 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Van Niel et al., 2005</a><dd class=reference><div class=contribution>T.G. Van Niel, T.R. McVicar, B. Datt<div><strong class=title>On the relationship between training sample size and data dimensionality of broadband multi-temporal classification</strong></div></div><div class=host>Remote Sensing of Environment, 98 (2005), pp. 468-480</div><div class="ReferenceLinks u-font-sans"><a class=link href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425705002750>Article</a><a class="anchor pdf link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425705002750/pdfft?md5=853ac51f275146f1d524328a1ce9cffe&amp;pid=1-s2.0-S0034425705002750-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-26944473196&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=On%20the%20relationship%20between%20training%20sample%20size%20and%20data%20dimensionality%20of%20broadband%20multi-temporal%20classification&amp;publication_year=2005&amp;author=T.G.%20Van%20Niel&amp;author=T.R.%20McVicar&amp;author=B.%20Datt">Google Scholar</a></div><dt class=label><a href=#bbib31 id=ref-id-bib31 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Vapnik, 1995</a><dd class=reference><div class=contribution>V. Vapnik<div><strong class=title>The Nature of Statistical Learning Theory</strong></div></div><div class=host>Springer-Verlag, New York (1995)</div><div class="ReferenceLinks u-font-sans"><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=The%20Nature%20of%20Statistical%20Learning%20Theory&amp;publication_year=1995&amp;author=V.%20Vapnik">Google Scholar</a></div><dt class=label><a href=#bbib32 id=ref-id-bib32 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Wang et al., 2005</a><dd class=reference><div class=contribution>J.G. Wang, P. Neskovic, L.N. Cooper<div><strong class=title>Training data selection for support vector machines</strong></div></div><div class=host>Lecture Notes in Computer Science, 3610 (2005), pp. 554-564</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1007/11539087_71></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1007/11539087_71>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-26844576797&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Training%20data%20selection%20for%20support%20vector%20machines&amp;publication_year=2005&amp;author=J.G.%20Wang&amp;author=P.%20Neskovic&amp;author=L.N.%20Cooper">Google Scholar</a></div><dt class=label><a href=#bbib33 id=ref-id-bib33 data-aa-button="sd:product:journal:article:location=references:type=anchor:name=citation-name">Zhuang et al., 1994</a><dd class=reference><div class=contribution>X. Zhuang, B.A. Engel, D.F. Lozano-Garcia, R.N. Fernandez, C.J. Johannsen<div><strong class=title>Optimisation of training data required for neuro-classification</strong></div></div><div class=host>International Journal of Remote Sensing, 15 (1994), pp. 3271-3277</div><div class="ReferenceLinks u-font-sans"><div class="u-margin-m-right u-display-inline"><els-view-pdf-element doc-type=doi doc-id=10.1080/01431169408954326></els-view-pdf-element></div><a class=link target=_blank rel="noopener noreferrer" href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.1080/01431169408954326>CrossRef</a><a class=link target=_blank rel="noopener noreferrer" href="https://www-scopus-com.simsrad.net.ocs.mq.edu.au/inward/record.url?eid=2-s2.0-0028602888&amp;partnerID=10&amp;rel=R3.0.0">View Record in Scopus</a><a class=link target=_blank rel="noopener noreferrer" href="https://scholar-google-com.simsrad.net.ocs.mq.edu.au/scholar_lookup?title=Optimisation%20of%20training%20data%20required%20for%20neuro-classification&amp;publication_year=1994&amp;author=X.%20Zhuang&amp;author=B.A.%20Engel&amp;author=D.F.%20Lozano-Garcia&amp;author=R.N.%20Fernandez&amp;author=C.J.%20Johannsen">Google Scholar</a></div></dl></section></section><div id=section-cited-by><section class="ListArticles preview"><div class=PageDivider></div><header id=citing-articles-header><h2 class="u-h3 u-margin-l-ver u-font-serif">Cited by (343)</h2></header><div aria-describedby=citing-articles-header><div class="citing-articles u-margin-l-bottom"><ul><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S2352938522000830 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article0-title>Urban footprint detection from night light, optical and SAR imageries: A comparison study</h3></a><div class="article-source ellipsis">2022, Remote Sensing Applications: Society and Environment</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article0-title aria-controls=citing-articles-article0 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0273117722002976 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article1-title>Galactic component mapping of galaxy UGC 2885 by machine learning classification</h3></a><div class="article-source ellipsis">2022, Advances in Space Research</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article1-title aria-controls=citing-articles-article1 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0169743921001337 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article2-title>PLS and kernel SVM based hybrid classifier for discriminating FTIR spectrum data with limited sample size</h3></a><div class="article-source ellipsis">2021, Chemometrics and Intelligent Laboratory Systems</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article2-title aria-controls=citing-articles-article2 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243420309211 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article3-title>Ensembles of multiple spectral water indices for improving surface water classification</h3></a><div class="article-source ellipsis">2021, International Journal of Applied Earth Observation and Geoinformation</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article3-title aria-controls=citing-articles-article3 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425721000122 target=_self><h3 class="article-title ellipsis text-s" id=citing-articles-article4-title>Completing the machine learning saga in fractional snow cover estimation from MODIS Terra reflectance data: Random forests versus support vector regression</h3></a><div class="article-source ellipsis">2021, Remote Sensing of Environment</div></div><div class=buttons><button class="button-link button-link-secondary list-item-details-toggle" data-aa-button="sd:product:journal:article:location=citing-articles:type=view-details" aria-describedby=citing-articles-article4-title aria-controls=citing-articles-article4 aria-expanded=false type=button><span class=button-link-text>Show abstract</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div class="u-display-none sf-hidden" aria-hidden=true></div><li class=ListArticleItem><div class=sub-heading><a href=https://doi-org.simsrad.net.ocs.mq.edu.au/10.2174/1573405618666220117151726 target=_blank><h3 class="article-title ellipsis text-s" id=citing-articles-article5-title>Recent Advances in Classification of Brain Tumor from MR Images â€“ State of the Art Review from 2017 to 2021</h3></a><div class="article-source ellipsis">2022, Current Medical Imaging</div></div><div class=buttons></div><div class="u-display-none sf-hidden" aria-hidden=true></div></ul><a class="button-alternative button-alternative-secondary button-cited-by-more" href="http://www-scopus-com.simsrad.net.ocs.mq.edu.au/scopus/inward/citedby.url?partnerID=10&amp;rel=3.0.0&amp;eid=2-s2.0-33745756516&amp;md5=64e98b60a5bd3c2d78ab5e8d1c516d" id=citing-articles-view-all-btn target=_blank><svg focusable=false viewBox="0 0 78 128" width=32 height=32 class="icon icon-arrow-up-right"><path d="m4 36h57.07l-59.5 59.5 7.07 7.08 59.36-59.36v56.78h1e1v-74h-74z"></path></svg><span class=button-alternative-text>View all citing articles on Scopus</span></a></div></div></section></div><a class="anchor abstract-link" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/abs/pii/S0034425706001350><span class=anchor-text>View Abstract</span></a><div class=Copyright><span class=copyright-line>Copyright Â© 2006 Elsevier Inc. All rights reserved.</span></div></article><div class="u-show-from-md col-lg-6 col-md-8 pad-right"><aside class=RelatedContent aria-label="Related content"><section class="SidePanel u-margin-s-bottom"><header id=recommended-articles-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true data-aa-button="sd:product:journal:article:location=recommended-articles:type=close" type=button><span class=button-link-text><h2 class="section-title u-h4">Recommended articles</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=recommended-articles-header><div id=recommended-articles><ul><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425714000704><h3 class="article-title ellipsis text-s" id=recommended-articles-article0-title><span>Good practices for estimating area and assessing accuracy of land change</span></h3></a><div class="article-source ellipsis"><div class=source>Remote Sensing of Environment, Volume 148, 2014, pp. 42-57</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425714000704/pdfft?md5=e4c8679b33daef5658aa791138ce86eb&amp;pid=1-s2.0-S0034425714000704-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article0-title aria-controls=recommended-articles-article0 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article0 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715300377><h3 class="article-title ellipsis text-s" id=recommended-articles-article1-title><span>A 30-year (1984â€“2013) record of annual urban dynamics of Beijing City derived from Landsat data</span></h3></a><div class="article-source ellipsis"><div class=source>Remote Sensing of Environment, Volume 166, 2015, pp. 78-90</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0034425715300377/pdfft?md5=066c15359f56f05c722718c8a5b01e4a&amp;pid=1-s2.0-S0034425715300377-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article1-title aria-controls=recommended-articles-article1 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article1 aria-hidden=true></div><li class=SidePanelItem><div class=sub-heading><a href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243414001445><h3 class="article-title ellipsis text-s" id=recommended-articles-article2-title><span>Enhancing endmember selection in multiple endmember spectral mixture analysis (MESMA) for urban impervious surface area mapping using spectral angle and spectral distance parameters</span></h3></a><div class="article-source ellipsis"><div class=source>International Journal of Applied Earth Observation and Geoinformation, Volume 33, 2014, pp. 290-301</div></div></div><div class=buttons><a class="anchor side-panel-pdf-link" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/science/article/pii/S0303243414001445/pdfft?md5=b98ac063985da26f0f86eea0bafada92&amp;pid=1-s2.0-S0303243414001445-main.pdf" target=_blank rel=nofollow><svg focusable=false viewBox="0 0 32 32" width=24 height=24 class="icon icon-pdf-multicolor"><path d="M7 .362h17.875l6.763 6.1V31.64H6.948V16z" stroke=#000 stroke-width=.703 fill=#fff></path><path d="M.167 2.592H22.39V9.72H.166z" stroke=#aaa stroke-width=.315 fill=#da0000></path><path fill=#fff9f9 d="M5.97 3.638h1.62c1.053 0 1.483.677 1.488 1.564.008.96-.6 1.564-1.492 1.564h-.644v1.66h-.977V3.64m.977.897v1.34h.542c.27 0 .596-.068.596-.673-.002-.6-.32-.667-.596-.667h-.542m3.8.036v2.92h.35c.933 0 1.223-.448 1.228-1.462.008-1.06-.316-1.45-1.23-1.45h-.347m-.977-.94h1.03c1.68 0 2.523.586 2.534 2.39.01 1.688-.607 2.4-2.534 2.4h-1.03V3.64m4.305 0h2.63v.934h-1.657v.894H16.6V6.4h-1.56v2.026h-.97V3.638"></path><path d="M19.462 13.46c.348 4.274-6.59 16.72-8.508 15.792-1.82-.85 1.53-3.317 2.92-4.366-2.864.894-5.394 3.252-3.837 3.93 2.113.895 7.048-9.25 9.41-15.394zM14.32 24.874c4.767-1.526 14.735-2.974 15.152-1.407.824-3.157-13.72-.37-15.153 1.407zm5.28-5.043c2.31 3.237 9.816 7.498 9.788 3.82-.306 2.046-6.66-1.097-8.925-4.164-4.087-5.534-2.39-8.772-1.682-8.732.917.047 1.074 1.307.67 2.442-.173-1.406-.58-2.44-1.224-2.415-1.835.067-1.905 4.46 1.37 9.065z" fill=#f91d0a></path></svg><span class=anchor-text>Download PDF</span></a><button class="button-link button-link-secondary side-panel-details-toggle move-right" data-aa-button="sd:product:journal:article:location=recommended-articles:type=view-details" aria-describedby=recommended-articles-article2-title aria-controls=recommended-articles-article2 aria-expanded=false type=button><span class=button-link-text>View details</span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div><div id=recommended-articles-article2 aria-hidden=true></div></ul></div><div class="pagination u-position-relative u-padding-s-bottom"><span class=u-position-absolute></span><span class=pagination-pages-label><span class="pagination-nav u-margin-xs-hor pagination-current underline-page-number">1</span><span class="pagination-nav u-margin-xs-hor">2</span></span><span class=u-position-absolute><button class="button-link button-link-secondary next-button" data-aa-button="sd:product:journal:article:location=recommended-articles:type=Next" type=button><span class=button-link-text>Next</span><svg focusable=false viewBox="0 0 54 128" width=10.125 height=24 class="icon icon-navigate-right"><path d="m1 99l38-38-38-38 7-7 45 45-45 45z"></path></svg></button></span></div></div></section><section class="SidePanel u-margin-s-bottom"><header id=metrics-header class="side-panel-header u-margin-s-bottom"><button class="button-link side-panel-toggle is-up button-link-primary" aria-expanded=true type=button><span class=button-link-text><h2 class="section-title u-h4">Article Metrics</h2></span><svg focusable=false viewBox="0 0 92 128" width=17.25 height=24 class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></header><div aria-hidden=false aria-describedby=metrics-header><div class=plum-sciencedirect-theme><div class=PlumX-Summary><div class="pps-container pps-container-vertical plx-no-print"><div class="pps-branding pps-branding-top sf-hidden"></div><div class=pps-cols><div class="pps-col plx-citation"><div class=plx-citation><div class=pps-title>Citations</div><ul><li class=plx-citation><span class=pps-label>Citation Indexes: </span><span class=pps-count>339</span><li class=plx-citation><span class=pps-label>Policy Citations: </span><span class=pps-count>2</span></ul></div></div><div class="pps-col plx-capture"><div class=plx-capture><div class=pps-title>Captures</div><ul><li class=plx-capture><span class=pps-label>Readers: </span><span class=pps-count>276</span><li class=plx-capture><span class=pps-label>Exports-Saves: </span><span class=pps-count>3</span></ul></div></div></div><div><div class="pps-branding pps-branding-bottom"><img alt="plumX logo" src=data:image/png;base64,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 class=plx-logo></div><a target=_blank href="https://plu.mx/plum/a/?doi=10.1016/j.rse.2006.04.001&amp;theme=plum-sciencedirect-theme&amp;hideUsage=true" class=pps-seemore title="PlumX Metrics Detail Page">View details<svg fill=currentColor tabindex=-1 focusable=false width=16 height=16 viewBox="0 0 16 16" class=svg-arrow><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></a></div></div></div></div></div></section></aside></div></div><div></div></div></div><footer role=contentinfo class="els-footer u-bg-white text-xs u-padding-s-hor u-padding-m-hor-from-sm u-padding-l-hor-from-md u-padding-l-ver u-margin-l-top u-margin-xl-top-from-sm u-margin-l-top-from-md"><div class="els-footer-elsevier u-margin-m-bottom u-margin-0-bottom-from-md u-margin-s-right u-margin-m-right-from-md u-margin-l-right-from-lg"><a aria-label="Elsevier home page (opens in a new tab)" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/ target=_blank rel=nofollow><img class=footer-logo src=data:null;base64, alt="Elsevier logo with wordmark" height=64 width=58 loading=lazy></a></div><div class=els-footer-content><div class=u-remove-if-print><ul class="els-footer-links u-margin-xs-bottom" style=list-style:none><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/solutions/sciencedirect id=els-footer-about-science-direct target=_blank rel=nofollow><span class=anchor-text>About ScienceDirect</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href="https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/user/institution/login?targetURL=" id=els-footer-remote-access target=_blank rel=nofollow><span class=anchor-text>Remote access</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://sd-cart-elsevier-com.simsrad.net.ocs.mq.edu.au/? id=els-footer-shopping-cart target=_blank rel=nofollow><span class=anchor-text>Shopping cart</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=http://elsmediakits.com/ id=els-footer-advertise target=_blank rel=nofollow><span class=anchor-text>Advertise</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://service-elsevier-com.simsrad.net.ocs.mq.edu.au/app/contact/supporthub/sciencedirect/ id=els-footer-contact-support target=_blank rel=nofollow><span class=anchor-text>Contact and support</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/elsevier-website-terms-and-conditions id=els-footer-terms-condition target=_blank rel=nofollow><span class=anchor-text>Terms and conditions</span></a><li><a class="anchor u-display-block u-clr-grey8 u-margin-s-bottom u-margin-0-bottom-from-sm u-margin-m-right-from-sm u-margin-l-right-from-md" href=https://www-elsevier-com.simsrad.net.ocs.mq.edu.au/legal/privacy-policy id=els-footer-privacy-policy target=_blank rel=nofollow><span class=anchor-text>Privacy policy</span></a></ul></div><p id=els-footer-cookie-message class=u-remove-if-print>We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the <a class="anchor u-clr-grey8 u-margin-0-right" href=https://www-sciencedirect-com.simsrad.net.ocs.mq.edu.au/legal/use-of-cookies target=_blank rel=nofollow><span class=anchor-text><strong>use of cookies</strong></span></a>.<p id=els-footer-copyright>Copyright Â© 2022 Elsevier B.V. or its licensors or contributors. <span class=u-remove-if-print>ScienceDirectÂ® is a registered trademark of Elsevier B.V.</span><p class="u-remove-if-not-print sf-hidden">ScienceDirectÂ® is a registered trademark of Elsevier B.V.</p></div><div class="els-footer-relx u-margin-0-top u-margin-m-top-from-xs u-margin-0-top-from-md"><a aria-label="RELX home page (opens in a new tab)" id=els-footer-relx href=https://www.relx.com/ target=_blank rel=nofollow><img loading=lazy src=data:null;base64, width=93 height=20 alt="RELX group home page"></a></div></footer></div></section></div></div></div>
<div id=UMS_TOOLTIP style=position:absolute;cursor:pointer;z-index:2147483647;background:transparent;top:-100000px;left:-100000px></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><div class=js-react-modal></div><button aria-label=Feedback type=button id=_pendo-badge_9BcFvkCLLiElWp6hocDK3ZG6Z4E data-layout=badgeBlank class="_pendo-badge _pendo-badge_" style="z-index:19000;margin:0px;line-height:1;font-size:0px;background:rgba(255,255,255,0);padding:0px;height:32px;width:128px;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;border:0px;float:none;vertical-align:baseline;cursor:pointer;position:absolute;top:28378px;left:1423px"><img id=pendo-image-badge-c2b2bcc0 src=data:image/svg+xml;base64,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 data-_pendo-image-1 class="_pendo-image _pendo-badge-image" style="display:block;height:32px;width:128px;padding:0px;margin:0px;line-height:1;border:none;box-shadow:rgb(136,136,136) 0px 0px 0px 0px;float:none;vertical-align:baseline"></button><umsdataelement id=UMSSendDataEventElement></umsdataelement><div id=tmtoolbar_manual_rating_injected style=display:none>init</div>