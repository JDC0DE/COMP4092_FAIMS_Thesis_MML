
@article{maggiori_convolutional_2017,
	title = {Convolutional {Neural} {Networks} for {Large}-{Scale} {Remote}-{Sensing} {Image} {Classification}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2016.2612821},
	abstract = {We propose an end-to-end framework for the dense, pixelwise classification of satellite imagery with convolutional neural networks (CNNs). In our framework, CNNs are directly trained to produce classification maps out of the input images. We first devise a fully convolutional architecture and demonstrate its relevance to the dense classification problem. We then address the issue of imperfect training data through a two-step training approach: CNNs are first initialized by using a large amount of possibly inaccurate reference data, and then refined on a small amount of accurately labeled data. To complete our framework, we design a multiscale neuron module that alleviates the common tradeoff between recognition and precise localization. A series of experiments show that our networks consider a large amount of context to provide fine-grained classification maps.},
	number = {2},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
	month = feb,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Biological neural networks, Classification, Context, convolutional neural networks (CNNs), deep learning, Neurons, Remote sensing, satellite images, Satellites, Training, Training data},
	pages = {645--657},
	file = {IEEE Xplore Abstract Record:files/2/7592858.html:text/html;IEEE Xplore Full Text PDF:files/3/Maggiori et al. - 2017 - Convolutional Neural Networks for Large-Scale Remo.pdf:application/pdf},
}

@misc{a_what_2019,
	type = {Forum post},
	title = {What is the difference between convolutional neural networks and deep learning?},
	url = {https://stats.stackexchange.com/q/234891},
	urldate = {2022-09-08},
	journal = {Cross Validated},
	author = {A, Aadnan Farooq},
	month = mar,
	year = {2019},
	file = {Snapshot:files/6/what-is-the-difference-between-convolutional-neural-networks-and-deep-learning.html:text/html},
}

@misc{miniquark_answer_2016,
	title = {Answer to "{What} is the difference between convolutional neural networks and deep learning?"},
	shorttitle = {Answer to "{What} is the difference between convolutional neural networks and deep learning?},
	url = {https://stats.stackexchange.com/a/234894},
	urldate = {2022-09-08},
	journal = {Cross Validated},
	author = {MiniQuark},
	month = sep,
	year = {2016},
	file = {Snapshot:files/7/what-is-the-difference-between-convolutional-neural-networks-and-deep-learning.html:text/html},
}

@article{abdel_magid_image_2020,
	title = {Image classification on {IoT} edge devices: profiling and modeling},
	volume = {23},
	issn = {1573-7543},
	shorttitle = {Image classification on {IoT} edge devices},
	url = {https://doi.org/10.1007/s10586-019-02971-9},
	doi = {10.1007/s10586-019-02971-9},
	abstract = {With the advent of powerful, low-cost IoT systems, processing data closer to where the data originates, known as edge computing, has become an increasingly viable option. In addition to lowering the cost of networking infrastructures, edge computing reduces edge-cloud delay, which is essential for mission-critical applications. In this paper, we show the feasibility and study the performance of image classification using IoT devices. Specifically, we explore the relationships between various factors of image classification algorithms that may affect energy consumption, such as dataset size, image resolution, algorithm type, algorithm phase, and device hardware. In order to provide a means of predicting the energy consumption of an edge device performing image classification, we investigate the usage of three machine learning algorithms using the data generated from our experiments. The performance as well as the trade-offs for using linear regression, Gaussian process, and random forests are discussed and validated. Our results indicate that the random forest model outperforms the two former algorithms, with an R-squared value of 0.95 and 0.79 for two different validation datasets. The random forest also served as a feature extraction mechanism which enabled us to identify which predictor variables influenced our model the most.},
	language = {en},
	number = {2},
	urldate = {2022-09-08},
	journal = {Cluster Comput},
	author = {Abdel Magid, Salma and Petrini, Francesco and Dezfouli, Behnam},
	month = jun,
	year = {2020},
	keywords = {Accuracy, Edge and fog computing, Energy efficiency, Machine learning},
	pages = {1025--1043},
	file = {Full Text PDF:files/9/Abdel Magid et al. - 2020 - Image classification on IoT edge devices profilin.pdf:application/pdf},
}

@article{dhar_survey_2021,
	title = {A {Survey} of {On}-{Device} {Machine} {Learning}: {An} {Algorithms} and {Learning} {Theory} {Perspective}},
	volume = {2},
	issn = {2691-1914},
	shorttitle = {A {Survey} of {On}-{Device} {Machine} {Learning}},
	url = {https://doi.org/10.1145/3450494},
	doi = {10.1145/3450494},
	abstract = {The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing numbers of smart devices and improved hardware, there is interest in performing model training on the device. Given this surge in interest, a comprehensive survey of the field from a device-agnostic perspective sets the stage for both understanding the state of the art and for identifying open challenges and future avenues of research. However, on-device learning is an expansive field with connections to a large number of related topics in AI and machine learning (including online learning, model adaptation, one/few-shot learning, etc.). Hence, covering such a large number of topics in a single survey is impractical. This survey finds a middle ground by reformulating the problem of on-device learning as resource constrained learning where the resources are compute and memory. This reformulation allows tools, techniques, and algorithms from a wide variety of research areas to be compared equitably. In addition to summarizing the state of the art, the survey also identifies a number of challenges and next steps for both the algorithmic and theoretical aspects of on-device learning.},
	number = {3},
	urldate = {2022-09-08},
	journal = {ACM Trans. Internet Things},
	author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi (Jason) and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
	month = jul,
	year = {2021},
	pages = {15:1--15:49},
}

@article{noauthor_neural_2021,
	title = {Neural interface systems with on-device computing: machine learning and neuromorphic architectures},
	volume = {72},
	issn = {0958-1669},
	shorttitle = {Neural interface systems with on-device computing},
	url = {https://www.sciencedirect.com/science/article/pii/S0958166921001993},
	doi = {10.1016/j.copbio.2021.10.012},
	abstract = {Development of neural interface and brain-machine interface (BMI) systems enables the treatment of neurological disorders including cognitive, sensory…},
	language = {en},
	urldate = {2022-09-08},
	journal = {Current Opinion in Biotechnology},
	month = dec,
	year = {2021},
	note = {Publisher: Elsevier Current Trends},
	pages = {95--101},
	file = {Full Text:files/14/2021 - Neural interface systems with on-device computing.pdf:application/pdf;Snapshot:files/15/S0958166921001993.html:text/html},
}

@misc{noauthor_difference_2020,
	title = {The {Difference} {Between} {Deep} {Learning} {Training} and {Inference}},
	url = {https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/The-Difference-Between-Deep-Learning-Training-and-Inference/post/1335634},
	abstract = {My last “AI 101” post covered the difference between artificial intelligence, machine learning, and deep learning. In this post, I’ll cover deep learning training and inference -- two key processes associated with developing and using AI. Training: Creating the deep learning model In the last post, ...},
	language = {en},
	urldate = {2022-09-08},
	month = jun,
	year = {2020},
	note = {Section: Artificial Intelligence (AI)},
	file = {Snapshot:files/17/1335634.html:text/html},
}

@misc{noauthor_machine_nodate,
	title = {Machine learning on mobile: on the device or in the cloud?},
	url = {https://machinethink.net/blog/machine-learning-device-or-cloud/},
	urldate = {2022-09-08},
	file = {Machine learning on mobile\: on the device or in the cloud?:files/19/machine-learning-device-or-cloud.html:text/html},
}

@misc{noauthor_3_nodate,
	title = {3 {Types} of {Machine} {Learning} {You} {Should} {Know}},
	url = {https://www.coursera.org/articles/types-of-machine-learning},
	abstract = {Machine learning is an exciting field and a subset of artificial intelligence. Use this guide to discover more about real-world applications, and the three types of machine learning you should know.},
	language = {en},
	urldate = {2022-09-08},
	journal = {Coursera},
	file = {Snapshot:files/21/types-of-machine-learning.html:text/html},
}

@misc{noauthor_fast_nodate,
	title = {Fast client-side {ML} with {TensorFlow}.js, by {Ann} {Yuan} ({Google})},
	url = {https://www.w3.org/2020/06/machine-learning-workshop/talks/fast_client_side_ml_with_tensorflow_js.html},
	language = {en},
	urldate = {2022-09-08},
	file = {Snapshot:files/23/fast_client_side_ml_with_tensorflow_js.html:text/html},
}

@misc{noauthor_performance_nodate,
	title = {Performance best practices {\textbar} {TensorFlow} {Lite}},
	url = {https://www.tensorflow.org/lite/performance/best_practices},
	language = {en},
	urldate = {2022-09-08},
	journal = {TensorFlow},
	file = {Snapshot:files/25/best_practices.html:text/html},
}

@article{ma_review_2017,
	title = {A review of supervised object-based land-cover image classification},
	volume = {130},
	issn = {0924-2716},
	url = {https://www.sciencedirect.com/science/article/pii/S092427161630661X},
	doi = {10.1016/j.isprsjprs.2017.06.001},
	abstract = {Object-based image classification for land-cover mapping purposes using remote-sensing imagery has attracted significant attention in recent years. Numerous studies conducted over the past decade have investigated a broad array of sensors, feature selection, classifiers, and other factors of interest. However, these research results have not yet been synthesized to provide coherent guidance on the effect of different supervised object-based land-cover classification processes. In this study, we first construct a database with 28 fields using qualitative and quantitative information extracted from 254 experimental cases described in 173 scientific papers. Second, the results of the meta-analysis are reported, including general characteristics of the studies (e.g., the geographic range of relevant institutes, preferred journals) and the relationships between factors of interest (e.g., spatial resolution and study area or optimal segmentation scale, accuracy and number of targeted classes), especially with respect to the classification accuracy of different sensors, segmentation scale, training set size, supervised classifiers, and land-cover types. Third, useful data on supervised object-based image classification are determined from the meta-analysis. For example, we find that supervised object-based classification is currently experiencing rapid advances, while development of the fuzzy technique is limited in the object-based framework. Furthermore, spatial resolution correlates with the optimal segmentation scale and study area, and Random Forest (RF) shows the best performance in object-based classification. The area-based accuracy assessment method can obtain stable classification performance, and indicates a strong correlation between accuracy and training set size, while the accuracy of the point-based method is likely to be unstable due to mixed objects. In addition, the overall accuracy benefits from higher spatial resolution images (e.g., unmanned aerial vehicle) or agricultural sites where it also correlates with the number of targeted classes. More than 95.6\% of studies involve an area less than 300ha, and the spatial resolution of images is predominantly between 0 and 2m. Furthermore, we identify some methods that may advance supervised object-based image classification. For example, deep learning and type-2 fuzzy techniques may further improve classification accuracy. Lastly, scientists are strongly encouraged to report results of uncertainty studies to further explore the effects of varied factors on supervised object-based image classification.},
	language = {en},
	urldate = {2022-09-18},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Ma, Lei and Li, Manchun and Ma, Xiaoxue and Cheng, Liang and Du, Peijun and Liu, Yongxue},
	month = aug,
	year = {2017},
	keywords = {GEOBIA, Land-cover mapping, Meta-analysis, OBIA, Review, Supervised object-based classification, General IC},
	pages = {277--293},
	file = {ScienceDirect Full Text PDF:files/27/Ma et al. - 2017 - A review of supervised object-based land-cover ima.pdf:application/pdf;ScienceDirect Snapshot:files/28/S092427161630661X.html:text/html},
}

@article{lu_survey_2007,
	title = {A survey of image classification methods and techniques for improving classification performance},
	volume = {28},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160600746456},
	doi = {10.1080/01431160600746456},
	abstract = {Image classification is a complex process that may be affected by many factors. This paper examines current practices, problems, and prospects of image classification. The emphasis is placed on the summarization of major advanced classification approaches and the techniques used for improving classification accuracy. In addition, some important issues affecting classification performance are discussed. This literature review suggests that designing a suitable image‐processing procedure is a prerequisite for a successful classification of remotely sensed data into a thematic map. Effective use of multiple features of remotely sensed data and the selection of a suitable classification method are especially significant for improving classification accuracy. Non‐parametric classifiers such as neural network, decision tree classifier, and knowledge‐based classification have increasingly become important approaches for multisource data classification. Integration of remote sensing, geographical information systems (GIS), and expert system emerges as a new research frontier. More research, however, is needed to identify and reduce uncertainties in the image‐processing chain to improve classification accuracy.},
	number = {5},
	urldate = {2022-09-18},
	journal = {International Journal of Remote Sensing},
	author = {Lu, D. and Weng, Q.},
	month = mar,
	year = {2007},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01431160600746456},
	keywords = {General IC},
	pages = {823--870},
}

@misc{phd_techniques_nodate,
	title = {Techniques: {A} {Review}},
	shorttitle = {Techniques},
	abstract = {Satellite image classification process involves grouping the image pixel values into meaningful categories. Several satellite image classification methods and techniques are available. Satellite image classification methods can be broadly classified into three categories 1) automatic 2) manual and 3) hybrid. All three methods have their own advantages and disadvantages. Majority of the satellite image classification methods fall under first category. Satellite image classification needs selection of appropriate classification method based on the requirements. The current research work is a study on satellite image classification methods and techniques. The research work also compares various researcher’s comparative results on satellite image classification methods.},
	author = {Phd, Sunitha Abburu and India, Hosur Tamilnadu and Golla, Suresh Babu and India, Hosur Tamilnadu},
	keywords = {General IC},
	file = {Citeseer - Full Text PDF:files/31/Phd et al. - Techniques A Review.pdf:application/pdf;Citeseer - Snapshot:files/32/download.html:text/html},
}

@inproceedings{nath_survey_2014,
	title = {A survey of image classification methods and techniques},
	doi = {10.1109/ICCICCT.2014.6993023},
	abstract = {In this paper, we review the current activity of image classification methodologies and techniques. Image classification is a complex process which depends upon various factors. Here, we discuss about the current techniques, problems as well as prospects of image classification. The main focus will be on advanced classification techniques which are used for improving classification accuracy. Additionally, some important issues relating to classification performance are also discussed.},
	booktitle = {2014 {International} {Conference} on {Control}, {Instrumentation}, {Communication} and {Computational} {Technologies} ({ICCICCT})},
	author = {Nath, Siddhartha Sankar and Mishra, Girish and Kar, Jajnyaseni and Chakraborty, Sayan and Dey, Nilanjan},
	month = jul,
	year = {2014},
	keywords = {Remote sensing, Training, General IC, Artificial Neural Network (ANN), Classification algorithms, Image classification, Sensors, Support Vector Machine (SVM), Support vector machines, Synthetic aperture radar, Synthetic Aperture Radar (SAR)},
	pages = {554--557},
	file = {IEEE Xplore Abstract Record:files/34/6993023.html:text/html},
}

@inproceedings{sultana_advancements_2018,
	title = {Advancements in {Image} {Classification} using {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICRCICN.2018.8718718},
	abstract = {Convolutional Neural Network (CNN) is the state-of-the-art for image classification task. Here we have briefly discussed different components of CNN. In this paper, We have explained different CNN architectures for image classification. Through this paper, we have shown advancements in CNN from LeNet-5 to latest SENet model. We have discussed the model description and training details of each model. We have also drawn a comparison among those models.},
	booktitle = {2018 {Fourth} {International} {Conference} on {Research} in {Computational} {Intelligence} and {Communication} {Networks} ({ICRCICN})},
	author = {Sultana, Farhana and Sufian, Abu and Dutta, Paramartha},
	month = nov,
	year = {2018},
	keywords = {Training, General IC, Image classification, AlexNet, Capsnet, Computational modeling, Computer architecture, Convolution, Convolutional Neural Network, Convolutional neural networks, Deep learning, DenseNet, ResNet, SENet, Visualization},
	pages = {122--129},
	file = {IEEE Xplore Abstract Record:files/36/8718718.html:text/html;Submitted Version:files/37/Sultana et al. - 2018 - Advancements in Image Classification using Convolu.pdf:application/pdf},
}

@inproceedings{al-saffar_review_2017,
	title = {Review of deep convolution neural network in image classification},
	doi = {10.1109/ICRAMET.2017.8253139},
	abstract = {With the development of large data age, Convolutional neural networks (CNNs) with more hidden layers have more complex network structure and more powerful feature learning and feature expression abilities than traditional machine learning methods. The convolution neural network model trained by the deep learning algorithm has made remarkable achievements in many large-scale identification tasks in the field of computer vision since its introduction. This paper first introduces the rise and development of deep learning and convolution neural network, and summarizes the basic model structure, convolution feature extraction and pooling operation of convolution neural network. Then, the research status and development trend of convolution neural network model based on deep learning in image classification are reviewed, which is mainly introduced from the aspects of typical network structure construction, training method and performance. Finally, some problems in the current research are briefly summarized and discussed, and the new direction of future development is forecasted.},
	booktitle = {2017 {International} {Conference} on {Radar}, {Antenna}, {Microwave}, {Electronics}, and {Telecommunications} ({ICRAMET})},
	author = {Al-Saffar, Ahmed Ali Mohammed and Tao, Hai and Talab, Mohammed Ahmed},
	month = oct,
	year = {2017},
	keywords = {Biological neural networks, Training, Image classification, Convolution, Deep learning, convolution neural network, Feature extraction, Image Classification, image recognition, Image recognition},
	pages = {26--31},
	file = {IEEE Xplore Abstract Record:files/39/8253139.html:text/html},
}

@article{sharma_analysis_2018,
	series = {International {Conference} on {Computational} {Intelligence} and {Data} {Science}},
	title = {An {Analysis} {Of} {Convolutional} {Neural} {Networks} {For} {Image} {Classification}},
	volume = {132},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050918309335},
	doi = {10.1016/j.procs.2018.05.198},
	abstract = {This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN’s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN’s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.},
	language = {en},
	urldate = {2022-09-18},
	journal = {Procedia Computer Science},
	author = {Sharma, Neha and Jain, Vibhor and Mishra, Anju},
	month = jan,
	year = {2018},
	keywords = {CNN, Deep Learning, Neural network, Object classification, Object detection},
	pages = {377--384},
	file = {ScienceDirect Full Text PDF:files/41/Sharma et al. - 2018 - An Analysis Of Convolutional Neural Networks For I.pdf:application/pdf;ScienceDirect Snapshot:files/42/S1877050918309335.html:text/html},
}

@article{korytkowski_fast_2016,
	title = {Fast image classification by boosting fuzzy classifiers},
	volume = {327},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025515006180},
	doi = {10.1016/j.ins.2015.08.030},
	abstract = {This paper presents a novel approach to visual objects classification based on generating simple fuzzy classifiers using local image features to distinguish between one known class and other classes. Boosting meta-learning is used to find the most representative local features. The proposed approach is tested on a state-of-the-art image dataset and compared with the bag-of-features image representation model combined with the Support Vector Machine classification. The novel method gives better classification accuracy and the time of learning and testing process is more than 30\% shorter.},
	language = {en},
	urldate = {2022-09-18},
	journal = {Information Sciences},
	author = {Korytkowski, Marcin and Rutkowski, Leszek and Scherer, Rafał},
	month = jan,
	year = {2016},
	keywords = {General IC, Boosting meta-learning, Fuzzy classification, Visual object categorization},
	pages = {175--182},
	file = {ScienceDirect Full Text PDF:files/44/Korytkowski et al. - 2016 - Fast image classification by boosting fuzzy classi.pdf:application/pdf;ScienceDirect Snapshot:files/45/S0020025515006180.html:text/html},
}

@article{ding_decode_2019,
	title = {{DECODE}: {Deep} {Confidence} {Network} for {Robust} {Image} {Classification}},
	volume = {28},
	issn = {1941-0042},
	shorttitle = {{DECODE}},
	doi = {10.1109/TIP.2019.2902115},
	abstract = {Recent years have witnessed the success of deep convolutional neural networks for image classification and many related tasks. It should be pointed out that the existing training strategies assume that there is a clean dataset for model learning. In elaborately constructed benchmark datasets, deep network has yielded promising performance under the assumption. However, in real-world applications, it is burdensome and expensive to collect sufficient clean training samples. On the other hand, collecting noisy labeled samples is very economical and practical, especially with the rapidly increasing amount of visual data in the web. Unfortunately, the accuracy of current deep models may drop dramatically even with 5\%-10\% label noise. Therefore, enabling label noise resistant classification has become a crucial issue in the data driven deep learning approaches. In this paper, we propose a DEep COnfiDEnce network (DECODE) to address this issue. In particular, based on the distribution of mislabeled data, we adopt a confidence evaluation module that is able to determine the confidence that a sample is mislabeled. With the confidence, we further use a weighting strategy to assign different weights to different samples so that the model pays less attention to low confidence data, which is more likely to be noise. In this way, the deep model is more robust to label noise. DECODE is designed to be general, such that it can be easily combined with existing studies. We conduct extensive experiments on several datasets, and the results validate that DECODE can improve the accuracy of deep models trained with noisy data.},
	number = {8},
	journal = {IEEE Transactions on Image Processing},
	author = {Ding, Guiguang and Guo, Yuchen and Chen, Kai and Chu, Chaoqun and Han, Jungong and Dai, Qionghai},
	month = aug,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {Training, Training data, Deep learning, Benchmark testing, confidence model, Data models, Error analysis, Noise measurement, robustness, Robustness, General IC - Alternate Approach},
	pages = {3752--3765},
	file = {IEEE Xplore Abstract Record:files/48/8653989.html:text/html;IEEE Xplore Full Text PDF:files/47/Ding et al. - 2019 - DECODE Deep Confidence Network for Robust Image C.pdf:application/pdf},
}

@article{maggiori_convolutional_2017-1,
	title = {Convolutional {Neural} {Networks} for {Large}-{Scale} {Remote}-{Sensing} {Image} {Classification}},
	volume = {55},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2016.2612821},
	abstract = {We propose an end-to-end framework for the dense, pixelwise classification of satellite imagery with convolutional neural networks (CNNs). In our framework, CNNs are directly trained to produce classification maps out of the input images. We first devise a fully convolutional architecture and demonstrate its relevance to the dense classification problem. We then address the issue of imperfect training data through a two-step training approach: CNNs are first initialized by using a large amount of possibly inaccurate reference data, and then refined on a small amount of accurately labeled data. To complete our framework, we design a multiscale neuron module that alleviates the common tradeoff between recognition and precise localization. A series of experiments show that our networks consider a large amount of context to provide fine-grained classification maps.},
	number = {2},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
	month = feb,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Biological neural networks, Classification, Context, convolutional neural networks (CNNs), deep learning, Neurons, Remote sensing, satellite images, Satellites, Training, Training data, General IC},
	pages = {645--657},
	file = {IEEE Xplore Abstract Record:files/50/7592858.html:text/html;IEEE Xplore Full Text PDF:files/51/Maggiori et al. - 2017 - Convolutional Neural Networks for Large-Scale Remo.pdf:application/pdf},
}

@article{chan_pcanet_2015,
	title = {{PCANet}: {A} {Simple} {Deep} {Learning} {Baseline} for {Image} {Classification}?},
	volume = {24},
	issn = {1941-0042},
	shorttitle = {{PCANet}},
	doi = {10.1109/TIP.2015.2475625},
	abstract = {In this paper, we propose a very simple deep learning network for image classification that is based on very basic data processing components: 1) cascaded principal component analysis (PCA); 2) binary hashing; and 3) blockwise histograms. In the proposed architecture, the PCA is employed to learn multistage filter banks. This is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus called the PCA network (PCANet) and can be extremely easily and efficiently designed and learned. For comparison and to provide a better understanding, we also introduce and study two simple variations of PCANet: 1) RandNet and 2) LDANet. They share the same topology as PCANet, but their cascaded filters are either randomly selected or learned from linear discriminant analysis. We have extensively tested these basic networks on many benchmark visual data sets for different tasks, including Labeled Faces in the Wild (LFW) for face verification; the MultiPIE, Extended Yale B, AR, Facial Recognition Technology (FERET) data sets for face recognition; and MNIST for hand-written digit recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state-of-the-art features either prefixed, highly hand-crafted, or carefully learned [by deep neural networks (DNNs)]. Even more surprisingly, the model sets new records for many classification tasks on the Extended Yale B, AR, and FERET data sets and on MNIST variations. Additional experiments on other public data sets also demonstrate the potential of PCANet to serve as a simple but highly competitive baseline for texture classification and object recognition.},
	number = {12},
	journal = {IEEE Transactions on Image Processing},
	author = {Chan, Tsung-Han and Jia, Kui and Gao, Shenghua and Lu, Jiwen and Zeng, Zinan and Ma, Yi},
	month = dec,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Image Processing},
	keywords = {deep learning, Training, Machine learning, General IC, Feature extraction, Deep Learning, Convolution neural network, Convolution Neural Network, Face, face recognition, Face recognition, Face Recognition, handwritten digit recognition, Handwritten Digit Recognition, Histograms, LDA network, LDA Network, object classification, Object Classification, PCA network, PCA Network, Principal component analysis, random network, Random Network},
	pages = {5017--5032},
	file = {IEEE Xplore Abstract Record:files/54/7234886.html:text/html;IEEE Xplore Full Text PDF:files/53/Chan et al. - 2015 - PCANet A Simple Deep Learning Baseline for Image .pdf:application/pdf},
}

@article{gao_defeatnetdeep_2016,
	title = {{DEFEATnet}—{A} {Deep} {Conventional} {Image} {Representation} for {Image} {Classification}},
	volume = {26},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2015.2389413},
	abstract = {To study underlying possibilities for the successes of conventional image representation and deep neural networks (DNNs) in image representation, we propose a deep feature extraction, encoding, and pooling network (DEFEATnet) architecture, which is a marriage between conventional image representation approaches and DNNs. In particular, in DEFEATnet, each layer consists of three components: feature extraction, feature encoding, and pooling. The primary advantage of DEFEATnet is twofold. First, it consolidates the prior knowledge (e.g., translation invariance) from extracting, encoding, and pooling handcrafted features, as in the conventional feature representation approaches. Second, it represents the object parts at different granularities by gradually increasing the local receptive fields in different layers, as in DNNs. Moreover, DEFEATnet is a generalized framework that can readily incorporate all types of local features as well as all kinds of well-designed feature encoding and pooling methods. Since prior knowledge is preserved in DEFEATnet, it is especially useful for image representation on small/medium size data sets, where DNNs usually fail due to the lack of sufficient training data. Promising experimental results clearly show that DEFEATnets outperform shallow conventional image representation approaches by a large margin when the same type of features, feature encoding and pooling are used. The extensive experiments also demonstrate the effectiveness of the deep architecture of our DEFEATnet in improving the robustness for image presentation.},
	number = {3},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Gao, Shenghua and Duan, Lixin and Tsang, Ivor W.},
	month = mar,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {General IC, Feature extraction, Robustness, Conventional image representation, Conventional Image Representation, deep architecture, Deep Architecture, Encoding, feature encoding, Feature Encoding, Image coding, Image representation, local max pooling, Local Max Pooling, Neural networks},
	pages = {494--505},
	file = {IEEE Xplore Abstract Record:files/57/7005474.html:text/html;IEEE Xplore Full Text PDF:files/56/Gao et al. - 2016 - DEFEATnet—A Deep Conventional Image Representation.pdf:application/pdf},
}

@misc{noauthor_limnology_nodate,
	title = {Limnology and {Oceanography}: {Methods}},
	shorttitle = {Limnology and {Oceanography}},
	url = {https://aslopubs.onlinelibrary.wiley.com/journal/15415856},
	abstract = {Click on the title to browse this journal},
	language = {en},
	urldate = {2022-09-18},
	journal = {ASLO},
	doi = {10.1002/(ISSN)1541-5856},
	note = {ISSN: 1541-5856},
	file = {Snapshot:files/59/15415856.html:text/html},
}

@article{gonzalez_validation_2017,
	title = {Validation methods for plankton image classification systems},
	volume = {15},
	issn = {1541-5856},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/lom3.10151},
	doi = {10.1002/lom3.10151},
	abstract = {In recent decades, the automatic study and analysis of plankton communities using imaging techniques has advanced significantly. The effectiveness of these automated systems appears to have improved, reaching acceptable levels of accuracy. However, plankton ecologists often find that classification systems do not work as well as expected when applied to new samples. This paper proposes a methodology to assess the efficacy of learned models which takes into account the fact that the data distribution (the plankton composition of the sample) can vary between the model building phase and the production phase. As opposed to most validation methods that consider the individual organism as the unit of validation, our approach uses a validation-by-sample, which is more appropriate when the objective is to estimate the abundance of different morphological groups. We argue that, in these cases, the base unit to correctly estimate the error is the sample, not the individual. Thus, model assessment processes require groups of samples with sufficient variability in order to provide precise error estimates.},
	language = {en},
	number = {3},
	urldate = {2022-09-18},
	journal = {Limnology and Oceanography: Methods},
	author = {González, Pablo and Álvarez, Eva and Díez, Jorge and López-Urrutia, Ángel and del Coz, Juan José},
	year = {2017},
	note = {\_eprint: https://aslopubs.onlinelibrary.wiley.com/doi/pdf/10.1002/lom3.10151},
	keywords = {General IC},
	pages = {221--237},
	file = {Full Text PDF:files/61/González et al. - 2017 - Validation methods for plankton image classificati.pdf:application/pdf;Snapshot:files/62/lom3.html:text/html},
}

@article{vailaya_image_1998,
	title = {{ON} {IMAGE} {CLASSIFICATION}: {CITY} {IMAGES} {VS}. {LANDSCAPES}},
	volume = {31},
	issn = {0031-3203},
	shorttitle = {{ON} {IMAGE} {CLASSIFICATION}},
	url = {https://www.sciencedirect.com/science/article/pii/S003132039800079X},
	doi = {10.1016/S0031-3203(98)00079-X},
	abstract = {Grouping images into semantically meaningful categories using low-level visual features is a challenging and important problem in content-based image retrieval. Based on these groupings, effective indices can be built for an image database. In this paper, we show how a specific high-level classification problem (city images vs landscapes) can be solved from relatively simple low-level features geared for the particular classes. We have developed a procedure to qualitatively measure the saliency of a feature towards a classification problem based on the plot of the intra-class and inter-class distance distributions. We use this approach to determine the discriminative power of the following features: color histogram, color coherence vector, DCT coefficient, edge direction histogram, and edge direction coherence vector. We determine that the edge direction-based features have the most discriminative power for the classification problem of interest here. A weighted k-NN classifier is used for the classification which results in an accuracy of 93.9\% when evaluated on an image database of 2716 images using the leave-one-out method. This approach has been extended to further classify 528 landscape images into forests, mountains, and sunset/sunrise classes. First, the input images are classified as sunset/sunrise images vs forest \& mountain images (94.5\% accuracy) and then the forest \& mountain images are classified as forest images or mountain images (91.7\% accuracy). We are currently identifying further semantic classes to assign to images as well as extracting low level features which are salient for these classes. Our final goal is to combine multiple 2-class classifiers into a single hierarchical classifier.},
	language = {en},
	number = {12},
	urldate = {2022-09-18},
	journal = {Pattern Recognition},
	author = {Vailaya, ADITYA and Jain, ANIL and Zhang, HONG JIANG},
	month = dec,
	year = {1998},
	keywords = {General IC, Image classification, Clustering, Content-based retrieval, Image database, Salient features, Similarity},
	pages = {1921--1935},
	file = {ScienceDirect Full Text PDF:files/65/Vailaya et al. - 1998 - ON IMAGE CLASSIFICATION CITY IMAGES VS. LANDSCAPE.pdf:application/pdf;ScienceDirect Snapshot:files/64/S003132039800079X.html:text/html},
}

@article{foody_use_2006,
	title = {The use of small training sets containing mixed pixels for accurate hard image classification: {Training} on mixed spectral responses for classification by a {SVM}},
	volume = {103},
	issn = {0034-4257},
	shorttitle = {The use of small training sets containing mixed pixels for accurate hard image classification},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425706001350},
	doi = {10.1016/j.rse.2006.04.001},
	abstract = {The accuracy of a supervised image classification is a function of the training data used in its generation. It is, therefore, critical that the training stage of a supervised classification is designed to provide the necessary information. Guidance on the design of the training stage of a classification typically calls for the use of a large sample of randomly selected pure pixels in order to characterise the classes. Such guidance is generally made without regard to the specific nature of the application in-hand, including the classifier to be used. The design of the training stage should really be based on the classifier to be used since individual training cases can vary in value as can any one training set to a range of classifiers. It is argued here that the training stage can be designed on the basis of the way the classifier operates and with emphasis on the desire to separate the classes rather than describe them. An approach to the training of a support vector machine (SVM) classifier that is the opposite of that generally promoted for training set design is suggested. This approach uses a small sample of mixed spectral responses drawn from purposefully selected locations (geographical boundaries) in training. The approach is based on mixed pixels which are normally masked-out of analyses as undesirable and problematic. A sample of such data should, however, be easier and cheaper to acquire than that suggested by conventional approaches. This new approach to training set design was evaluated against conventional approaches with a set of classifications of agricultural crops from satellite sensor data. The main result was that classifications derived from the use of the mixed spectral responses and the conventional approach did not differ significantly, with the overall accuracy of classifications generally ∼92\%.},
	language = {en},
	number = {2},
	urldate = {2022-09-18},
	journal = {Remote Sensing of Environment},
	author = {Foody, Giles M. and Mathur, Ajay},
	month = jul,
	year = {2006},
	keywords = {Classification, General IC, Mixed pixel, Support vector machine, Training set},
	pages = {179--189},
	file = {ScienceDirect Full Text PDF:files/67/Foody and Mathur - 2006 - The use of small training sets containing mixed pi.pdf:application/pdf;ScienceDirect Snapshot:files/68/S0034425706001350.html:text/html},
}

@inproceedings{ren_novel_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Novel} {Image} {Classification} {Method} with {CNN}-{XGBoost} {Model}},
	isbn = {978-3-319-64185-0},
	doi = {10.1007/978-3-319-64185-0_28},
	abstract = {Image classification problem is one of most important research directions in image processing and has become the focus of research in many years due to its diversity and complexity of image information. In view of the existing image classification models’ failure to fully utilize the information of images, this paper proposes a novel image classification method of combining the Convolutional Neural Network (CNN) and eXtreme Gradient Boosting (XGBoost), which are two outstanding classifiers. The presented CNN-XGBoost model provides more precise output by integrating CNN as a trainable feature extractor to automatically obtain features from input and XGBoost as a recognizer in the top level of the network to produce results. Experiments are implemented on the well-known MNIST and CIFAR-10 databases. The results prove that the new method performs better compared with other methods on the same databases, which verify the effectiveness of the proposed method in image classification problem.},
	language = {en},
	booktitle = {Digital {Forensics} and {Watermarking}},
	publisher = {Springer International Publishing},
	author = {Ren, Xudie and Guo, Haonan and Li, Shenghong and Wang, Shilin and Li, Jianhua},
	editor = {Kraetzer, Christian and Shi, Yun-Qing and Dittmann, Jana and Kim, Hyoung Joong},
	year = {2017},
	keywords = {General IC, Image classification, Convolutional Neural Network, eXtreme Gradient Boosting},
	pages = {378--390},
	file = {Full Text PDF:files/70/Ren et al. - 2017 - A Novel Image Classification Method with CNN-XGBoo.pdf:application/pdf},
}

@article{ohi_negative_2004,
	title = {Negative {Staining} and {Image} {Classification} - {Powerful} {Tools} in {Modern} {Electron} {Microscopy}},
	volume = {6},
	issn = {1480-9222},
	doi = {10.1251/bpo70},
	abstract = {Vitrification is the state-of-the-art specimen preparation technique for molecular electron microscopy (EM) and therefore negative staining may appear to be an outdated approach. In this paper we illustrate the specific advantages of negative staining, ensuring that this technique will remain an important tool for the study of biological macromolecules. Due to the higher image contrast, much smaller molecules can be visualized by negative staining. Also, while molecules prepared by vitrification usually adopt random orientations in the amorphous ice layer, negative staining tends to induce preferred orientations of the molecules on the carbon support film. Combining negative staining with image classification techniques makes it possible to work with very heterogeneous molecule populations, which are difficult or even impossible to analyze using vitrified specimens.},
	language = {eng},
	journal = {Biol Proced Online},
	author = {Ohi, Melanie and Li, Ying and Cheng, Yifan and Walz, Thomas},
	year = {2004},
	pmid = {15103397},
	pmcid = {PMC389902},
	keywords = {General IC},
	pages = {23--34},
	file = {Full Text:files/73/Ohi et al. - 2004 - Negative Staining and Image Classification - Power.pdf:application/pdf},
}

@article{farinella_representing_2015,
	title = {Representing scenes for real-time context classification on mobile devices},
	volume = {48},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S003132031400199X},
	doi = {10.1016/j.patcog.2014.05.014},
	abstract = {In this paper we introduce the DCT-GIST image representation model which is useful to summarize the context of the scene. The proposed image descriptor addresses the problem of real-time scene context classification on devices with limited memory and low computational resources (e.g., mobile and other single sensor devices such as wearable cameras). Images are holistically represented starting from the statistics collected in the Discrete Cosine Transform (DCT) domain. Since the DCT coefficients are usually computed within the digital signal processor for the JPEG conversion/storage, the proposed solution allows to obtain an instant and “free of charge” image signature. The novel image representation exploits the DCT coefficients of natural images by modelling them as Laplacian distributions which are summarized by the scale parameter in order to capture the context of the scene. Only discriminative DCT frequencies corresponding to edges and textures are retained to build the descriptor of the image. A spatial hierarchy approach allows to collect the DCT statistics on image sub-regions to better encode the spatial envelope of the scene. The proposed image descriptor is coupled with a Support Vector Machine classifier for context recognition purpose. Experiments on the well-known 8 Scene Context Dataset as well as on the MIT-67 Indoor Scene dataset demonstrate that the proposed representation technique achieves better results with respect to the popular GIST descriptor, outperforming this last representation also in terms of computational costs. Moreover, the experiments pointed out that the proposed representation model closely matches other state-of-the-art methods based on bag of Textons collected on spatial hierarchy.},
	language = {en},
	number = {4},
	urldate = {2022-09-18},
	journal = {Pattern Recognition},
	author = {Farinella, G. M. and Ravì, D. and Tomaselli, V. and Guarnera, M. and Battiato, S.},
	month = apr,
	year = {2015},
	keywords = {DCT features, GIST, Image descriptor, JPEG, Mobile devices, Scene classification, Scene representation, Wearable cameras, On Device IC},
	pages = {1086--1100},
	file = {ScienceDirect Full Text PDF:files/76/Farinella et al. - 2015 - Representing scenes for real-time context classifi.pdf:application/pdf;ScienceDirect Snapshot:files/75/S003132031400199X.html:text/html},
}

@article{rajabizadeh_comparative_2021,
	title = {A comparative study on image-based snake identification using machine learning},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-021-96031-1},
	doi = {10.1038/s41598-021-96031-1},
	abstract = {Automated snake image identification is important from different points of view, most importantly, snake bite management. Auto-identification of snake images might help the avoidance of venomous snakes and also providing better treatment for patients. In this study, for the first time, it’s been attempted to compare the accuracy of a series of state-of-the-art machine learning methods, ranging from the holistic to neural network algorithms. The study is performed on six snake species in Lar National Park, Tehran Province, Iran. In this research, the holistic methods [k-nearest neighbors (kNN), support vector machine (SVM) and logistic regression (LR)] are used in combination with a dimension reduction approach [principle component analysis (PCA) and linear discriminant analysis (LDA)] as the feature extractor. In holistic methods (kNN, SVM, LR), the classifier in combination with PCA does not yield an accuracy of more than 50\%, But the use of LDA to extract the important features significantly improves the performance of the classifier. A combination of LDA and SVM (kernel = 'rbf') is achieved to a test accuracy of 84\%. Compared to holistic methods, convolutional neural networks show similar to better performance, and accuracy reaches 93.16\% using MobileNetV2. Visualizing intermediate activation layers in VGG model reveals that just in deep activation layers, the color pattern and the shape of the snake contribute to the discrimination of snake species. This study presents MobileNetV2 as a powerful deep convolutional neural network algorithm for snake image classification that could be used even on mobile devices. This finding pave the road for generating mobile applications for snake image identification.},
	language = {en},
	number = {1},
	urldate = {2022-09-18},
	journal = {Sci Rep},
	author = {Rajabizadeh, Mahdi and Rezghi, Mansoor},
	month = sep,
	year = {2021},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {On Device IC, Computer science, Herpetology},
	pages = {19142},
	file = {Full Text PDF:files/79/Rajabizadeh and Rezghi - 2021 - A comparative study on image-based snake identific.pdf:application/pdf;Snapshot:files/78/s41598-021-96031-1.html:text/html},
}

@article{boddapati_classifying_2017,
	series = {Knowledge-{Based} and {Intelligent} {Information} \& {Engineering} {Systems}: {Proceedings} of the 21st {International} {Conference}, {KES}-20176-8 {September} 2017, {Marseille}, {France}},
	title = {Classifying environmental sounds using image recognition networks},
	volume = {112},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050917316599},
	doi = {10.1016/j.procs.2017.08.250},
	abstract = {Automatic classification of environmental sounds, such as dog barking and glass breaking, is becoming increasingly interesting, especially for mobile devices. Most mobile devices contain both cameras and microphones, and companies that develop mobile devices would like to provide functionality for classifying both videos/images and sounds. In order to reduce the development costs one would like to use the same technology for both of these classification tasks. One way of achieving this is to represent environmental sounds as images, and use an image classification neural network when classifying images as well as sounds. In this paper we consider the classification accuracy for different image representations (Spectrogram, MFCC, and CRP) of environmental sounds. We evaluate the accuracy for environmental sounds in three publicly available datasets, using two well-known convolutional deep neural networks for image recognition (AlexNet and GoogLeNet). Our experiments show that we obtain good classification accuracy for the three datasets.},
	language = {en},
	urldate = {2022-09-18},
	journal = {Procedia Computer Science},
	author = {Boddapati, Venkatesh and Petef, Andrej and Rasmusson, Jim and Lundberg, Lars},
	month = jan,
	year = {2017},
	keywords = {Image Classification, Deep Learning, On Device IC, Convolutional Neural Networks, Environmental Sound Classification, GPU Processing},
	pages = {2048--2056},
	file = {ScienceDirect Full Text PDF:files/81/Boddapati et al. - 2017 - Classifying environmental sounds using image recog.pdf:application/pdf;ScienceDirect Snapshot:files/82/S1877050917316599.html:text/html},
}

@inproceedings{rosales_corripio_unsupervised_2015,
	title = {Unsupervised {Classification} of {Mobile} {Device} {Images}},
	isbn = {978-9957-8583-3-9},
	url = {http://icit.zuj.edu.jo/icit15/DOI/Artificial_Intelligence/0014.pdf},
	doi = {10.15849/icit.2015.0014},
	abstract = {As mobile devices are seeing widespread usage in the everyday life, the images from mobile devices can be used as evidence in legal purposes. Accordingly, the identification of mobile devices images are of significant interest in digital forensics. In this paper, we propose a method to determine the mobile devices camera source based on the grouping or clustering of images according to their source acquisition. Our clustering technique does not involve a priori knowledge of the number of images or devices to be identified or training data for a future classification stage. The proposal combines of hierarchical and flat clustering and the use of sensor pattern noise. Experimental results show that our approach is very promising for identifying mobile devices source.},
	language = {en},
	urldate = {2022-09-18},
	booktitle = {The 7th {International} {Conference} on {Information} {Technology}},
	publisher = {Al-Zaytoonah University of Jordan},
	author = {Rosales Corripio, Jocelin and Sandoval Orozco, Ana Lucila and García Villalba, Luis Javier},
	month = may,
	year = {2015},
	pages = {96--101},
	file = {Rosales Corripio et al. - 2015 - Unsupervised Classification of Mobile Device Image.pdf:files/83/Rosales Corripio et al. - 2015 - Unsupervised Classification of Mobile Device Image.pdf:application/pdf},
}

@inproceedings{singla_foodnon-food_2016,
	address = {New York, NY, USA},
	series = {{MADiMa} '16},
	title = {Food/{Non}-food {Image} {Classification} and {Food} {Categorization} using {Pre}-{Trained} {GoogLeNet} {Model}},
	isbn = {978-1-4503-4520-0},
	url = {https://doi.org/10.1145/2986035.2986039},
	doi = {10.1145/2986035.2986039},
	abstract = {Recent past has seen a lot of developments in the field of image-based dietary assessment. Food image classification and recognition are crucial steps for dietary assessment. In the last couple of years, advancements in the deep learning and convolutional neural networks proved to be a boon for the image classification and recognition tasks, specifically for food recognition because of the wide variety of food items. In this paper, we report experiments on food/non-food classification and food recognition using a GoogLeNet model based on deep convolutional neural network. The experiments were conducted on two image datasets created by our own, where the images were collected from existing image datasets, social media, and imaging devices such as smart phone and wearable cameras. Experimental results show a high accuracy of 99.2\% on the food/non-food classification and 83.6\% on the food category recognition.},
	urldate = {2022-09-18},
	booktitle = {Proceedings of the 2nd {International} {Workshop} on {Multimedia} {Assisted} {Dietary} {Management}},
	publisher = {Association for Computing Machinery},
	author = {Singla, Ashutosh and Yuan, Lin and Ebrahimi, Touradj},
	month = oct,
	year = {2016},
	keywords = {deep learning, On Device IC, caffe, convolutional neural network (CNN), food recognition, food/non-food classification, googlenet},
	pages = {3--11},
	file = {Full Text PDF:files/86/Singla et al. - 2016 - FoodNon-food Image Classification and Food Catego.pdf:application/pdf},
}

@article{bagul_interactive_nodate,
	title = {Interactive {Multitudinous} {Visual} {Search} on {Mobile} {Device}},
	abstract = {This paper describes the robust interactive multitudinous visual search system on Android mobile device. This system is able to take any type of input e.g. text, image, voice and perform visual search on mobile device. It gives the relevant results what an user want by taking the full benefit of interactive nature of mobile device. The main focus is on color features, the searching method searches similar images from the image database by matching the feature vector to locate object that matches query object perfectly. Proposed system makes the use of Naïve-Bayes algorithm which classifies the object according to their features and predicts the new label to an image. Main contribution of this paper is Multimedia search, in which image is given as input and user get similar youtube links, web links. This is designed for the users who have number of pictures in their mind but unable to describe and address them, and cannot achieve expected results for given query. The proposed system will differ fro m existing system by having following features- 1) Provide better features to user 2) Performs automated image prediction 3) Increased search performance.},
	language = {en},
	author = {Bagul, Roshani E and Rokade, P P},
	keywords = {On Device IC},
	pages = {6},
	file = {Bagul and Rokade - Interactive Multitudinous Visual Search on Mobile .pdf:files/87/Bagul and Rokade - Interactive Multitudinous Visual Search on Mobile .pdf:application/pdf},
}

@article{kang_neurosurgeon_2017,
	title = {Neurosurgeon: {Collaborative} {Intelligence} {Between} the {Cloud} and {Mobile} {Edge}},
	volume = {45},
	issn = {0163-5964},
	shorttitle = {Neurosurgeon},
	url = {https://doi.org/10.1145/3093337.3037698},
	doi = {10.1145/3093337.3037698},
	abstract = {The computation for today's intelligent personal assistants such as Apple Siri, Google Now, and Microsoft Cortana, is performed in the cloud. This cloud-only approach requires significant amounts of data to be sent to the cloud over the wireless network and puts significant computational pressure on the datacenter. However, as the computational resources in mobile devices become more powerful and energy efficient, questions arise as to whether this cloud-only processing is desirable moving forward, and what are the implications of pushing some or all of this compute to the mobile devices on the edge. In this paper, we examine the status quo approach of cloud-only processing and investigate computation partitioning strategies that effectively leverage both the cycles in the cloud and on the mobile device to achieve low latency, low energy consumption, and high datacenter throughput for this class of intelligent applications. Our study uses 8 intelligent applications spanning computer vision, speech, and natural language domains, all employing state-of-the-art Deep Neural Networks (DNNs) as the core machine learning technique. We find that given the characteristics of DNN algorithms, a fine-grained, layer-level computation partitioning strategy based on the data and computation variations of each layer within a DNN has significant latency and energy advantages over the status quo approach. Using this insight, we design Neurosurgeon, a lightweight scheduler to automatically partition DNN computation between mobile devices and datacenters at the granularity of neural network layers. Neurosurgeon does not require per-application profiling. It adapts to various DNN architectures, hardware platforms, wireless networks, and server load levels, intelligently partitioning computation for best latency or best mobile energy. We evaluate Neurosurgeon on a state-of-the-art mobile development platform and show that it improves end-to-end latency by 3.1X on average and up to 40.7X, reduces mobile energy consumption by 59.5\% on average and up to 94.7\%, and improves datacenter throughput by 1.5X on average and up to 6.7X.},
	number = {1},
	urldate = {2022-09-18},
	journal = {SIGARCH Comput. Archit. News},
	author = {Kang, Yiping and Hauswald, Johann and Gao, Cao and Rovinski, Austin and Mudge, Trevor and Mars, Jason and Tang, Lingjia},
	month = apr,
	year = {2017},
	keywords = {On Device IC, cloud computing, deep neural networks, intelligent applications, mobile computing},
	pages = {615--629},
	file = {Full Text PDF:files/90/Kang et al. - 2017 - Neurosurgeon Collaborative Intelligence Between t.pdf:application/pdf},
}

@inproceedings{chen_tree_2009,
	title = {Tree {Histogram} {Coding} for {Mobile} {Image} {Matching}},
	doi = {10.1109/DCC.2009.33},
	abstract = {For mobile image matching applications, a mobile device captures a query image, extracts descriptive features, and transmits these features wirelessly to a server. The server recognizes the query image by comparing the extracted features to its database and returns information associated with the recognition result. For slow links, query feature compression is crucial for low-latency retrieval. Previous image retrieval systems transmit compressed feature descriptors, which is well suited for pairwise image matching. For fast retrieval from large databases, however, scalable vocabulary trees are commonly employed. In this paper, we propose a rate-efficient codec designed for tree-based retrieval. By encoding a tree histogram, our codec can achieve a more than 5times rate reduction compared to sending compressed feature descriptors. By discarding the order amongst a list of features, histogram coding requires 1.5times lower rate than sending a tree node index for every feature. A statistical analysis is performed to study how the entropy of encoded symbols varies with tree depth and the number of features.},
	booktitle = {2009 {Data} {Compression} {Conference}},
	author = {Chen, David M. and Tsai, Sam S. and Chandrasekhar, Vijay and Takacs, Gabriel and Singh, Jatinder and Girod, Bernd},
	month = mar,
	year = {2009},
	note = {ISSN: 2375-0359},
	keywords = {Feature extraction, Image recognition, Histograms, Image coding, On Device IC, Codecs, Data mining, histogram coding, Image databases, Image matching, image retrieval, Image retrieval, robust features, scalable vocabulary tree, Spatial databases, vector quantization},
	pages = {143--152},
	file = {IEEE Xplore Abstract Record:files/93/4976458.html:text/html;Submitted Version:files/92/Chen et al. - 2009 - Tree Histogram Coding for Mobile Image Matching.pdf:application/pdf},
}

@article{pouladzadeh_mobile_2017,
	title = {Mobile {Multi}-{Food} {Recognition} {Using} {Deep} {Learning}},
	volume = {13},
	issn = {1551-6857},
	url = {https://doi.org/10.1145/3063592},
	doi = {10.1145/3063592},
	abstract = {In this article, we propose a mobile food recognition system that uses the picture of the food, taken by the user’s mobile device, to recognize multiple food items in the same meal, such as steak and potatoes on the same plate, to estimate the calorie and nutrition of the meal. To speed up and make the process more accurate, the user is asked to quickly identify the general area of the food by drawing a bounding circle on the food picture by touching the screen. The system then uses image processing and computational intelligence for food item recognition. The advantage of recognizing items, instead of the whole meal, is that the system can be trained with only single item food images. At the training stage, we first use region proposal algorithms to generate candidate regions and extract the convolutional neural network (CNN) features of all regions. Second, we perform region mining to select positive regions for each food category using maximum cover by our proposed submodular optimization method. At the testing stage, we first generate a set of candidate regions. For each region, a classification score is computed based on its extracted CNN features and predicted food names of the selected regions. Since fast response is one of the important parameters for the user who wants to eat the meal, certain heavy computational parts of the application are offloaded to the cloud. Hence, the processes of food recognition and calorie estimation are performed in cloud server. Our experiments, conducted with the FooDD dataset, show an average recall rate of 90.98\%, precision rate of 93.05\%, and accuracy of 94.11\% compared to 50.8\% to 88\% accuracy of other existing food recognition systems.},
	number = {3s},
	urldate = {2022-09-18},
	journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin},
	month = aug,
	year = {2017},
	keywords = {deep learning, On Device IC, cloud computing, Mobile food recognition},
	pages = {36:1--36:21},
	file = {Full Text PDF:files/95/Pouladzadeh and Shirmohammadi - 2017 - Mobile Multi-Food Recognition Using Deep Learning.pdf:application/pdf},
}

@misc{dhar_-device_2020,
	title = {On-{Device} {Machine} {Learning}: {An} {Algorithms} and {Learning} {Theory} {Perspective}},
	shorttitle = {On-{Device} {Machine} {Learning}},
	url = {http://arxiv.org/abs/1911.00623},
	abstract = {The predominant paradigm for using machine learning models on a device is to train a model in the cloud and perform inference using the trained model on the device. However, with increasing number of smart devices and improved hardware, there is interest in performing model training on the device. Given this surge in interest, a comprehensive survey of the field from a device-agnostic perspective sets the stage for both understanding the state-of-the-art and for identifying open challenges and future avenues of research. However, on-device learning is an expansive field with connections to a large number of related topics in AI and machine learning (including online learning, model adaptation, one/few-shot learning, etc.). Hence, covering such a large number of topics in a single survey is impractical. This survey finds a middle ground by reformulating the problem of on-device learning as resource constrained learning where the resources are compute and memory. This reformulation allows tools, techniques, and algorithms from a wide variety of research areas to be compared equitably. In addition to summarizing the state-of-the-art, the survey also identifies a number of challenges and next steps for both the algorithmic and theoretical aspects of on-device learning.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
	month = jul,
	year = {2020},
	note = {arXiv:1911.00623 [cs, stat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:files/98/Dhar et al. - 2020 - On-Device Machine Learning An Algorithms and Lear.pdf:application/pdf;arXiv.org Snapshot:files/99/1911.html:text/html},
}

@misc{noauthor_researchgate_nodate,
	title = {{ResearchGate}},
	url = {https://www.researchgate.net/publication/358908273_Machine_Learning_in_Mobile_Applications/link/621d87586051a1658201675c/download},
	urldate = {2022-09-20},
	file = {ResearchGate:files/101/download.html:text/html},
}

@article{ganesan_machine_2022,
	title = {Machine {Learning} in {Mobile} {Applications}},
	volume = {11},
	doi = {10.47760/ijcsmc.2022.v11i02.013},
	abstract = {Machine learning is a branch of computer science that enables computers to learn without being explicitly programmed. One of the most exciting technologies that one has ever encountered is machine learning. As the name implies, it provides the computer with the ability to learn, which makes it more human-like. Machine learning is now employed in a variety of applications, including self-driving cars, personal assistants such as Cortana, Alexa, and Siri, and security technologies such as face recognition. Developers of mobile applications are increasingly being pushed to include machine learning technology in their apps. This is unfamiliar territory for many of them. In this research paper, we will look at how machine learning relates to AI in this context, with an emphasis on application developers. This paper demonstrates different machine learning types, frameworks, and tools available on the market and how to use them to create the statistical models needed to use them in mobile applications without having to learn more about the complexity of algorithms and how they train and learn models. Also, this article covers the basic architectures that mobile applications can leverage to work with machine learning.},
	journal = {International Journal of Computer Science and Mobile Computing},
	author = {Ganesan, Veeramani},
	month = feb,
	year = {2022},
	pages = {110--118},
	file = {Full Text PDF:files/103/Ganesan - 2022 - Machine Learning in Mobile Applications.pdf:application/pdf},
}

@book{chollet_deep_2018,
	address = {Shelter Island, New York},
	title = {Deep learning with {Python}},
	isbn = {978-1-61729-443-3},
	language = {en},
	publisher = {Manning Publications Co},
	author = {Chollet, François},
	year = {2018},
	note = {OCLC: ocn982650571},
	keywords = {Machine learning, Neural networks (Computer science), Python (Computer program language)},
	file = {Chollet - 2018 - Deep learning with Python.pdf:files/106/Chollet - 2018 - Deep learning with Python.pdf:application/pdf},
}

@inproceedings{hauswald_hybrid_2014,
	title = {A hybrid approach to offloading mobile image classification},
	doi = {10.1109/ICASSP.2014.6855235},
	abstract = {Current mobile devices are unable to execute complex vision applications in a timely and power efficient manner without offloading some of the computation. This paper examines the tradeoffs that arise from executing some of the workload onboard and some remotely. Feature extraction and matching play an essential role in image classification and have the potential to be executed locally. Along with advances in mobile hardware, understanding the computation requirements of these applications is essential to realize their full potential in mobile environments. We analyze the ability of a mobile platform to execute feature extraction and matching, and prediction workloads under various scenarios. The best configuration for optimal runtime (11\% faster) executes feature extraction with a GPU onboard and offloads the rest of the pipeline. Alternatively, compressing and sending the image over the network achieves lowest data transferred (2.5× better) and lowest energy usage (3.7× better) than the next best option.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hauswald, J. and Manville, T. and Zheng, Q. and Dreslinski, R. and Chakrabarti, C. and Mudge, T.},
	month = may,
	year = {2014},
	note = {ISSN: 2379-190X},
	keywords = {Accuracy, Feature extraction, Image coding, mobile computing, energy management, image classification, Mobile communication, offloading, Pipelines, Predictive models, Runtime},
	pages = {8375--8379},
	file = {IEEE Xplore Abstract Record:files/109/6855235.html:text/html},
}

@article{konecny_federated_2016,
	title = {Federated {Optimization}: {Distributed} {Machine} {Learning} for {On}-{Device} {Intelligence}},
	shorttitle = {Federated {Optimization}},
	url = {https://arxiv.org/abs/1610.02527v1},
	doi = {10.48550/arXiv.1610.02527},
	abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of {\textbackslash}federated optimization.},
	language = {en},
	urldate = {2022-09-22},
	author = {Konečný, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richtárik, Peter},
	month = oct,
	year = {2016},
	file = {Full Text PDF:files/112/Konečný et al. - 2016 - Federated Optimization Distributed Machine Learni.pdf:application/pdf;Snapshot:files/111/1610.html:text/html},
}

@article{ravi_projectionnet_2017,
	title = {{ProjectionNet}: {Learning} {Efficient} {On}-{Device} {Deep} {Networks} {Using} {Neural} {Projections}},
	shorttitle = {{ProjectionNet}},
	url = {https://arxiv.org/abs/1708.00630v2},
	doi = {10.48550/arXiv.1708.00630},
	abstract = {Deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. However, it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. While these devices could make use of machine learning models running on high-performance data centers with CPUs or GPUs, this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly "on" device. We introduce a new architecture for training compact neural networks using a joint optimization framework. At its core lies a novel objective that jointly trains using two different types of networks--a full trainer neural network (using existing architectures like Feed-forward NNs or LSTM RNNs) combined with a simpler "projection" network that leverages random projections to transform inputs or intermediate representations into bits. The simpler network encodes lightweight and efficient-to-compute operations in bit space with a low memory footprint. The two networks are trained jointly using backpropagation, where the projection network learns from the full network similar to apprenticeship learning. Once trained, the smaller network can be used directly for inference at low memory and computation cost. We demonstrate the effectiveness of the new approach at significantly shrinking the memory requirements of different types of neural networks while preserving good accuracy on visual recognition and text classification tasks. We also study the question "how many neural bits are required to solve a given task?" using the new framework and show empirical results contrasting model predictive capacity (in bits) versus accuracy on several datasets.},
	language = {en},
	urldate = {2022-09-22},
	author = {Ravi, Sujith},
	month = aug,
	year = {2017},
	file = {Full Text PDF:files/115/Ravi - 2017 - ProjectionNet Learning Efficient On-Device Deep N.pdf:application/pdf;Snapshot:files/114/1708.html:text/html},
}

@inproceedings{zhengj_mobile_2016,
	title = {Mobile {Device} {Based} {Outdoor} {Navigation} {With} {On}-{Line} {Learning} {Neural} {Network}: {A} {Comparison} {With} {Convolutional} {Neural} {Network}},
	shorttitle = {Mobile {Device} {Based} {Outdoor} {Navigation} {With} {On}-{Line} {Learning} {Neural} {Network}},
	url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w3/html/Zhengj_Mobile_Device_Based_CVPR_2016_paper.html},
	urldate = {2022-09-22},
	author = {Zhengj, Zejia and Weng, Juyang},
	year = {2016},
	pages = {11--18},
	file = {Full Text PDF:files/118/Zhengj and Weng - 2016 - Mobile Device Based Outdoor Navigation With On-Lin.pdf:application/pdf;Snapshot:files/117/Zhengj_Mobile_Device_Based_CVPR_2016_paper.html:text/html},
}

@inproceedings{wang_accelerating_2016,
	address = {New York, NY, USA},
	series = {{MM} '16},
	title = {Accelerating {Convolutional} {Neural} {Networks} for {Mobile} {Applications}},
	isbn = {978-1-4503-3603-1},
	url = {https://doi.org/10.1145/2964284.2967280},
	doi = {10.1145/2964284.2967280},
	abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in a wide range of computer vision tasks, typically at the cost of massive computational complexity. The low speed of these networks may hinder real-time applications especially when computational resources are limited. In this paper, an efficient and effective approach is proposed to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. Specifically, for each convolutional layer, the kernel tensor is decomposed into the sum of a small number of low multilinear rank tensors. Then we replace the original kernel tensors in all layers with the approximate tensors and fine-tune the whole net with respect to the final classification task using standard backpropagation. {\textbackslash}{\textbackslash} Comprehensive experiments on ILSVRC-12 demonstrate significant reduction in computational complexity, at the cost of negligible loss in accuracy. For the widely used VGG-16 model, our approach obtains a 6.6\${\textbackslash}times\$ speed-up on PC and 5.91\${\textbackslash}times\$ speed-up on mobile device of the whole network with less than 1{\textbackslash}\% increase on top-5 error.},
	urldate = {2022-09-21},
	booktitle = {Proceedings of the 24th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Wang, Peisong and Cheng, Jian},
	month = oct,
	year = {2016},
	keywords = {image classification, acceleration, convolutional neural networks, tensor decomposition},
	pages = {541--545},
}

@inproceedings{prasad_mobile_2013,
	title = {Mobile plant species classification: {A} low computational aproach},
	shorttitle = {Mobile plant species classification},
	doi = {10.1109/ICIIP.2013.6707624},
	abstract = {In this paper a reduced shape and color feature extraction method is proposed for a mobile device based plant classification system. For scientists, botanists, farmers, and others plant identification is a useful and important task. The original image captured is reduced to similar aspect ratio which does not affect the shape information but reduces the computation cost nearly up to half of the total cost. The algorithm first calculates the geometric feature and then polar Fourier transform and trained using k-NN classifier. Then two nearest classes were selected on the basics of smallest distance which is further rectified by the color features using a decision tree. The algorithm proves to be better in performance compared to other already existing algorithms.},
	booktitle = {2013 {IEEE} {Second} {International} {Conference} on {Image} {Information} {Processing} ({ICIIP}-2013)},
	author = {Prasad, Shitala and Peddoju, Sateesh K. and Ghosh, Debashis},
	month = dec,
	year = {2013},
	keywords = {Feature extraction, Mobile communication, Color Features, Conferences, Fourier transforms, Geometric Shape Features, Image color analysis, Image segmentation, k-NN, Leaf Recognition, Low Computation, Plant Identification, Polar Fourier Transform, Shape},
	pages = {405--409},
	file = {IEEE Xplore Abstract Record:files/121/6707624.html:text/html},
}

@inproceedings{hays_mobile_2013,
	title = {Mobile device to cloud co-processing of {ASL} finger spelling to text conversion},
	doi = {10.1109/WNYIPW.2013.6890987},
	abstract = {Computer recognition of American Sign Language (ASL) is a computationally intensive task. This research investigates transcription of static ASL signs on a consumer-level mobile device. The application provides real-time sign to text translation by processing a live video stream to detect the ASL alphabet as well as custom signs to perform tasks on the device. The chosen classification algorithm uses Locality Preserving Projections (LPP) as manifold learning along with Support Vector Machine (SVM) multi-class classification. The algorithm is contrasted with and without cloud assistance. In comparison to the local mobile application, the cloud-assisted application increased classification speed, reduced memory us-age, and kept the network usage low while barely increasing the power required.},
	booktitle = {2013 {IEEE} {Western} {New} {York} {Image} {Processing} {Workshop} ({WNYIPW})},
	author = {Hays, Philip and Ptucha, Raymond and Melton, Roy},
	month = nov,
	year = {2013},
	keywords = {Accuracy, American Sign Language, Assistive technology, Cloud Co-Processing, Gesture recognition, Image edge detection, Locality Preserving Projections, Mobile handsets, Servers, Skin, Smart Phones, Support Vector Machines},
	pages = {39--43},
	file = {IEEE Xplore Abstract Record:files/123/6890987.html:text/html},
}

@inproceedings{monsur_mobile-based_2017,
	title = {A mobile-based image analysis system for cervical cancer detection},
	doi = {10.1109/ICECCO.2017.8333312},
	abstract = {Cervical cancer is the third major killer disease in developed and developing countries. Whereas screening and other preventive measures reduce the mortality in developed countries, mortality rate still remains very high in developing countries. This project focuses on the analysis of digital image of the cervix, captured with a low-level camera and under a contrast agent (the visual inspection with acetic acid (VIA)). Gaussian and mean filter techniques were used to remove the speckles. A segmentation algorithm was used to isolate the Region of Interest (ROI) from the image. Additionally a Canny edge detection algorithm was used to find edges. Furthermore, quantification and classification of the images were done. An android application was used to integrated all the above. This allows usage in rural settings. The results obtained were quite satisfactory (Specificity 79\% and Sensitivity of 83\%).},
	booktitle = {2017 13th {International} {Conference} on {Electronics}, {Computer} and {Computation} ({ICECCO})},
	author = {Monsur, Saka Abiola and Adeshina, Steve A. and Sud, Shivani and Soboyejo, Winston O.},
	month = nov,
	year = {2017},
	keywords = {Image segmentation, Image edge detection, Acetowhite region, Cervical cancer, Clustering algorithms, Gray-scale, Human Papilomavirus, Mathematical model, Region of Interest, Visual Inspection},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:files/125/8333312.html:text/html},
}

@inproceedings{huang_speedaccuracy_2017,
	title = {Speed/{Accuracy} {Trade}-{Offs} for {Modern} {Convolutional} {Object} {Detectors}},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html},
	urldate = {2022-09-22},
	author = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin},
	year = {2017},
	pages = {7310--7311},
	file = {Full Text PDF:files/129/Huang et al. - 2017 - SpeedAccuracy Trade-Offs for Modern Convolutional.pdf:application/pdf;Snapshot:files/128/Huang_SpeedAccuracy_Trade-Offs_for_CVPR_2017_paper.html:text/html},
}

@inproceedings{li_fitcnn_2017,
	title = {{FitCNN}: {A} cloud-assisted lightweight convolutional neural network framework for mobile devices},
	shorttitle = {{FitCNN}},
	doi = {10.1109/RTCSA.2017.8046337},
	abstract = {Recently convolutional neural networks (CNNs) have essentially reached the state-of-the-art accuracies in image classification and recognition. CNNs are usually deployed in server side or cloud to handle tasks collected from mobile devices, such as smartphones, wearable devices, unmanned systems and so on. However, significant data transmission overhead and privacy issues have made it necessary to use CNNs directly in device side. Nevertheless, the statically trained model deployed on mobile devices cannot effectively handle the unknown data and objects in new environments, which could lead to low accuracy and unsatisfied user experience. Hence, it would be crucial to retrain a better model via future unknown data. However, with tremendous computing cost and memory usage, training a CNN on mobile devices with limited hardware resources is intolerable in practical. To solve this issue, by using the power of cloud is a promising solution to assist mobile devices to train a deep neural network. Therefore, this paper proposes a cloud-assisted lightweight CNN framework, named FitCNN, with incremental learning and low data transmission, to deploy CNNs on mobile devices and make them smarter. To reduce the data transmission during incremental learning, we propose a strategy to selectively upload the data with high learning value, and develop an extracting strategy to choose light weights of the new CNN model trained on the cloud to update the old one on devices. Experimental results show that the selectively uploading strategy can reduce 39.4\% uploading transmission based on a certain dataset, and the extracting weights strategy reduces by more than 60\% updating transmission with multiple CNNs and datasets.},
	booktitle = {2017 {IEEE} 23rd {International} {Conference} on {Embedded} and {Real}-{Time} {Computing} {Systems} and {Applications} ({RTCSA})},
	author = {Li, Shiming and Liu, Duo and Xiang, Chaoneng and Liu, Jianfeng and Ling, Yingjian and Liao, Tianjun and Liang, Liang},
	month = aug,
	year = {2017},
	note = {ISSN: 2325-1301},
	keywords = {Training, Computational modeling, Data models, Neural networks, Data mining, Mobile handsets, Data communication},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:files/131/8046337.html:text/html},
}

@inproceedings{groeneweg_fast_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Fast} {Offline} {Building} {Recognition} {Application} on a {Mobile} {Telephone}},
	isbn = {978-3-540-44632-3},
	doi = {10.1007/11864349_102},
	abstract = {Today most mobile telephones come equipped with a camera. This gives rise to interesting new possibilities for applications of computer vision, such as building recognition software running locally on the mobile phone. Algorithms for building recognition need to be robust under noise, occlusion, varying lighting conditions and different points of view. We present such an algorithm using local invariant regions which allows for mobile building recognition despite the limited processing power and storage capacity of mobile phones. This algorithm was shown to obtain state of the art performance on the Zürich Building Database (91\% accuracy). An implementation on a mobile phone (Sony Ericsson K700i) is presented that obtains good performance (80\% accuracy) on a dataset using real-world query images taken under varying, suboptimal conditions. Our algorithm runs in the order of several seconds while requiring only around 10 KB of memory to represent a single building within the local database.},
	language = {en},
	booktitle = {Advanced {Concepts} for {Intelligent} {Vision} {Systems}},
	publisher = {Springer},
	author = {Groeneweg, N. J. C. and de Groot, B. and Halma, A. H. R. and Quiroga, B. R. and Tromp, M. and Groen, F. C. A.},
	editor = {Blanc-Talon, Jacques and Philips, Wilfried and Popescu, Dan and Scheunders, Paul},
	year = {2006},
	keywords = {Interest Point, Mobile Phone, Mobile Telephone, Query Image, Training Image},
	pages = {1122--1132},
	file = {Full Text PDF:files/133/Groeneweg et al. - 2006 - A Fast Offline Building Recognition Application on.pdf:application/pdf},
}

@article{cheung_mobile_2015,
	title = {Mobile image analysis for medical applications},
	doi = {10.1117/2.1201506.005997},
	journal = {SPIE Newsroom},
	author = {Cheung, Ngai-Man and Pomponiu, Victor and Do, Thanh-Toan and Nejati, Hossein},
	month = jul,
	year = {2015},
}

@inproceedings{mcmahan_communication-efficient_2017,
	title = {Communication-{Efficient} {Learning} of {Deep} {Networks} from {Decentralized} {Data}},
	url = {https://proceedings.mlr.press/v54/mcmahan17a.html},
	abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
	month = apr,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {1273--1282},
	file = {Full Text PDF:files/137/McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf:application/pdf;Supplementary PDF:files/138/McMahan et al. - 2017 - Communication-Efficient Learning of Deep Networks .pdf:application/pdf},
}

@inproceedings{doukas_automated_2012,
	title = {Automated skin lesion assessment using mobile technologies and cloud platforms},
	doi = {10.1109/EMBC.2012.6346458},
	abstract = {This paper presents a smart phone based system for storing digital images of skin areas depicting regions of interest (lesions) and performing self-assessment of these skin lesions within these areas. The system consists of a mobile application that can acquire and identify moles in skin images and classify them according their severity into melanoma, nevus and benign lesions. The proposed system includes also a cloud infrastructure exploiting computational and storage resources. This cloud-based architecture provides interoperability and support of various mobile environments as well as flexibility in enhancing the classification model. Initial evaluation results are quite promising and indicate that the application can be used for the task of skin lesions initial assessment.},
	booktitle = {2012 {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	author = {Doukas, Charalampos and Stagkopoulos, Paris and Kiranoudis, Chris T. and Maglogiannis, Ilias},
	month = aug,
	year = {2012},
	note = {ISSN: 1558-4615},
	keywords = {Support vector machines, Mobile communication, Image color analysis, Skin, Cancer, Lesions, Web services},
	pages = {2444--2447},
	file = {IEEE Xplore Abstract Record:files/140/6346458.html:text/html},
}

@inproceedings{hauswald_hybrid_2014-1,
	title = {A hybrid approach to offloading mobile image classification},
	doi = {10.1109/ICASSP.2014.6855235},
	abstract = {Current mobile devices are unable to execute complex vision applications in a timely and power efficient manner without offloading some of the computation. This paper examines the tradeoffs that arise from executing some of the workload onboard and some remotely. Feature extraction and matching play an essential role in image classification and have the potential to be executed locally. Along with advances in mobile hardware, understanding the computation requirements of these applications is essential to realize their full potential in mobile environments. We analyze the ability of a mobile platform to execute feature extraction and matching, and prediction workloads under various scenarios. The best configuration for optimal runtime (11\% faster) executes feature extraction with a GPU onboard and offloads the rest of the pipeline. Alternatively, compressing and sending the image over the network achieves lowest data transferred (2.5× better) and lowest energy usage (3.7× better) than the next best option.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Hauswald, J. and Manville, T. and Zheng, Q. and Dreslinski, R. and Chakrabarti, C. and Mudge, T.},
	month = may,
	year = {2014},
	note = {ISSN: 2379-190X},
	keywords = {Accuracy, Feature extraction, Image coding, mobile computing, energy management, image classification, Mobile communication, offloading, Pipelines, Predictive models, Runtime},
	pages = {8375--8379},
	file = {IEEE Xplore Abstract Record:files/142/6855235.html:text/html},
}

@incollection{zheng_compact_2015,
	address = {Cham},
	title = {Compact {Deep} {Neural} {Networks} for {Device}-{Based} {Image} {Classification}},
	isbn = {978-3-319-24702-1},
	url = {https://doi.org/10.1007/978-3-319-24702-1_8},
	abstract = {Convolutional Neural Network (CNN) is efficient in learning hierarchical features from large image datasets, but its model complexity and large memory footprints prevent it from being deployed to devices without a server back-end support. Modern CNNs are always trained on GPUs or even GPU clusters with high-speed computation capability due to the immense size of the network. A device-based deep learning CNN engine for image classification can be very useful for situations where server back end is either not available, or its communication link is weak and unreliable. Methods on regulating the size of the network, on the other hand, are rarely studied. In this chapter we present a novel compact architecture that minimizes the number and complexity of lower level filters in a CNN by separating the color information from the original image. A 9-patch histogram extractor is built to exploit the unused color information. A high-level classifier is then used to learn the features obtained from the compact CNN that was trained only on grayscale image with limited number of filters and the 9-patch histogram extracted from the color information in the image. We apply our compact architecture to Samsung Mobile Image Dataset for image classification. The proposed solution has a recognition accuracy on par with the state-of-the-art CNNs, while achieving significant reduction in model memory footprint. With these advantages, our system is being deployed to the mobile devices.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {Mobile {Cloud} {Visual} {Media} {Computing}: {From} {Interaction} to {Service}},
	publisher = {Springer International Publishing},
	author = {Zheng, Zejia and Li, Zhu and Nagar, Abhishek},
	editor = {Hua, Gang and Hua, Xian-Sheng},
	year = {2015},
	doi = {10.1007/978-3-319-24702-1_8},
	keywords = {Convolutional Neural Network, Convolutional Layer, Filter Suppression, Global Histogram, Original Architecture},
	pages = {201--217},
	file = {Full Text PDF:files/144/Zheng et al. - 2015 - Compact Deep Neural Networks for Device-Based Imag.pdf:application/pdf},
}

@article{an_efficient_2015,
	title = {An efficient block classification for media healthcare service in mobile cloud computing},
	volume = {74},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-014-2039-6},
	doi = {10.1007/s11042-014-2039-6},
	abstract = {Recently, the strict investment in healthcare domain achieves the professional quality and convenient demands of customers. Especially, with the rapid growth of modern high technology, healthcare is providing many kinds of services for patients. One of those is mobile healthcare that is the integration of mobile computing and health monitoring. By using remote protocol, the service via mobile can directly send the patient heart failure sign to the doctor. Among the existing client-server protocols, Remote Desktop Protocol (RDP) is the typical method but it needs enhancement to adapt for more rigorous requirements: real time, quality of service (QoS), etc. In this paper, we present the architecture with flexibly coding screen image to improve quality of experience (QoE) of users and ensure low bandwidth services. Besides, in order to guarantee the best image quality and reduce redundant communication by using different compression encoding techniques for movies, images and other formats. Based on the edge detection Sobel operator that strongly focuses through horizontal, vertical direction and low complexity computation, our proposed method introduces the efficient block classification for the media healthcare services.},
	language = {en},
	number = {14},
	urldate = {2022-09-22},
	journal = {Multimed Tools Appl},
	author = {An, Nguyen Thuy and Huynh, Cong-Thinh and Lee, ByungKwan and Hong, Choong Seon and Huh, Eui-Nam},
	month = jul,
	year = {2015},
	keywords = {Visualization, Block classification, Healthcare, Image processing, Mobile thin client, Remote protocol},
	pages = {5209--5223},
	file = {Full Text PDF:files/146/An et al. - 2015 - An efficient block classification for media health.pdf:application/pdf},
}

@article{pouladzadeh_cloud-based_2015,
	title = {Cloud-based {SVM} for food categorization},
	volume = {74},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-014-2116-x},
	doi = {10.1007/s11042-014-2116-x},
	abstract = {As people across the globe are becoming more interested in watching their weight, eating more healthily, and avoiding obesity, a system that can measure calories and nutrition in everyday meals can be very useful. Recently, due to ubiquity of mobile devices such as smart phones, the health monitoring applications are accessible by the patients practically all the time. We have created a semi-automatic food calorie and nutrition measurement system via mobile that can help patients and dietitians to measure and manage daily food intake. While segmentation and recognition are the two main steps of a food calorie measurement system, in this paper we have focused on the recognition part and mainly the training phase of the classification algorithm. This paper presents a cloud-based Support Vector Machine (SVM) method for classifying objects in cluster. We propose a method for food recognition application that is referred to as the Cloud SVM training mechanism in a cloud computing environment with Map Reduce technique for distributed machine learning. The results show that by using cloud computing system in classification phase and updating the database periodically, the accuracy of the recognition step has increased in single food portion, non-mixed and mixed plate of food compared to LIBSVM.},
	language = {en},
	number = {14},
	urldate = {2022-09-22},
	journal = {Multimed Tools Appl},
	author = {Pouladzadeh, Parisa and Shirmohammadi, Shervin and Bakirov, Aslan and Bulut, Ahmet and Yassine, Abdulsalam},
	month = jul,
	year = {2015},
	keywords = {Calorie measurement, Cloud computing, Food image processing},
	pages = {5243--5260},
	file = {Full Text PDF:files/148/Pouladzadeh et al. - 2015 - Cloud-based SVM for food categorization.pdf:application/pdf},
}

@inproceedings{pouladzadeh_mobile_2014,
	title = {Mobile cloud based food calorie measurement},
	doi = {10.1109/ICMEW.2014.6890665},
	abstract = {Mobile-based applications have become ubiquitous in many aspects of people's lives over the past few years. Harnessing the potential of this trend for healthcare purposes has become a focal point for researchers and industry, in particular designing applications that can be used by patients as part of their wellness, prevention, or treatment process. Along the way, mobile cloud computing (MCC) has been introduced to be a potential paradigm for mobile health services to overcome the interoperability issues across different information formats. In this paper, we propose a mobile cloud-based food calorie measurement system. Our system provides users with convenient and intelligent mechanisms that allow them to track their food intake and monitor their calorie count. The food recognition technique in our system uses cloud Support Vector Machine (SVM) training mechanism in a cloud computing environment with Map Reduce technique for distributed machine learning. The details of the system and its implementation results are recorded in this paper.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Multimedia} and {Expo} {Workshops} ({ICMEW})},
	author = {Pouladzadeh, Parisa and Kuhad, Pallavi and Peddi, Sri Vijay Bharat and Yassine, Abdulsalam and Shirmohammadi, Shervin},
	month = jul,
	year = {2014},
	note = {ISSN: 1945-7871},
	keywords = {Classification, Training, Accuracy, Support vector machines, Mobile communication, Servers, Cloud computing, Databases, Food recognition, Graph cut, Segmentation},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:files/150/6890665.html:text/html},
}

@inproceedings{borges_oliveira_fast_2017,
	address = {Venice},
	title = {Fast {CNN}-{Based} {Document} {Layout} {Analysis}},
	isbn = {978-1-5386-1034-3},
	url = {https://ieeexplore.ieee.org/document/8265351/},
	doi = {10.1109/ICCVW.2017.142},
	abstract = {Automatic document layout analysis is a crucial step in cognitive computing and processes that extract information out of document images, such as speciﬁc-domain knowledge database creation, graphs and images understanding, extraction of structured data from tables, and others. Even with the progress observed in this ﬁeld in the last years, challenges are still open and range from accurately detecting content boxes to classifying them into semantically meaningful classes. With the popularization of mobile devices and cloud-based services, the need for approaches that are both fast and economic in data usage is a reality. In this paper we propose a fast one-dimensional approach for automatic document layout analysis considering text, ﬁgures and tables based on convolutional neural networks (CNN). We take advantage of the inherently one-dimensional pattern observed in text and table blocks to reduce the dimension analysis from bi-dimensional documents images to 1D signatures, improving signiﬁcantly the overall performance: we present considerably faster execution times and more compact data usage with no loss in overall accuracy if compared with a classical bidimensional CNN approach.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCVW})},
	publisher = {IEEE},
	author = {Borges Oliveira, Dario Augusto and Viana, Matheus Palhares},
	month = oct,
	year = {2017},
	pages = {1173--1180},
	file = {Borges Oliveira and Viana - 2017 - Fast CNN-Based Document Layout Analysis.pdf:files/151/Borges Oliveira and Viana - 2017 - Fast CNN-Based Document Layout Analysis.pdf:application/pdf},
}

@article{ballsun-stanton_faims_2018,
	title = {{FAIMS} {Mobile}: {Flexible}, open-source software for field research},
	volume = {7},
	issn = {2352-7110},
	shorttitle = {{FAIMS} {Mobile}},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711017300869},
	doi = {10.1016/j.softx.2017.12.006},
	abstract = {FAIMS Mobile is a native Android application supported by an Ubuntu server facilitating human-mediated field research across disciplines. It consists of ‘core’ Java and Ruby software providing a platform for data capture, which can be deeply customised using ‘definition packets’ consisting of XML documents (data schema and UI) and Beanshell scripts (automation). Definition packets can also be generated using an XML-based domain-specific language, making customisation easier. FAIMS Mobile includes features allowing rich and efficient data capture tailored to the needs of fieldwork. It also promotes synthetic research and improves transparency and reproducibility through the production of comprehensive datasets that can be mapped to vocabularies or ontologies as they are created.},
	language = {en},
	urldate = {2022-09-23},
	journal = {SoftwareX},
	author = {Ballsun-Stanton, Brian and Ross, Shawn A. and Sobotkova, Adela and Crook, Penny},
	month = jan,
	year = {2018},
	keywords = {Android, Field research, Field science, Mobile software},
	pages = {47--52},
	file = {Full Text:files/155/Ballsun-Stanton et al. - 2018 - FAIMS Mobile Flexible, open-source software for f.pdf:application/pdf;ScienceDirect Snapshot:files/154/S2352711017300869.html:text/html},
}

@misc{cho_how_2016,
	title = {How much data is needed to train a medical image deep learning system to achieve necessary high accuracy?},
	url = {http://arxiv.org/abs/1511.06348},
	abstract = {The use of Convolutional Neural Networks (CNN) in natural image classification systems has produced very impressive results. Combined with the inherent nature of medical images that make them ideal for deep-learning, further application of such systems to medical image classification holds much promise. However, the usefulness and potential impact of such a system can be completely negated if it does not reach a target accuracy. In this paper, we present a study on determining the optimum size of the training data set necessary to achieve high classification accuracy with low variance in medical image classification systems. The CNN was applied to classify axial Computed Tomography (CT) images into six anatomical classes. We trained the CNN using six different sizes of training data set (5, 10, 20, 50, 100, and 200) and then tested the resulting system with a total of 6000 CT images. All images were acquired from the Massachusetts General Hospital (MGH) Picture Archiving and Communication System (PACS). Using this data, we employ the learning curve approach to predict classification accuracy at a given training sample size. Our research will present a general methodology for determining the training data set size necessary to achieve a certain target classification accuracy that can be easily applied to other problems within such systems.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Cho, Junghwan and Lee, Kyewook and Shin, Ellie and Choy, Garry and Do, Synho},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06348 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:files/157/Cho et al. - 2016 - How much data is needed to train a medical image d.pdf:application/pdf;arXiv.org Snapshot:files/158/1511.html:text/html},
}

@misc{perez_effectiveness_2017,
	title = {The {Effectiveness} of {Data} {Augmentation} in {Image} {Classification} using {Deep} {Learning}},
	url = {http://arxiv.org/abs/1712.04621},
	abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Perez, Luis and Wang, Jason},
	month = dec,
	year = {2017},
	note = {arXiv:1712.04621 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:files/161/Perez and Wang - 2017 - The Effectiveness of Data Augmentation in Image Cl.pdf:application/pdf;arXiv.org Snapshot:files/162/1712.html:text/html},
}

@inproceedings{hussain_study_2019,
	address = {Cham},
	series = {Advances in {Intelligent} {Systems} and {Computing}},
	title = {A {Study} on {CNN} {Transfer} {Learning} for {Image} {Classification}},
	isbn = {978-3-319-97982-3},
	doi = {10.1007/978-3-319-97982-3_16},
	abstract = {Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.},
	language = {en},
	booktitle = {Advances in {Computational} {Intelligence} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hussain, Mahbub and Bird, Jordan J. and Faria, Diego R.},
	editor = {Lotfi, Ahmad and Bouchachia, Hamid and Gegov, Alexander and Langensiepen, Caroline and McGinnity, Martin},
	year = {2019},
	keywords = {Caltech Face, Convolutional Neural Network (CNN), Pre-trained Model, Rectified Linear Unit (ReLU), Transfer Learning},
	pages = {191--202},
	file = {Full Text PDF:files/164/Hussain et al. - 2019 - A Study on CNN Transfer Learning for Image Classif.pdf:application/pdf},
}

@misc{konecny_federated_2016-1,
	title = {Federated {Optimization}: {Distributed} {Machine} {Learning} for {On}-{Device} {Intelligence}},
	shorttitle = {Federated {Optimization}},
	url = {http://arxiv.org/abs/1610.02527},
	doi = {10.48550/arXiv.1610.02527},
	abstract = {We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of {\textbackslash}federated optimization.},
	urldate = {2022-09-23},
	publisher = {arXiv},
	author = {Konečný, Jakub and McMahan, H. Brendan and Ramage, Daniel and Richtárik, Peter},
	month = oct,
	year = {2016},
	note = {arXiv:1610.02527 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:files/167/Konečný et al. - 2016 - Federated Optimization Distributed Machine Learni.pdf:application/pdf;arXiv.org Snapshot:files/169/1610.html:text/html},
}

@article{nguyen_machine_2019,
	title = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining: a survey},
	volume = {52},
	issn = {1573-7462},
	shorttitle = {Machine {Learning} and {Deep} {Learning} frameworks and libraries for large-scale data mining},
	url = {https://doi.org/10.1007/s10462-018-09679-z},
	doi = {10.1007/s10462-018-09679-z},
	abstract = {The combined impact of new computing resources and techniques with an increasing avalanche of large datasets, is transforming many research areas and may lead to technological breakthroughs that can be used by billions of people. In the recent years, Machine Learning and especially its subfield Deep Learning have seen impressive advances. Techniques developed within these two fields are now able to analyze and learn from huge amounts of real world examples in a disparate formats. While the number of Machine Learning algorithms is extensive and growing, their implementations through frameworks and libraries is also extensive and growing too. The software development in this field is fast paced with a large number of open-source software coming from the academy, industry, start-ups or wider open-source communities. This survey presents a recent time-slide comprehensive overview with comparisons as well as trends in development and usage of cutting-edge Artificial Intelligence software. It also provides an overview of massive parallelism support that is capable of scaling computation effectively and efficiently in the era of Big Data.},
	language = {en},
	number = {1},
	urldate = {2022-09-23},
	journal = {Artif Intell Rev},
	author = {Nguyen, Giang and Dlugolinsky, Stefan and Bobák, Martin and Tran, Viet and López García, Álvaro and Heredia, Ignacio and Malík, Peter and Hluchý, Ladislav},
	month = jun,
	year = {2019},
	keywords = {Deep Learning, Artificial Intelligence software, Graphics processing unit (GPU), Intensive computing, Large-scale data mining, Machine Learning, Parallel processing},
	pages = {77--124},
	file = {Full Text PDF:files/170/Nguyen et al. - 2019 - Machine Learning and Deep Learning frameworks and .pdf:application/pdf},
}

@article{lane_squeezing_2017,
	title = {Squeezing {Deep} {Learning} into {Mobile} and {Embedded} {Devices}},
	volume = {16},
	issn = {1558-2590},
	doi = {10.1109/MPRV.2017.2940968},
	abstract = {This department provides an overview the progress the authors have made to the emerging area of embedded and mobile forms of on-device deep learning. Their work addresses two core technical questions. First, how should deep learning principles and algorithms be applied to sensor inference problems that are central to this class of computing? Second, what is required for current and future deep learning innovations to be efficiently integrated into a variety of mobile resource-constrained systems? Toward answering such questions, the authors describe phone, watch, and embedded prototypes that can locally run large-scale deep networks processing audio, images, and inertial sensor data. These prototypes are enabled with a variety of algorithmic and system-level innovations that vastly reduce conventional inference-time overhead of deep models.},
	number = {3},
	journal = {IEEE Pervasive Computing},
	author = {Lane, Nicholas D. and Bhattacharya, Sourav and Mathur, Akhil and Georgiev, Petko and Forlivesi, Claudio and Kawsar, Fahim},
	year = {2017},
	note = {Conference Name: IEEE Pervasive Computing},
	keywords = {deep learning, Machine learning, Sensors, Computer architecture, Deep learning, Neural networks, deep neural networks, Mobile communication, Digital signal processing, embedded systems, Embedded systems, mobile, pervasive computing, Smart devices, Smart phones, smart watches, smartphones},
	pages = {82--88},
	file = {IEEE Xplore Abstract Record:files/172/7994570.html:text/html},
}

@incollection{schapire_foundations_2012,
	title = {Foundations of {Machine} {Learning}},
	isbn = {978-0-262-30118-3},
	url = {https://ieeexplore.ieee.org/document/6282245},
	abstract = {This chapter contains sections titled: 2.1 A Direct Approach to Machine Learning, 2.2 General Methods of Analysis, 2.3 A Foundation for the Study of Boosting Algorithms, Summary, Bibliographic Notes, Exercises},
	urldate = {2022-09-23},
	booktitle = {Boosting: {Foundations} and {Algorithms}},
	publisher = {MIT Press},
	author = {Schapire, Robert E. and Freund, Yoav},
	year = {2012},
	note = {Conference Name: Boosting: Foundations and Algorithms},
	pages = {23--52},
	file = {IEEE Xplore Abstract Record:files/174/6282245.html:text/html},
}

@article{motawie_security_2016,
	title = {Security {Problems} in {Cloud} {Computing}},
	volume = {4},
	copyright = {Copyright (c) 2017 Rola Motawie, Mahmoud M. El-Khouly, Samir Abou El-Seoud},
	issn = {2197-8581},
	url = {https://online-journals.org/index.php/i-jes/article/view/6538},
	doi = {10.3991/ijes.v4i4.6538},
	abstract = {Cloud is a pool of computing resources which are distributed among cloud users. Cloud computing has many benefits like scalability, flexibility, cost savings, reliability, maintenance and mobile accessibility. Since cloud-computing technology is growing day by day, it comes with many security problems. Securing the data in the cloud environment is most critical challenges which act as a barrier when implementing the cloud. There are many new concepts that cloud introduces, such as resource sharing, multi-tenancy, and outsourcing, create new challenges for the security community. In this work, we provide a comparable study of cloud computing privacy and security concerns. We identify and classify known security threats, cloud vulnerabilities, and attacks.},
	language = {en},
	number = {4},
	urldate = {2022-09-23},
	journal = {International Journal of Recent Contributions from Engineering, Science \& IT (iJES)},
	author = {Motawie, Rola and El-Khouly, Mahmoud M. and El-Seoud, Samir Abou},
	month = dec,
	year = {2016},
	note = {Number: 4},
	pages = {36--40},
	file = {Full Text PDF:files/176/Motawie et al. - 2016 - Security Problems in Cloud Computing.pdf:application/pdf},
}

@inproceedings{xanthopoulos_comparative_2013,
	address = {Thessaloniki, Greece},
	title = {A comparative analysis of cross-platform development approaches for mobile applications},
	isbn = {978-1-4503-1851-8},
	url = {http://dl.acm.org/citation.cfm?doid=2490257.2490292},
	doi = {10.1145/2490257.2490292},
	abstract = {Nowadays, native mobile applications (mobile apps) are targeted at specific mobile platforms. This phenomenon imposes severe constraints, such as the use of different development environments, technologies, and APIs (Application Programming Interfaces) for each mobile platform, leading inevitably to a waste of development time and effort, and an increased maintenance cost.},
	language = {en},
	urldate = {2022-09-23},
	booktitle = {Proceedings of the 6th {Balkan} {Conference} in {Informatics} on - {BCI} '13},
	publisher = {ACM Press},
	author = {Xanthopoulos, Spyros and Xinogalos, Stelios},
	year = {2013},
	pages = {213},
	file = {Xanthopoulos and Xinogalos - 2013 - A comparative analysis of cross-platform developme.pdf:files/177/Xanthopoulos and Xinogalos - 2013 - A comparative analysis of cross-platform developme.pdf:application/pdf},
}

@article{ballsun-stanton_faims3_2021,
	title = {{FAIMS3} {Elaboration} {Report}},
	url = {https://zenodo.org/record/4616766},
	abstract = {This document is the technical elaboration milestone for FAIMS3 development. For more details on FAIMS3 see https://osf.io/mqk45/. In December 2019 the Australian Research Data Commons (ARDC) awarded a team of researchers led by Macquarie University \$600,000 over three years to build a digital platform for data collection in the field and offline. This was matched by \$720,920 in co-investment from project partners. FAIMS3 is a ground-up rewrite of the FAIMS Mobile (v2.6) offline-capable, geospatial, multimedia, field-data collection application. This rewrite is designed to be multi-platform, maintainable, and to support data collection at a citizen-science scale. The code and platform should last for at least five years, assuming regular maintenance. This document proposes how we can achieve these objectives.},
	language = {eng},
	urldate = {2022-09-23},
	author = {Ballsun-Stanton, Brian and Angreani, Rini and Cassidy, Steve and O'Toole, Simon and Lorente, Nuria and Mannering, Elizabeth},
	month = mar,
	year = {2021},
	doi = {10.5281/zenodo.4616766},
	note = {Publisher: Zenodo},
	keywords = {FAIMS, Milestone},
	file = {Zenodo Full Text PDF:files/180/Ballsun-Stanton et al. - 2021 - FAIMS3 Elaboration Report.pdf:application/pdf},
}

@article{albert_fall_2012,
	title = {Fall {Classification} by {Machine} {Learning} {Using} {Mobile} {Phones}},
	volume = {7},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0036556},
	doi = {10.1371/journal.pone.0036556},
	abstract = {Fall prevention is a critical component of health care; falls are a common source of injury in the elderly and are associated with significant levels of mortality and morbidity. Automatically detecting falls can allow rapid response to potential emergencies; in addition, knowing the cause or manner of a fall can be beneficial for prevention studies or a more tailored emergency response. The purpose of this study is to demonstrate techniques to not only reliably detect a fall but also to automatically classify the type. We asked 15 subjects to simulate four different types of falls–left and right lateral, forward trips, and backward slips–while wearing mobile phones and previously validated, dedicated accelerometers. Nine subjects also wore the devices for ten days, to provide data for comparison with the simulated falls. We applied five machine learning classifiers to a large time-series feature set to detect falls. Support vector machines and regularized logistic regression were able to identify a fall with 98\% accuracy and classify the type of fall with 99\% accuracy. This work demonstrates how current machine learning approaches can simplify data collection for prevention in fall-related research as well as improve rapid response to potential injuries due to falls.},
	language = {en},
	number = {5},
	urldate = {2022-09-23},
	journal = {PLOS ONE},
	author = {Albert, Mark V. and Kording, Konrad and Herrmann, Megan and Jayaraman, Arun},
	month = may,
	year = {2012},
	note = {Publisher: Public Library of Science},
	keywords = {Machine learning, Support vector machines, Accelerometers, Cell phones, Falls, Geriatric care, Head injury, Machine learning algorithms},
	pages = {e36556},
	file = {Full Text PDF:files/182/Albert et al. - 2012 - Fall Classification by Machine Learning Using Mobi.pdf:application/pdf;Snapshot:files/183/article.html:text/html},
}

@article{tiedemann_automotive_2016,
	series = {3rd {International} {Conference} on {System}-{Integrated} {Intelligence}: {New} {Challenges} for {Product} and {Production} {Engineering}},
	title = {An {Automotive} {Distributed} {Mobile} {Sensor} {Data} {Collection} with {Machine} {Learning} {Based} {Data} {Fusion} and {Analysis} on a {Central} {Backend} {System}},
	volume = {26},
	issn = {2212-0173},
	url = {https://www.sciencedirect.com/science/article/pii/S2212017316304182},
	doi = {10.1016/j.protcy.2016.08.071},
	abstract = {One of the most extensive examples for ubiquitous computing today is automotion. The equipment of sensors and independent computing devices in current vehicles is vast if not endless. Furthermore, traffic infrastructure is realized using global and local computing devices. Communication initiated by the car itself (e.g., to an emergency hotline) will be obligatory in some countries soon. And finally, by using a smart phone the driver brings an additional powerful computing device and sensor set to the vehicle. However, all these automotive sensors and computing devices are used just for fixed (and in most cases single) purposes. Data exchange between vehicles or vehicles and infrastructure is rarely done. And dynamic changes like compensating for a broken sensor with available other data, using old sensor equipment for new functions, or improving old driver assistance systems with new sensors is not possible, either. The objective of the collaborative research project Smart Adaptive Data Aggregation (SADA) is to develop technologies that enable linking data from distributed mobile on-board sensors (on vehicles) with data from previously unknown stationary (e.g., infrastructure) or mobile sensors (e.g., other vehicles, smart devices). One focus of the project is the dynamic and fully-automated switching between different sensors or sensor configurations, including the adaptation of data fusion processes. Technically, one important component for some of the SADA use cases is a central backend system that (1) collects sensor data of the vehicles and/or the infrastructure, (2) fuses these data, and (3) carries out machine learning (ML) based analysis of the data to generate new information for the drivers (sometimes refered to by the term “virtual sensors”). The article gives a short overview of the SADA project and describes in more detail the concept of the backend system architec- ture, the user interface, and the methods and processes needed for a demonstration use-case.},
	language = {en},
	urldate = {2022-09-23},
	journal = {Procedia Technology},
	author = {Tiedemann, Tim and Backe, Christian and Vögele, Thomas and Conradi, Peter},
	month = jan,
	year = {2016},
	keywords = {Big Data, Distributed ML, IoT, Pervasive Computing, Sensor Cloud},
	pages = {570--579},
	file = {Full Text:files/186/Tiedemann et al. - 2016 - An Automotive Distributed Mobile Sensor Data Colle.pdf:application/pdf;ScienceDirect Snapshot:files/185/S2212017316304182.html:text/html},
}

@phdthesis{mofor_nkaze_pymedserver_2019,
	type = {Thesis},
	title = {{PyMedServer} : a server framework for mobile data collection and machine learning},
	copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
	shorttitle = {{PyMedServer}},
	url = {https://dspace.mit.edu/handle/1721.1/123747},
	abstract = {Over the past decade, the widespread adoption of smart phones has enabled their use as a primary data collection platform for a wide variety of research studies, including areas such as global health field research. In addition, smart phones also provide a portable means of collecting labelled data for use in Machine Learning and Artificial Intelligence. Despite these important global trends, existing scalable mobile data collection platforms are not available for use with machine learning data collection. In order to address this need, I have created PyMedServer, which is an easy-to-use server framework designed for large scale medical research spanning multiple distinct institutions. The framework provides built-in abstractions for clinicians, patients, medical measurements, clinician diagnoses, and machine learning analyses. To further facilitate adoption, PyMedServer provides client libraries for Android and Web. These libraries are compatible with any server built using the PyMedServer framework and provide features such as Local Storage, API integration, User Authentication and Authorization, Multi-Group Support and Measurement Labelling, to name a few. PyMedServer is written in Python and is designed to permit and facilitate the addition of plugins. Developers are granted access to labeled data and can contribute with Feature Extraction and Machine Learning plugins, while the framework takes care of concerns such as of Group Isolation, Security, Scalability, and Deployability.},
	language = {eng},
	urldate = {2022-09-23},
	school = {Massachusetts Institute of Technology},
	author = {Mofor Nkaze, John},
	year = {2019},
	note = {Accepted: 2020-02-10T21:41:10Z
Journal Abbreviation: Server framework for mobile data collection and machine learning},
	file = {Full Text PDF:files/188/Mofor Nkaze - 2019 - PyMedServer  a server framework for mobile data c.pdf:application/pdf;Snapshot:files/189/123747.html:text/html},
}

@article{chen_joint_2022,
	title = {Joint {Data} {Collection} and {Resource} {Allocation} for {Distributed} {Machine} {Learning} at the {Edge}},
	volume = {21},
	issn = {1558-0660},
	doi = {10.1109/TMC.2020.3045436},
	abstract = {Under the paradigm of edge computing, the enormous data generated at the network edge can be processed locally. To make full utilization of these widely distributed data, we focus on an edge computing system that conducts distributed machine learning using gradient-descent based approaches. To ensure the system’s performance, there are two major challenges: how to collect data from multiple data source nodes for training jobs and how to allocate the limited resources on each edge server among these jobs. In this paper, we jointly consider the two challenges for distributed training (without service requirement), aiming to maximize the system throughput while ensuring the system’s quality of service (QoS). Specifically, we formulate the joint problem as a mixed-integer non-linear program, which is NP-hard, and propose an efficient approximation algorithm. Furthermore, we take service placement into consideration for diverse training jobs and propose an approximation algorithm. We also analyze that our proposed algorithm can achieve the constant bipartite approximation under many practical situations. We build a test-bed to evaluate the effectiveness of our proposed algorithm in a practical scenario. Extensive simulation results and testing results show that the proposed algorithms can improve the system throughput 56-69 percent compared with the conventional algorithms.},
	number = {8},
	journal = {IEEE Transactions on Mobile Computing},
	author = {Chen, Min and Wang, Haichuan and Meng, Zeyu and Xu, Hongli and Xu, Yang and Liu, Jianchun and Huang, He},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Mobile Computing},
	keywords = {Training, Data models, Servers, Distributed databases, Edge computing, job assignment, model training, resource allocation, Resource management, service placement, Task analysis},
	pages = {2876--2894},
}

@article{beker_machine_2022,
	title = {Machine {Learning} {May} {Sometimes} {Simply} {Capture} {Literature} {Popularity} {Trends}: {A} {Case} {Study} of {Heterocyclic} {Suzuki}–{Miyaura} {Coupling}},
	volume = {144},
	issn = {0002-7863},
	shorttitle = {Machine {Learning} {May} {Sometimes} {Simply} {Capture} {Literature} {Popularity} {Trends}},
	url = {https://doi.org/10.1021/jacs.1c12005},
	doi = {10.1021/jacs.1c12005},
	abstract = {Applications of machine learning (ML) to synthetic chemistry rely on the assumption that large numbers of literature-reported examples should enable construction of accurate and predictive models of chemical reactivity. This paper demonstrates that abundance of carefully curated literature data may be insufficient for this purpose. Using an example of Suzuki–Miyaura coupling with heterocyclic building blocks─and a carefully selected database of {\textgreater}10,000 literature examples─we show that ML models cannot offer any meaningful predictions of optimum reaction conditions, even if the search space is restricted to only solvents and bases. This result holds irrespective of the ML model applied (from simple feed-forward to state-of-the-art graph-convolution neural networks) or the representation to describe the reaction partners (various fingerprints, chemical descriptors, latent representations, etc.). In all cases, the ML methods fail to perform significantly better than naive assignments based on the sheer frequency of certain reaction conditions reported in the literature. These unsatisfactory results likely reflect subjective preferences of various chemists to use certain protocols, other biasing factors as mundane as availability of certain solvents/reagents, and/or a lack of negative data. These findings highlight the likely importance of systematically generating reliable and standardized data sets for algorithm training.},
	number = {11},
	urldate = {2022-09-23},
	journal = {J. Am. Chem. Soc.},
	author = {Beker, Wiktor and Roszak, Rafał and Wołos, Agnieszka and Angello, Nicholas H. and Rathore, Vandana and Burke, Martin D. and Grzybowski, Bartosz A.},
	month = mar,
	year = {2022},
	note = {Publisher: American Chemical Society},
	pages = {4819--4827},
	file = {ACS Full Text Snapshot:files/193/jacs.html:text/html;Full Text PDF:files/192/Beker et al. - 2022 - Machine Learning May Sometimes Simply Capture Lite.pdf:application/pdf},
}

@incollection{carbonell_1_1983,
	address = {San Francisco (CA)},
	title = {1 - {AN} {OVERVIEW} {OF} {MACHINE} {LEARNING}},
	isbn = {978-0-08-051054-5},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080510545500054},
	abstract = {This chapter discusses the objective and concept of machine learning. The study and computer modeling of learning processes in their multiple manifestations constitutes the subject matter of machine learning. At present, the field of machine learning is organized around three primary research foci: (1) task-oriented studies—the development and analysis of learning systems to improve performance in a predetermined set of tasks also known as the engineering approach. (2) Cognitive simulation—the investigation and computer simulation of human learning processes. (3) Theoretical analysis—the theoretical exploration of the space of possible learning methods and algorithms independent of application domain. An equally basic scientific objective of machine learning is the exploration of alternative learning mechanisms, including the discovery of different induction algorithms, the scope and limitations of certain methods, the information that must be available to the learner, the issue of coping with imperfect training data, and the creation of general techniques applicable in many task domains.},
	language = {en},
	urldate = {2022-09-23},
	booktitle = {Machine {Learning}},
	publisher = {Morgan Kaufmann},
	author = {Carbonell, Jaime G. and Michalski, Ryszard S. and Mitchell, Tom M.},
	editor = {Michalski, Ryszard S. and Carbonell, Jaime G. and Mitchell, Tom M.},
	month = jan,
	year = {1983},
	doi = {10.1016/B978-0-08-051054-5.50005-4},
	pages = {3--23},
	file = {ScienceDirect Snapshot:files/195/B9780080510545500054.html:text/html},
}

@article{jordan_machine_2015,
	title = {Machine learning: {Trends}, perspectives, and prospects},
	volume = {349},
	shorttitle = {Machine learning},
	url = {https://www.science.org/doi/full/10.1126/science.aaa8415},
	doi = {10.1126/science.aaa8415},
	number = {6245},
	urldate = {2022-09-23},
	journal = {Science},
	author = {Jordan, M. I. and Mitchell, T. M.},
	month = jul,
	year = {2015},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {255--260},
	file = {Full Text PDF:files/197/Jordan and Mitchell - 2015 - Machine learning Trends, perspectives, and prospe.pdf:application/pdf},
}

@article{reich_evaluating_1999,
	title = {Evaluating machine learning models for engineering problems},
	volume = {13},
	issn = {0954-1810},
	url = {https://www.sciencedirect.com/science/article/pii/S0954181098000211},
	doi = {10.1016/S0954-1810(98)00021-1},
	abstract = {The use of machine learning (ML), and in particular, artificial neural networks (ANN), in engineering applications has increased dramatically over the last years. However, by and large, the development of such applications or their report lack proper evaluation. Deficient evaluation practice was observed in the general neural networks community and again in engineering applications through a survey we conducted of articles published in AI in Engineering and elsewhere. This status hinders understanding and prevents progress. This article goal is to remedy this situation. First, several evaluation methods are discussed with their relative qualities. Second, these qualities are illustrated by using the methods to evaluate ANN performance in two engineering problems. Third, a systematic evaluation procedure for ML is discussed. This procedure will lead to better evaluation of studies, and consequently to improved research and practice in the area of ML in engineering applications.},
	language = {en},
	number = {3},
	urldate = {2022-09-23},
	journal = {Artificial Intelligence in Engineering},
	author = {Reich, Yoram and Barai, S. V.},
	month = jul,
	year = {1999},
	keywords = {Artificial neural networks, Modeling, Performance evaluation, Research methodology, Statistical tests},
	pages = {257--272},
	file = {ScienceDirect Full Text PDF:files/199/Reich and Barai - 1999 - Evaluating machine learning models for engineering.pdf:application/pdf;ScienceDirect Snapshot:files/200/S0954181098000211.html:text/html},
}

@article{kourou_machine_2015,
	title = {Machine learning applications in cancer prognosis and prediction},
	volume = {13},
	issn = {2001-0370},
	url = {https://www.sciencedirect.com/science/article/pii/S2001037014000464},
	doi = {10.1016/j.csbj.2014.11.005},
	abstract = {Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised ML techniques as well as on different input features and data samples. Given the growing trend on the application of ML methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes.},
	language = {en},
	urldate = {2022-09-23},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Kourou, Konstantina and Exarchos, Themis P. and Exarchos, Konstantinos P. and Karamouzis, Michalis V. and Fotiadis, Dimitrios I.},
	month = jan,
	year = {2015},
	keywords = {Machine learning, Predictive models, Cancer recurrence, Cancer survival, Cancer susceptibility},
	pages = {8--17},
	file = {Full Text:files/203/Kourou et al. - 2015 - Machine learning applications in cancer prognosis .pdf:application/pdf;ScienceDirect Snapshot:files/202/S2001037014000464.html:text/html},
}

@misc{shaikh_inception_2018,
	title = {Inception {Network} {\textbar} {Implementation} {Of} {GoogleNet} {In} {Keras}},
	url = {https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/},
	abstract = {Inception network used for solving image recognition and detection problems. Learn about Inception networks and implementation of googlenet in Keras.},
	language = {en},
	urldate = {2022-09-29},
	journal = {Analytics Vidhya},
	author = {Shaikh, JalFaizy},
	month = oct,
	year = {2018},
	file = {Snapshot:files/205/understanding-inception-network-from-scratch.html:text/html},
}

@misc{huilgol_top_2020,
	title = {Top 4 {Pre}-{Trained} {Models} for {Image} {Classification} {\textbar} {With} {Python} {Code}},
	url = {https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/},
	abstract = {We cover 4 pre-trained models for Image Classification that are state-of-the-art(SOTA) and are widely used in the industry as well.},
	language = {en},
	urldate = {2022-09-29},
	journal = {Analytics Vidhya},
	author = {Huilgol, Purva},
	month = aug,
	year = {2020},
	file = {Snapshot:files/207/top-4-pre-trained-models-for-image-classification-with-python-code.html:text/html},
}

@misc{woudsma_retrain_2021,
	title = {Retrain {MobileNet} for the web},
	url = {https://github.com/woudsma/retrain-mobilenet-for-the-web},
	abstract = {Retrain a MobileNet V1 or V2 model and use it in the browser with TensorFlow.js},
	urldate = {2022-09-29},
	author = {Woudsma, Tjerk},
	month = jan,
	year = {2021},
	note = {original-date: 2018-07-27T00:27:57Z},
	keywords = {javascript, machine-learning, mobilenet-v1, mobilenet-v2, python, tensorflow-for-poets, tensorflowjs, transfer-learning},
}
